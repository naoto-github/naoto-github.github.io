<!DOCTYPE html>
<html lang="ja">
    <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    畳み込みニューラルネットワーク | mLAB
  </title>

  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
  <link rel="stylesheet" href="/css/custom.css">
  <link rel="stylesheet" href="/css/syntax.css">
  
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="mLAB" />

  
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Kosugi&display=swap" rel="stylesheet">

  
  <script src="https://kit.fontawesome.com/0c97f11cd6.js" crossorigin="anonymous"></script>

  
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.js"></script>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.css"/>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick-theme.min.css"/>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/SlickNav/1.0.10/jquery.slicknav.js"></script>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/SlickNav/1.0.10/slicknav.min.css" />

  
  <div id="fb-root"></div>
  <script async defer crossorigin="anonymous" src="https://connect.facebook.net/ja_JP/sdk.js#xfbml=1&version=v9.0"></script>  
  
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
	     tex2jax: {
	        inlineMath: [['$','$']]
        }
    });
  </script>

  
  

  
  <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
  
  
  

  <meta property="og:title" content="畳み込みニューラルネットワーク" />



  <meta property="og:type" content="article" />



  <meta property="og:url" content="https://mukai-lab.info/pages/classes/advanced_studies_seminar_1/chapter6/" />



  <meta property="og:image" content="https://i.gyazo.com/e25cc14c4809d6fb8e8e430c5656209b.png" />



  <meta property="og:site_name" content="mLAB" />



  <meta property="og:description" content="畳み込みニューラルネットワーク畳み込みニューラルネットワーク(Convolutional Neural Network: CNN) は， 画像や動画の分類に用いられる特殊なニューラルネットワークです． CNNは， 畳み込み層(Convolutional Layer) ， プーリング層(Pooling Layer) ， 全結合層(Full Connected Layer) の3種類で構成されます．
入力画像は，畳み込み層に入力され，プーリング層を経て， 全結合層で集約され，最終的に画像の分類を出力します． この畳み込み層とプーリング層の処理は，複数回繰り返されることがあります． 畳み込み層では複数のフィルタを用いて画像の特徴量を抽出します． プーリング層では特徴を失わないように画像サイズを縮小します（ダウンサンプリング）． 全結合層は多層パーセプトロンで用いられる一般的な層と同じです．

畳み込み層 畳み込み層の仕組みを確認しましょう． 畳み込み層の目的は入力された複数の画像の特徴量をフィルタを利用して抽出することです． 下図は$4 \times 4$の入力画像に，$2 \times 2$のフィルタを適用した様子です． 入力画像からフィルタと同じ$2 \times 2$の画素を取り出し，フィルタの値と掛け合わせることで特徴量となります． この操作を1ピクセルだけずらしながら画像全体に適用すると$3 \times 3$の画像に変換されます． フィルタの値を変えることで，垂直方向の輪郭線や，水平方向の輪郭線を強調することが出来ます．

それでは，この畳み込み層の振る舞いを再現してみましょう．
ノートブックを作成し，ノートブックのタイトルをNN-6 に設定します． まずは，ライブラリをインポートします． 画像処理ライブラリのPillowと 数値計算ライブラリのScipyも追加します．
import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import torchvision import torchvision.transforms as transforms import numpy as np import matplotlib." />



  
  
<meta name="twitter:card" content="summary" />

<meta name="twitter:site" content="@nmukai1978" />


  <meta name="twitter:title" content="畳み込みニューラルネットワーク" />



  <meta name="twitter:description" content="畳み込みニューラルネットワーク畳み込みニューラルネットワーク(Convolutional Neural Network: CNN) は， 画像や動画の分類に用いられる特殊なニューラルネットワークです． CNNは， 畳み込み層(Convolutional Layer) ， プーリング層(Pooling Layer) ， 全結合層(Full Connected Layer) の3種類で構成されます．
入力画像は，畳み込み層に入力され，プーリング層を経て， 全結合層で集約され，最終的に画像の分類を出力します． この畳み込み層とプーリング層の処理は，複数回繰り返されることがあります． 畳み込み層では複数のフィルタを用いて画像の特徴量を抽出します． プーリング層では特徴を失わないように画像サイズを縮小します（ダウンサンプリング）． 全結合層は多層パーセプトロンで用いられる一般的な層と同じです．

畳み込み層 畳み込み層の仕組みを確認しましょう． 畳み込み層の目的は入力された複数の画像の特徴量をフィルタを利用して抽出することです． 下図は$4 \times 4$の入力画像に，$2 \times 2$のフィルタを適用した様子です． 入力画像からフィルタと同じ$2 \times 2$の画素を取り出し，フィルタの値と掛け合わせることで特徴量となります． この操作を1ピクセルだけずらしながら画像全体に適用すると$3 \times 3$の画像に変換されます． フィルタの値を変えることで，垂直方向の輪郭線や，水平方向の輪郭線を強調することが出来ます．

それでは，この畳み込み層の振る舞いを再現してみましょう．
ノートブックを作成し，ノートブックのタイトルをNN-6 に設定します． まずは，ライブラリをインポートします． 画像処理ライブラリのPillowと 数値計算ライブラリのScipyも追加します．
import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim import torchvision import torchvision.transforms as transforms import numpy as np import matplotlib." />



  <meta name="twitter:image" content="https://i.gyazo.com/e25cc14c4809d6fb8e8e430c5656209b.png" />


  
</head>

    <body>
        <nav class="nav">
  <div class="nav-container">
    
    <a href="/">
    <h2 class="nav-title">
      <img src="https://mukai-lab.info/favicon/favicon-48x48.png" align="top"/>
      <span>mLAB</span>
      

<ul id="menu">
  
  
  <li>
    <a href="/">Top</a>
  </li>
  
  
  
  <li>
    <a href="/classes/">Classes</a>
  </li>
  
  
  
  <li>
    <a href="/posts/">Comics</a>
  </li>
  
  
</ul>

<script>
  $(function(){
    $("#menu").slicknav({
      label: "メニュー"
    });
  });
</script>



    </h2>
    </a>
  </div>
</nav>

        

<main>
  <div class="post">
    
    <h1 class="post-title">畳み込みニューラルネットワーク</h1>
<div class="post-line"></div>
    
    

    <p><a href="https://gyazo.com/e25cc14c4809d6fb8e8e430c5656209b"><img src="https://i.gyazo.com/e25cc14c4809d6fb8e8e430c5656209b.png" alt="Image from Gyazo"></a></p>
<h1 style="padding-left:40px; line-height: 30px; background: url(http://mukai-lab.info/logo/logo.png) no-repeat">
  畳み込みニューラルネットワーク
</h1>

<p><strong>畳み込みニューラルネットワーク(Convolutional Neural Network: CNN)</strong> は，
画像や動画の分類に用いられる特殊なニューラルネットワークです．
CNNは， <strong>畳み込み層(Convolutional Layer)</strong> ， <strong>プーリング層(Pooling Layer)</strong> ，
<strong>全結合層(Full Connected Layer)</strong> の3種類で構成されます．</p>
<p>入力画像は，畳み込み層に入力され，プーリング層を経て，
全結合層で集約され，最終的に画像の分類を出力します．
この畳み込み層とプーリング層の処理は，複数回繰り返されることがあります．
畳み込み層では複数のフィルタを用いて画像の特徴量を抽出します．
プーリング層では特徴を失わないように画像サイズを縮小します（ダウンサンプリング）．
全結合層は多層パーセプトロンで用いられる一般的な層と同じです．</p>
<p><a href="https://gyazo.com/028beb90c4e1df589234ee12228ecc62"><img src="https://i.gyazo.com/028beb90c4e1df589234ee12228ecc62.png" alt="Image from Gyazo"></a></p>
<h2 id="畳み込み層">畳み込み層</h2>
<p>畳み込み層の仕組みを確認しましょう．
畳み込み層の目的は入力された複数の画像の特徴量をフィルタを利用して抽出することです．
下図は$4 \times 4$の入力画像に，$2 \times 2$のフィルタを適用した様子です．
入力画像からフィルタと同じ$2 \times 2$の画素を取り出し，フィルタの値と掛け合わせることで特徴量となります．
この操作を1ピクセルだけずらしながら画像全体に適用すると$3 \times 3$の画像に変換されます．
フィルタの値を変えることで，垂直方向の輪郭線や，水平方向の輪郭線を強調することが出来ます．</p>
<p><a href="https://gyazo.com/07d6ec107bd1478b64775030d3de4964"><img src="https://i.gyazo.com/07d6ec107bd1478b64775030d3de4964.png" alt="Image from Gyazo"></a></p>
<p>それでは，この畳み込み層の振る舞いを再現してみましょう．</p>
<p>ノートブックを作成し，ノートブックのタイトルを<strong>NN-6</strong> に設定します．
まずは，ライブラリをインポートします．
画像処理ライブラリの<a href="http://www.pythonware.com/products/pil/">Pillow</a>と
数値計算ライブラリの<a href="https://www.scipy.org/">Scipy</a>も追加します．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> torch
<span style="color:#f92672">import</span> torch.nn <span style="color:#f92672">as</span> nn
<span style="color:#f92672">import</span> torch.nn.functional <span style="color:#f92672">as</span> F
<span style="color:#f92672">import</span> torch.optim <span style="color:#f92672">as</span> optim
<span style="color:#f92672">import</span> torchvision
<span style="color:#f92672">import</span> torchvision.transforms <span style="color:#f92672">as</span> transforms
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> scipy <span style="color:#f92672">import</span> signal
<span style="color:#f92672">from</span> PIL <span style="color:#f92672">import</span> Image
</code></pre></div><p>前回と同じ<a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10</a>データセットを利用します．
また，簡単化のためRGB画像をグレースケール画像に変換します．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dataset <span style="color:#f92672">=</span> torchvision<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>CIFAR10(root<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./data&#34;</span>, download<span style="color:#f92672">=</span>True)
image, label <span style="color:#f92672">=</span> dataset[<span style="color:#ae81ff">0</span>]
gray_image <span style="color:#f92672">=</span> image<span style="color:#f92672">.</span>convert(<span style="color:#e6db74">&#39;L&#39;</span>) <span style="color:#75715e">#グレースケールに変換</span>
plt<span style="color:#f92672">.</span>imshow(gray_image, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gray&#34;</span>) <span style="color:#75715e">#グレースケールとして表示</span>
</code></pre></div><p><a href="https://gyazo.com/d4844e881162d9abcd2b30dbd637088d"><img src="https://i.gyazo.com/d4844e881162d9abcd2b30dbd637088d.png" alt="Image from Gyazo"></a></p>
<p><strong>PIL.Image.Image</strong> を <strong>Numpy</strong> に変換します．
$32 \times 32 $の行列でグレースケールの画素値が保持されています．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">gray_image <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>asarray(gray_image)
<span style="color:#66d9ef">print</span>(gray_image<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(gray_image[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>])
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#出力</span>
(<span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">32</span>)
<span style="color:#ae81ff">61</span>
</code></pre></div><p>水平方向の輪郭線を強調するための$3 \times 3$のフィルタを作成します．</p>
<p>$$
\left(
\begin{array}{ccc}
-1 &amp; -1 &amp; -1 \\\<br>
2 &amp; 2 &amp; 2	\\\<br>
-1 &amp; -1 &amp; -1<br>
\end{array}
\right)\<br>
$$</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">filter <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>],[<span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">2.0</span>, <span style="color:#ae81ff">2.0</span>],[<span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.0</span>]])
<span style="color:#66d9ef">print</span>(filter<span style="color:#f92672">.</span>shape)
<span style="color:#66d9ef">print</span>(filter)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#出力</span>
(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>)
[[<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>]
 [ <span style="color:#ae81ff">2.</span>  <span style="color:#ae81ff">2.</span>  <span style="color:#ae81ff">2.</span>]
 [<span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1.</span>]]
</code></pre></div><p><code>signal.convolve2d()</code>で2次元の畳み込みを行います．
出力画像は$30 \times 30$に縮小されることに注意してください．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">converted_image <span style="color:#f92672">=</span> signal<span style="color:#f92672">.</span>convolve2d(gray_image, filter, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;valid&#34;</span>)
<span style="color:#66d9ef">print</span>(converted_image<span style="color:#f92672">.</span>shape)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#出力</span>
(<span style="color:#ae81ff">30</span>, <span style="color:#ae81ff">30</span>)
</code></pre></div><p>画像を <strong>Numpy</strong> から， <strong>PIL.Imgage.Image</strong> に戻して表示してみます．
水平方向の輪郭線に強く反応した画像が生成されていることが分かります．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">converted_image <span style="color:#f92672">=</span> Image<span style="color:#f92672">.</span>fromarray(converted_image)
plt<span style="color:#f92672">.</span>imshow(converted_image, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;gray&#34;</span>)
</code></pre></div><p><a href="https://gyazo.com/f920da097366590a8878c7f050511c6e"><img src="https://i.gyazo.com/f920da097366590a8878c7f050511c6e.png" alt="Image from Gyazo"></a></p>
<p>同様の操作で垂直の輪郭線を強調するフィルタを適用すると下記の画像となります．</p>
<p>$$
\left(
\begin{array}{ccc}
-1 &amp; 2 &amp; -1 \\\<br>
-1 &amp; 2 &amp; -1	\\\<br>
-1 &amp; 2 &amp; -1<br>
\end{array}
\right)\<br>
$$</p>
<p><a href="https://gyazo.com/a493c53745059e91cfab6262bb2ff5c2"><img src="https://i.gyazo.com/a493c53745059e91cfab6262bb2ff5c2.png" alt="Image from Gyazo"></a></p>
<p>このようにフィルタを複数用意することで，画像の異なる特徴を抽出することが可能です．
上記はグレースケール画像でしたが，RGB画像の場合は，3色（3チャネル）それぞれにフィルタが用意されます．
また，抽出された特徴量は，多層パーセプトロンと同様に，バイアスを加えた後，活性化関数を適用して，
次のプーリング層に伝達されます．</p>
<h2 id="プーリング層">プーリング層</h2>
<p>プーリング層の仕組みを確認しましょう．
プーリング層の目的は，画像サイズを縮小することで計算量を減らすことに加え，
画像中の認識対象の位置変化に対する柔軟性を向上させます（位置が違っても同じように認識できる）．</p>
<p>下図は$4 \times 4$の特徴量に，$2 \times 2$のMAXプーリングを適用した様子です．
MAXプーリングは対象領域の最大値を代表値とする方法です．
この値が全結合層に伝達され，全結合層で分類結果を出力します．
プーリング層では重みやバイアスの学習は行われません．</p>
<p><a href="https://gyazo.com/c3fee05027aec01e97e09ae07ae410e5"><img src="https://i.gyazo.com/c3fee05027aec01e97e09ae07ae410e5.png" alt="Image from Gyazo"></a></p>
<h1 style="padding-left:40px; line-height: 30px; background: url(http://mukai-lab.info/logo/logo.png) no-repeat">
  ネットワークの学習
</h1>

<p>それでは，畳込みニューラルネットワークを利用して，CIFAR-10の分類に挑戦しましょう．
<code>transform=Transforms.ToTensor()</code>オプションを設定して，データセットを読み込みます．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">dataset <span style="color:#f92672">=</span> torchvision<span style="color:#f92672">.</span>datasets<span style="color:#f92672">.</span>CIFAR10(root<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;./data&#34;</span>, transform<span style="color:#f92672">=</span>transforms<span style="color:#f92672">.</span>ToTensor())
</code></pre></div><p>今回も分類問題の難易度を下げるため，
ラベルが0(airplane)，1(automobile)，2(bird)のみを対象とします．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">my_dataset <span style="color:#f92672">=</span> []
<span style="color:#66d9ef">for</span> image, label <span style="color:#f92672">in</span> dataset:
    <span style="color:#66d9ef">if</span> label <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">or</span> label <span style="color:#f92672">==</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">or</span> label <span style="color:#f92672">==</span> <span style="color:#ae81ff">2</span>:
        my_dataset<span style="color:#f92672">.</span>append((image, label))
</code></pre></div><p>これをバッチサイズ$n=64$のミニバッチ用のデータセットとします．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">loader <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>DataLoader(dataset<span style="color:#f92672">=</span>my_dataset, batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>)
</code></pre></div><p>4層で構成される畳み込みニューラルネットワークを定義します．
これまでの方法とは異なり<code>nn.Module</code>を継承した新しいクラスとして定義します．
コンストラクタである<code>__init__</code>で，畳み込み層1(<code>conv1</code>)，畳み込み層2(<code>conv2</code>)，
結合層1(<code>fc1</code>)，結合層2(<code>fc2</code>)の順でネットワークの層を定義しています．
また，<code>forward()</code>でネットワークの順伝播の方法を定義しています．</p>
<p>畳み込み層は<code>conv2d()</code>で定義します．
畳み込み層1は，入力チャネル（RGBの3色）が$3$，出力チャネル（フィルタ数）が$6$，フィルタサイズが$5 \times 5$です．
畳み込み層2は，入力チャネル（フィルタ数）が$6$，出力チャネル（フィルタ数）が$16$，フィルタサイズが$5 \times 5$です．
また，MAXプーリングは<code>F.max_pool2d()</code>で定義します．
プーリングのサイズは$2 \times 2$に設定しています．
下図は畳み込み層1の処理を示しています．
フィルタごとに畳み込みが行われ，活性化関数ReLUを経て，6つの出力を生成します．
その後で，プーリング層でダウンサンプリングされ，畳み込み層2の入力として与えられます．</p>
<p><a href="https://gyazo.com/bb8cb8681116f6bdfe357f1327043a32"><img src="https://i.gyazo.com/bb8cb8681116f6bdfe357f1327043a32.png" alt="Image from Gyazo"></a></p>
<p>ここで，畳み込み層が出力する画像のサイズを考えます．
元の画像は$32 \times 32$ですが，畳み込み層1で$5 \times 5$のフィルタを適用することで，
画像の大きさは$28 \times 28$に縮小します．
これを$2 \times 2$でプーリングすると，画像は$1/2$のサイズの$14 \times 14$に縮小します．
さらに，畳み込み層2で$5 \times $5のフィルタを適用すると，画像の大きさは$10 \times 10$に縮小し，
$2 \times 2$でプーリングすると$1/2$のサイズの$5 \times 5$に縮小します．
この畳み込み層2の出力が，結合層1の入力となります．</p>
<p><a href="https://gyazo.com/d9cf6509b5c4234e6abf80f891a8d736"><img src="https://i.gyazo.com/d9cf6509b5c4234e6abf80f891a8d736.png" alt="Image from Gyazo"></a></p>
<p>結果的に，$16(フィルタ数) \times 5 \times 5 = 400$が結合層1の入力数になります(<code>view()</code>で1次元に変換してから入力)．
これをもう一度，結合層2に伝搬・集約し，最終的にソフトマックスで各ラベルの確率を出力します．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Net</span>(nn<span style="color:#f92672">.</span>Module):
    <span style="color:#66d9ef">def</span> __init__(self):
        super(Net, self)<span style="color:#f92672">.</span>__init__()
        self<span style="color:#f92672">.</span>conv1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">6</span>, (<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>))　<span style="color:#75715e"># 入力チャネル，出力チャネル，フィルタサイズ</span>
        self<span style="color:#f92672">.</span>conv2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Conv2d(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">16</span>, (<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>))　<span style="color:#75715e"># 入力チャネル，出力チャネル，フィルタサイズ</span>
        self<span style="color:#f92672">.</span>fc1 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">16</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">64</span>) <span style="color:#75715e"># 入力数，出力数</span>
        self<span style="color:#f92672">.</span>fc2 <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Linear(<span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">3</span>)  <span style="color:#75715e"># 入力数，出力数</span>

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>max_pool2d(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv1(x)), <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># 2x2でMAXプーリング</span>
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>max_pool2d(F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>conv2(x)), <span style="color:#ae81ff">2</span>) <span style="color:#75715e"># 2x2でMAXプーリング</span>
        x <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">16</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">*</span> <span style="color:#ae81ff">5</span>) <span style="color:#75715e"># 1次元に整形</span>
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>relu(self<span style="color:#f92672">.</span>fc1(x))
        x <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>softmax(self<span style="color:#f92672">.</span>fc2(x), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
        <span style="color:#66d9ef">return</span> x

network <span style="color:#f92672">=</span> Net()
<span style="color:#66d9ef">print</span>(network)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#出力</span>
Net(
  (conv1): Conv2d(<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">6</span>, kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>), stride<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>))
  (conv2): Conv2d(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">16</span>, kernel_size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>), stride<span style="color:#f92672">=</span>(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>))
  (fc1): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">400</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, bias<span style="color:#f92672">=</span>True)
  (fc2): Linear(in_features<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, out_features<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, bias<span style="color:#f92672">=</span>True)
)
</code></pre></div><p>ここで，学習データに対する正解率を確認してみます．
畳み込みニューラルネットワークの入力の形状は <strong>[バッチサイズ][チャネル数][縦][横]</strong> というテンソルになっている必要があるため，
<code>torch.unsqueeze</code>で次元を拡張し，バッチサイズ$n=1$のデータセットとして，入力していることに注意してください．
学習前であるため，$0.33$という低い正解率でした．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">counter <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
<span style="color:#66d9ef">for</span> image, label <span style="color:#f92672">in</span> my_dataset:
    image <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>unsqueeze(image, <span style="color:#ae81ff">0</span>) <span style="color:#75715e">#次元を増やす</span>
    z <span style="color:#f92672">=</span> network(image)
    t <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(z)
    counter <span style="color:#f92672">=</span> counter<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> (t <span style="color:#f92672">==</span> label) <span style="color:#66d9ef">else</span> counter

acc <span style="color:#f92672">=</span> counter <span style="color:#f92672">/</span> len(my_dataset)
<span style="color:#66d9ef">print</span>(acc)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#出力</span>
<span style="color:#ae81ff">0.3333333333333333</span>
</code></pre></div><p>損失関数と最適化関数を定義します．
損失関数は <strong>ソフトマックス交差エントロピー</strong> ，
最適化関数は <strong>Adam</strong> を採用しました．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>CrossEntropyLoss()
optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>Adam(network<span style="color:#f92672">.</span>parameters())
</code></pre></div><p>ミニバッチ学習を10エポック繰り返します．
損失の推移を可視化すると，エポックごとに損失が減少していることが確認できます．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">loss_history <span style="color:#f92672">=</span> []

<span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">10</span>):

    loss_epoch <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>

    <span style="color:#66d9ef">for</span> i, (images, labels) <span style="color:#f92672">in</span> enumerate(loader):

        optimizer<span style="color:#f92672">.</span>zero_grad()
        z <span style="color:#f92672">=</span> network(images)

        loss <span style="color:#f92672">=</span> criterion(z, labels)
        loss<span style="color:#f92672">.</span>backward()

        loss_epoch <span style="color:#f92672">+=</span> loss<span style="color:#f92672">.</span>item()

        optimizer<span style="color:#f92672">.</span>step()

    <span style="color:#66d9ef">print</span>(f<span style="color:#e6db74">&#34;{epoch} {loss_epoch / i}&#34;</span>)
    loss_history<span style="color:#f92672">.</span>append(loss_epoch <span style="color:#f92672">/</span> i)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#出力</span>
<span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0.9353420349777254</span>
<span style="color:#ae81ff">1</span> <span style="color:#ae81ff">0.8237722921065795</span>
<span style="color:#ae81ff">2</span> <span style="color:#ae81ff">0.7915834282707964</span>
<span style="color:#ae81ff">3</span> <span style="color:#ae81ff">0.7671963142024146</span>
<span style="color:#ae81ff">4</span> <span style="color:#ae81ff">0.7540506386858785</span>
<span style="color:#ae81ff">5</span> <span style="color:#ae81ff">0.7422569465433431</span>
<span style="color:#ae81ff">6</span> <span style="color:#ae81ff">0.7337749562202356</span>
<span style="color:#ae81ff">7</span> <span style="color:#ae81ff">0.7245291201477377</span>
<span style="color:#ae81ff">8</span> <span style="color:#ae81ff">0.7186577434723194</span>
<span style="color:#ae81ff">9</span> <span style="color:#ae81ff">0.7188627052510905</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">plt<span style="color:#f92672">.</span>plot(loss_history)
plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;epoch&#34;</span>)
plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;loss&#34;</span>)
</code></pre></div><p><a href="https://gyazo.com/3b6fad087fa4adef57e38843a46501cd"><img src="https://i.gyazo.com/3b6fad087fa4adef57e38843a46501cd.png" alt="Image from Gyazo"></a></p>
<p>最後に正解率を再度計算してみましょう．
あくまで学習データに対する正解率であり，評価用のデータではないことに注意）．
正解率は0.846となり，前回の多層パーセプトロンよりも，高い正解率を実現できました．</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">counter <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
<span style="color:#66d9ef">for</span> image, label <span style="color:#f92672">in</span> my_dataset:
    image <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>unsqueeze(image, <span style="color:#ae81ff">0</span>) <span style="color:#75715e">#次元を増やす</span>
    z <span style="color:#f92672">=</span> network(image)
    t <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>argmax(z)
    counter <span style="color:#f92672">=</span> counter<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span> <span style="color:#66d9ef">if</span> (t <span style="color:#f92672">==</span> label) <span style="color:#66d9ef">else</span> counter

acc <span style="color:#f92672">=</span> counter <span style="color:#f92672">/</span> len(my_dataset)
<span style="color:#66d9ef">print</span>(acc)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#出力</span>
<span style="color:#ae81ff">0.8462</span>
</code></pre></div><h3>参考書籍</h3>








<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=tf_til&t=naotoassociat-22&m=amazon&o=9&p=8&l=as1&IS2=1&detail=1&asins=B07GWHP9YM&linkId=08226226a8fb428fe7bc7d48e47ff8a0&bc1=f8f8f8&lt1=_blank&fc1=333333&lc1=0066c0&bg1=f8f8f8&f=ifr">
    </iframe>

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=tf_til&t=naotoassociat-22&m=amazon&o=9&p=8&l=as1&IS2=1&detail=1&asins=B07FVJKP1L&linkId=9f5d740609362f2de596d41b9b060b30&bc1=f8f8f8&lt1=_blank&fc1=333333&lc1=0066c0&bg1=f8f8f8&f=ifr">
    </iframe>

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=tf_til&t=naotoassociat-22&m=amazon&o=9&p=8&l=as1&IS2=1&detail=1&asins=B07VPDVNKW&linkId=ab58ead321f3b247b4b658408f167d43&bc1=f8f8f8&lt1=_blank&fc1=333333&lc1=0066c0&bg1=f8f8f8&f=ifr">
    </iframe>

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=tf_til&t=naotoassociat-22&m=amazon&o=9&p=8&l=as1&IS2=1&detail=1&asins=B07SPZ4Z82&linkId=dcc9a4aec3676eac587eb38f0412ccdd&bc1=f8f8f8&lt1=_blank&fc1=333333&lc1=0066c0&bg1=f8f8f8&f=ifr">
    </iframe>



    

    
    <div id="sns-box" style="display:flex; justify-content:center; align-items: center;">

  <a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-via="nmukai1978" data-hashtags="mlab" data-dnt="true" data-show-count="false"></a>
  <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

  <div class="fb-share-button" data-href="https://mukai-lab.info/pages/classes/advanced_studies_seminar_1/chapter6/" data-layout="button" data-size="small"><a target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fmukai-lab.info%2F&amp;src=sdkpreparse" class="fb-xfbml-parse-ignore">シェア</a></div>
    
</div>


    <div class="pagination">
      <a href="/"><span>Top</span></a>
      <a href='javascript:history.back()' style="margin-left: 30px"><span>Back</span></a>
    </div>
    
  </div>

  
  <div>

  <div style="margin-top:20px; border-top: solid 10px #007B50; border-bottom: solid 10px #007B50;">
    <p>
      愛知県名古屋市にある椙山女学園大学 文化情報学部 向研究室の公式サイトです．
      専門は情報科学であり，人工知能やデータベースなどの技術要素を指導しています．
      この公式サイトでは，授業で使用している教材を公開すると共に，
      ベールに包まれた女子大教員のミステリアスな日常を４コマ漫画でお伝えしていきます．
      サイトに関するご意見やご質問は<a href="https://www.facebook.com/nmukai1978">Facebook</a>または<a href="https://twitter.com/nmukai1978">Twitter</a>でお問い合わせください．
    </p>
  </div>  

</div>

  
  
  

    
  

  
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-77390013-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


</main>


        <footer>
  <div>
    <span>&copy; 2016 Naoto Mukai All Rights Reserved.</span> 
  </div>
</footer>


    </body>
</html>
