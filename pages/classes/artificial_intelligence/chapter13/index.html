<!DOCTYPE html>
<html lang="ja">
    <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    強化学習・TD学習 | mLAB
  </title>

  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
  <link rel="stylesheet" href="/css/custom.css">
  <link rel="stylesheet" href="/css/syntax.css">
  
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="mLAB" />

  
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Kosugi&display=swap" rel="stylesheet">

  
  <script src="https://kit.fontawesome.com/0c97f11cd6.js" crossorigin="anonymous"></script>

  
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.js"></script>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.css"/>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick-theme.min.css"/>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/SlickNav/1.0.10/jquery.slicknav.js"></script>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/SlickNav/1.0.10/slicknav.min.css" />

  
  <div id="fb-root"></div>
  <script async defer crossorigin="anonymous" src="https://connect.facebook.net/ja_JP/sdk.js#xfbml=1&version=v9.0"></script>  
  
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
	     tex2jax: {
	        inlineMath: [['$','$']]
        }
    });
  </script>

  
  

  
  <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
  
  
  

  <meta property="og:title" content="強化学習・TD学習" />



  <meta property="og:type" content="article" />



  <meta property="og:url" content="https://mukai-lab.info/pages/classes/artificial_intelligence/chapter13/" />



  <meta property="og:image" content="https://i.gyazo.com/f5b64cf98161c3aee3d50e7a033fbda7.png" />



  <meta property="og:site_name" content="mLAB" />



  <meta property="og:description" content="TD学習 TD学習（Temporal Difference Learning） は，強化学習の手法の一つであり， 前回紹介した「状態$s_x$における価値$V(s_x)$」の更新式を一般化したものです．
前回の更新式を振り返りましょう． 次の式は，状態$s_x$で報酬$r$を得られたときの更新式です． この式は状態の遷移を考慮していません． このため，ランダムに状態$s_x$が選択されるときに適用します．
$$ V&rsquo;(s_x) = (1 - \alpha) \cdot V(s_x) &#43; \alpha \cdot r $$
次の式は，状態$s_x$から状態$s_y$に遷移したときの更新式です． 将来の状態$s_y$の価値を基に，現在の状態$s_x$の価値を算出しています（ブートストラップ型学習）． このため，$s_x \rightarrow s_y$のように状態遷移が生じるときに適用します
$$ V&rsquo;(s_x) = (1 - \alpha) \cdot V(s_x) &#43; \alpha \cdot V(s_y) $$
一方，TD学習 は次の式で定義されます． 現在の状態$s_x$で得た報酬$r$と，将来の状態$s_y$の価値を組み合わせて， 現在の状態$s_x$の価値を算出しています． ここで，$\gamma$ は割引率と呼ばれ，将来に得られる価値を割り引いて加算するために用いられます（将来得られる価値は確実ではないため）． この式は，$s_x \rightarrow s_y \rightarrow, \cdots, \rightarrow s_z$のように状態遷移が繰り返されるときに適用します．
$$ V&rsquo;(s_x) = (1 - \alpha) \cdot V(s_x) &#43; \alpha (r &#43; \gamma \cdot V(s_y)) $$" />



  
  
<meta name="twitter:card" content="summary" />

<meta name="twitter:site" content="@nmukai1978" />


  <meta name="twitter:title" content="強化学習・TD学習" />



  <meta name="twitter:description" content="TD学習 TD学習（Temporal Difference Learning） は，強化学習の手法の一つであり， 前回紹介した「状態$s_x$における価値$V(s_x)$」の更新式を一般化したものです．
前回の更新式を振り返りましょう． 次の式は，状態$s_x$で報酬$r$を得られたときの更新式です． この式は状態の遷移を考慮していません． このため，ランダムに状態$s_x$が選択されるときに適用します．
$$ V&rsquo;(s_x) = (1 - \alpha) \cdot V(s_x) &#43; \alpha \cdot r $$
次の式は，状態$s_x$から状態$s_y$に遷移したときの更新式です． 将来の状態$s_y$の価値を基に，現在の状態$s_x$の価値を算出しています（ブートストラップ型学習）． このため，$s_x \rightarrow s_y$のように状態遷移が生じるときに適用します
$$ V&rsquo;(s_x) = (1 - \alpha) \cdot V(s_x) &#43; \alpha \cdot V(s_y) $$
一方，TD学習 は次の式で定義されます． 現在の状態$s_x$で得た報酬$r$と，将来の状態$s_y$の価値を組み合わせて， 現在の状態$s_x$の価値を算出しています． ここで，$\gamma$ は割引率と呼ばれ，将来に得られる価値を割り引いて加算するために用いられます（将来得られる価値は確実ではないため）． この式は，$s_x \rightarrow s_y \rightarrow, \cdots, \rightarrow s_z$のように状態遷移が繰り返されるときに適用します．
$$ V&rsquo;(s_x) = (1 - \alpha) \cdot V(s_x) &#43; \alpha (r &#43; \gamma \cdot V(s_y)) $$" />



  <meta name="twitter:image" content="https://i.gyazo.com/f5b64cf98161c3aee3d50e7a033fbda7.png" />


  
</head>

    <body>
        <nav class="nav">
  <div class="nav-container">
    
    <a href="/">
    <h2 class="nav-title">
      <img src="https://mukai-lab.info/favicon/favicon-48x48.png" align="top"/>
      <span>mLAB</span>
      

<ul id="menu">
  
  
  <li>
    <a href="/">Top</a>
  </li>
  
  
  
  <li>
    <a href="/classes/">Classes</a>
  </li>
  
  
  
  <li>
    <a href="/posts/">Comics</a>
  </li>
  
  
</ul>

<script>
  $(function(){
    $("#menu").slicknav({
      label: "メニュー"
    });
  });
</script>



    </h2>
    </a>
  </div>
</nav>

        <div id="darkModeToggle" onclick="toggleDarkMode()">
  &#9680; 
</div>

        

<main>
  <div class="post">
    
    <h1 class="post-title">強化学習・TD学習</h1>
<div class="post-line"></div>
    
    

    <p><a href="https://gyazo.com/f5b64cf98161c3aee3d50e7a033fbda7"><img src="https://i.gyazo.com/f5b64cf98161c3aee3d50e7a033fbda7.png" alt="Image from Gyazo"></a></p>
<h1 style="padding-left:40px; line-height: 30px; background: url(http://mukai-lab.info/logo/logo.png) no-repeat">
  TD学習
</h1>

<p><strong>TD学習（Temporal Difference Learning）</strong> は，強化学習の手法の一つであり，
前回紹介した「状態$s_x$における価値$V(s_x)$」の更新式を一般化したものです．</p>
<p>前回の更新式を振り返りましょう．
次の式は，状態$s_x$で報酬$r$を得られたときの更新式です．
この式は状態の遷移を考慮していません．
このため，ランダムに状態$s_x$が選択されるときに適用します．</p>
<p>$$
V&rsquo;(s_x) = (1 - \alpha) \cdot V(s_x) + \alpha \cdot r
$$</p>
<p>次の式は，状態$s_x$から状態$s_y$に遷移したときの更新式です．
将来の状態$s_y$の価値を基に，現在の状態$s_x$の価値を算出しています（ブートストラップ型学習）．
このため，$s_x \rightarrow s_y$のように状態遷移が生じるときに適用します</p>
<p>$$
V&rsquo;(s_x) = (1 - \alpha) \cdot V(s_x) + \alpha \cdot V(s_y)
$$</p>
<p>一方，<strong>TD学習</strong> は次の式で定義されます．
現在の状態$s_x$で得た報酬$r$と，将来の状態$s_y$の価値を組み合わせて，
現在の状態$s_x$の価値を算出しています．
ここで，$\gamma$ は割引率と呼ばれ，将来に得られる価値を割り引いて加算するために用いられます（将来得られる価値は確実ではないため）．
この式は，$s_x \rightarrow s_y \rightarrow, \cdots,  \rightarrow s_z$のように状態遷移が繰り返されるときに適用します．</p>
<p>$$
V&rsquo;(s_x) = (1 - \alpha) \cdot V(s_x) + \alpha (r + \gamma \cdot V(s_y))
$$</p>
<h1 style="padding-left:40px; line-height: 30px; background: url(http://mukai-lab.info/logo/logo.png) no-repeat">
  ノートブックの作成
</h1>

<p><strong>Google Colaboratory</strong> を起動し，新規にノートブックを作成してください．
ノートブックのタイトルは <strong>AI-13</strong> とします．
ノートブックの作成方法は<a href="https://mukai-lab.info/pages/classes/artificial_intelligence/chapter1/">第1回の資料</a>を参照してください．</p>
<p>ネットワーク分析のためのライブラリ<a href="https://networkx.org/documentation/stable/index.html">NetworkX</a>をインストールします．</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010">!</span>pip install network
</span></span></code></pre></div><h1 style="padding-left:40px; line-height: 30px; background: url(http://mukai-lab.info/logo/logo.png) no-repeat">
  宝探しゲーム
</h1>

<p>宝探しゲームを題材に， <strong>TD学習</strong> の振る舞いを確認します．</p>
<div style="border:3px solid black; border-radius: 10px; padding-left: 10px; padding-right: 10px; margin-bottom:20px">
  
  <h3 style="color:#006600">
    宝探しゲーム
  </h3>

  <p>
    

エージェントが迷路の中で宝探しをする．
状態$s_A$からスタートし，遷移可能な状態を辿る．
エージェントは，元の状態に戻ることはせず，遷移可能な状態がなければ状態$s_A$に戻る．
状態$S_H$に到着すると報酬$+100$，状態$S_F$に到着すると報酬$+30$を獲得する．
このとき，各状態の価値（報酬の期待値）を求めよ．

<a href="https://gyazo.com/f3a91fcaef4118c28d115fd62c5ff272"><img src="https://i.gyazo.com/f3a91fcaef4118c28d115fd62c5ff272.png" alt="Image from Gyazo" width="446"/></a>


  </p>
  
</div>

<h1 style="padding-left:40px; line-height: 30px; background: url(http://mukai-lab.info/logo/logo.png) no-repeat">
  実装
</h1>

<p>ネットワーク（グラフ）を分析する<code>networkx</code>と，乱数を生成する<code>random</code>をインポートします．</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> networkx <span style="color:#66d9ef">as</span> nx
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> random
</span></span></code></pre></div><p>対象のネットワーク（グラフ）を作成します．
ノードは状態を表し$s_A, s_B, \cdots, s_J$の10種類が存在します．
エッジは遷移可能な状態を表しています．
例えば，状態$s_A$からは，$s_B$と$s_C$に遷移が可能です．</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># グラフの初期化</span>
</span></span><span style="display:flex;"><span>graph <span style="color:#f92672">=</span> nx<span style="color:#f92672">.</span>Graph()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># ノード</span>
</span></span><span style="display:flex;"><span>nodes <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;A&#34;</span>, <span style="color:#e6db74">&#34;B&#34;</span>, <span style="color:#e6db74">&#34;C&#34;</span>, <span style="color:#e6db74">&#34;D&#34;</span>, <span style="color:#e6db74">&#34;E&#34;</span>, <span style="color:#e6db74">&#34;F&#34;</span>, <span style="color:#e6db74">&#34;G&#34;</span>, <span style="color:#e6db74">&#34;H&#34;</span>, <span style="color:#e6db74">&#34;I&#34;</span>, <span style="color:#e6db74">&#34;J&#34;</span>]
</span></span><span style="display:flex;"><span>graph<span style="color:#f92672">.</span>add_nodes_from(nodes)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># エッジ</span>
</span></span><span style="display:flex;"><span>edges <span style="color:#f92672">=</span> [(<span style="color:#e6db74">&#34;A&#34;</span>, <span style="color:#e6db74">&#34;B&#34;</span>), (<span style="color:#e6db74">&#34;A&#34;</span>, <span style="color:#e6db74">&#34;C&#34;</span>), (<span style="color:#e6db74">&#34;B&#34;</span>, <span style="color:#e6db74">&#34;D&#34;</span>), (<span style="color:#e6db74">&#34;B&#34;</span>, <span style="color:#e6db74">&#34;E&#34;</span>), (<span style="color:#e6db74">&#34;C&#34;</span>, <span style="color:#e6db74">&#34;F&#34;</span>), (<span style="color:#e6db74">&#34;C&#34;</span>, <span style="color:#e6db74">&#34;G&#34;</span>), (<span style="color:#e6db74">&#34;D&#34;</span>, <span style="color:#e6db74">&#34;H&#34;</span>), (<span style="color:#e6db74">&#34;D&#34;</span>, <span style="color:#e6db74">&#34;I&#34;</span>), (<span style="color:#e6db74">&#34;E&#34;</span>, <span style="color:#e6db74">&#34;J&#34;</span>)]
</span></span><span style="display:flex;"><span>graph<span style="color:#f92672">.</span>add_edges_from(edges)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 描画</span>
</span></span><span style="display:flex;"><span>nx<span style="color:#f92672">.</span>draw(graph, with_labels<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p><a href="https://gyazo.com/a9791034f9b822e94ef25325d2380939"><img src="https://i.gyazo.com/a9791034f9b822e94ef25325d2380939.png" alt="Image from Gyazo"></a></p>
<p>特定の状態に到達したときに得られる報酬を定義します．
状態$s_H$に到達したときに報酬$+100$，状態$s_F$に到達したときに報酬$+30$を獲得します．</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 報酬</span>
</span></span><span style="display:flex;"><span>rewards <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;A&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;B&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;C&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;D&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;E&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;F&#34;</span>: <span style="color:#ae81ff">30</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;G&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;H&#34;</span>: <span style="color:#ae81ff">100</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;I&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;J&#34;</span>: <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>ノード間の状態遷移を<code>move()</code>として定義します．
遷移可能な状態が複数ある場合は<code>random.choice()</code>でランダムに一つを選択します．
遷移可能な状態が存在しない$s_J$，$s_H$，$s_I$，$s_F$，$s_G$に到達した場合は，
状態$s_A$に戻ることにします．</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">move</span>(state):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># 現在の状態から選択可能なエッジ</span>
</span></span><span style="display:flex;"><span>  candidates <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> edge <span style="color:#f92672">in</span> edges:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> edge[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> state:
</span></span><span style="display:flex;"><span>      candidates<span style="color:#f92672">.</span>append(edge)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span>(len(candidates) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 候補がなければ初期状態に戻る</span>
</span></span><span style="display:flex;"><span>    next_state <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;A&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># ランダムに次の状態を選択</span>
</span></span><span style="display:flex;"><span>    next_state <span style="color:#f92672">=</span> random<span style="color:#f92672">.</span>choice(candidates)[<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># 得られる報酬</span>
</span></span><span style="display:flex;"><span>  reward <span style="color:#f92672">=</span> rewards[state]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> (next_state, reward)
</span></span></code></pre></div><p>状態$s_A$をスタートとして，10回の状態遷移を繰り返してみます．
この結果，次のように状態遷移を実行したことがわかります．</p>
<p>$$
A \rightarrow C \rightarrow G
$$</p>
<p>$$
A \rightarrow C \rightarrow F (+30)
$$</p>
<p>$$
A \rightarrow B \rightarrow D \rightarrow H (+100)
$$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>state <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;A&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">10</span>):
</span></span><span style="display:flex;"><span>  next_state, reward <span style="color:#f92672">=</span> move(state)
</span></span><span style="display:flex;"><span>  print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;</span><span style="color:#e6db74">{</span>state<span style="color:#e6db74">}</span><span style="color:#e6db74"> -&gt; </span><span style="color:#e6db74">{</span>next_state<span style="color:#e6db74">}</span><span style="color:#e6db74"> reward=</span><span style="color:#e6db74">{</span>reward<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span><span style="display:flex;"><span>  state <span style="color:#f92672">=</span> next_state
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>A <span style="color:#f92672">-&gt;</span> C reward<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>C <span style="color:#f92672">-&gt;</span> G reward<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>G <span style="color:#f92672">-&gt;</span> A reward<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">-&gt;</span> C reward<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>C <span style="color:#f92672">-&gt;</span> F reward<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>F <span style="color:#f92672">-&gt;</span> A reward<span style="color:#f92672">=</span><span style="color:#ae81ff">30</span>
</span></span><span style="display:flex;"><span>A <span style="color:#f92672">-&gt;</span> B reward<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>B <span style="color:#f92672">-&gt;</span> D reward<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>D <span style="color:#f92672">-&gt;</span> H reward<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>H <span style="color:#f92672">-&gt;</span> A reward<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>
</span></span></code></pre></div><p>それでは，<strong>TD学習</strong> で状態$s_x$の価値$V(s_x)$を算出してみましょう．
まずは，各状態の価値を0で初期化します．</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 価値</span>
</span></span><span style="display:flex;"><span>values <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;A&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;B&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;C&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;D&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;E&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;F&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;G&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;H&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;I&#34;</span>: <span style="color:#ae81ff">0</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;J&#34;</span>: <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>状態遷移を10,000回繰り返し，TD学習で状態$s_x$の価値$V(s_x)$を推定します．
このとき，学習率は$0.01$，割引率は$0.9$に設定しています．
また，価値がループして伝搬することを防ぐため，
状態$s_A$の価値$V(s_A)$は$0$とみなすことにしています．</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>alpha <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.01</span>
</span></span><span style="display:flex;"><span>gamma <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.9</span>
</span></span><span style="display:flex;"><span>state <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;A&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">10000</span>):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  next_state, reward <span style="color:#f92672">=</span> move(state)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># 次の状態の価値</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">if</span> next_state <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;A&#34;</span>:
</span></span><span style="display:flex;"><span>    next_value <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>    next_value <span style="color:#f92672">=</span> values[next_state]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># TD学習</span>
</span></span><span style="display:flex;"><span>  values[state] <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> alpha) <span style="color:#f92672">*</span> values[state] <span style="color:#f92672">+</span> alpha <span style="color:#f92672">*</span> (reward  <span style="color:#f92672">+</span> gamma <span style="color:#f92672">*</span> next_value)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># 状態の更新</span>
</span></span><span style="display:flex;"><span>  state <span style="color:#f92672">=</span> next_state
</span></span></code></pre></div><p>この結果，状態$s_x$の価値$V(s_x)$は次のように算出されました．
ここで，状態の価値に応じて，遷移する状態を選択することを考えます．</p>
<ul>
<li>$s_A$では，$s_C=11.6$より，$s_B=18.4$の方が価値が高いため，状態$s_B$を選択</li>
<li>$s_B$では，$s_E=0.0$より，$s_D=37.4$の方が価値が高いため，状態$s_D$を選択</li>
<li>$s_D$では，$s_I=0.0$より，$s_H=97.0$の方が価値が高いため，状態$s_H$を選択</li>
</ul>
<p>よって，下記の状態遷移が最適であることがわかります．</p>
<p>$$
A \rightarrow B \rightarrow D \rightarrow H (+100)
$$</p>
<p>このように，TD学習で状態の価値を定めることで，エージェントの最適な方策（状態遷移の選択方法）を得ることができます．</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> state <span style="color:#f92672">in</span> values:
</span></span><span style="display:flex;"><span>  print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;state=</span><span style="color:#e6db74">{</span>state<span style="color:#e6db74">}</span><span style="color:#e6db74"> value=</span><span style="color:#e6db74">{</span>values[state]<span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>state<span style="color:#f92672">=</span>A value<span style="color:#f92672">=</span><span style="color:#ae81ff">13.988914011222555</span>
</span></span><span style="display:flex;"><span>state<span style="color:#f92672">=</span>B value<span style="color:#f92672">=</span><span style="color:#ae81ff">18.4207396271639</span>
</span></span><span style="display:flex;"><span>state<span style="color:#f92672">=</span>C value<span style="color:#f92672">=</span><span style="color:#ae81ff">11.640561941925904</span>
</span></span><span style="display:flex;"><span>state<span style="color:#f92672">=</span>D value<span style="color:#f92672">=</span><span style="color:#ae81ff">37.423600796056085</span>
</span></span><span style="display:flex;"><span>state<span style="color:#f92672">=</span>E value<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>state<span style="color:#f92672">=</span>F value<span style="color:#f92672">=</span><span style="color:#ae81ff">29.961695017091582</span>
</span></span><span style="display:flex;"><span>state<span style="color:#f92672">=</span>G value<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>state<span style="color:#f92672">=</span>H value<span style="color:#f92672">=</span><span style="color:#ae81ff">96.97275395867995</span>
</span></span><span style="display:flex;"><span>state<span style="color:#f92672">=</span>I value<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>state<span style="color:#f92672">=</span>J value<span style="color:#f92672">=</span><span style="color:#ae81ff">0.0</span>
</span></span></code></pre></div><!--
<h1 style="padding-left:40px; line-height: 30px; background: url(http://mukai-lab.info/logo/logo.png) no-repeat">
  完成したノートブックの確認
</h1>


[<i class="fas fa-external-link-alt"></i>ノートブックの確認](https://colab.research.google.com/drive/1NP7JG09vdePNyCOxYF7DSmy8TELeMwuU?usp=sharing)
-->
<h1 style="padding-left:40px; line-height: 30px; background: url(http://mukai-lab.info/logo/logo.png) no-repeat">
  課題
</h1>

<p>状態$s_H$の報酬（ペナルティ）を$-100$に設定する．
このとき，$V(s_x)$はどのような値になるか示せ．</p>
<p>Google Colaboratoryで作成した <strong>AI-13.ipynb</strong> を保存し，
<strong>ノートブック（.ipynb）</strong> をダウンロードして提出しなさい．
提出の前に必ず下記の設定を行うこと．</p>
<ul>
<li>ノートブックの設定で「セルの出力を除外する」のチェックを外す</li>
<li>ノートブックの変更内容を保存して固定</li>
</ul>

<h3>参考書籍</h3>

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=tf_til&t=naotoassociat-22&m=amazon&o=9&p=8&l=as1&IS2=1&detail=1&asins=477416013X&linkId=a3e3de6cdd05f8a87e01091a9529a212&bc1=f8f8f8&lt1=_blank&fc1=333333&lc1=0066c0&bg1=f8f8f8&f=ifr">
    </iframe>

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=tf_til&t=naotoassociat-22&m=amazon&o=9&p=8&l=as1&IS2=1&detail=1&asins=4061538233&linkId=9b22cb2b9a07a725897b3598e4bb4418&bc1=f8f8f8&lt1=_blank&fc1=333333&lc1=0066c0&bg1=f8f8f8&f=ifr">
</iframe>

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=tf_til&t=naotoassociat-22&m=amazon&o=9&p=8&l=as1&IS2=1&detail=1&asins=4339023957&linkId=8f5ccc75e9cf345c48d02210a3230069&bc1=f8f8f8&lt1=_blank&fc1=333333&lc1=0066c0&bg1=f8f8f8&f=ifr">
</iframe>

<iframe style="width:120px;height:240px;" marginwidth="0" marginheight="0" scrolling="no" frameborder="0" src="https://rcm-fe.amazon-adsystem.com/e/cm?ref=tf_til&t=naotoassociat-22&m=amazon&o=9&p=8&l=as1&IS2=1&detail=1&asins=427422371X&linkId=d5436c8655f90c573302f07ed9cb9ddf&bc1=f8f8f8&lt1=_blank&fc1=333333&lc1=0066c0&bg1=f8f8f8&f=ifr">
    </iframe>



    

    
    <div id="sns-box" style="display:flex; justify-content:center; align-items: center;">

  <a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-via="nmukai1978" data-hashtags="mlab" data-dnt="true" data-show-count="false"></a>
  <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

  <div class="fb-share-button" data-href="https://mukai-lab.info/pages/classes/artificial_intelligence/chapter13/" data-layout="button" data-size="small"><a target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fmukai-lab.info%2F&amp;src=sdkpreparse" class="fb-xfbml-parse-ignore">シェア</a></div>
    
</div>


    <div class="pagination">
      <a href="/"><span>Top</span></a>
      <a href='javascript:history.back()' style="margin-left: 30px"><span>Back</span></a>
    </div>
    
  </div>

  
  <div>

  <div style="margin-top:20px; border-top: solid 10px #007B50; border-bottom: solid 10px #007B50;">
    <p>
      愛知県名古屋市にある椙山女学園大学 文化情報学部 向研究室の公式サイトです．
      専門は情報科学であり，人工知能やデータベースなどの技術要素を指導しています．
      この公式サイトでは，授業で使用している教材を公開すると共に，
      ベールに包まれた女子大教員のミステリアスな日常を４コマ漫画でお伝えしていきます．
      サイトに関するご意見やご質問は<a href="https://www.facebook.com/nmukai1978">Facebook</a>または<a href="https://twitter.com/nmukai1978">Twitter</a>でお問い合わせください．
    </p>
  </div>  

</div>

  
  
  

    
  

  
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-77390013-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


</main>


        <footer>
  <div>
    <span>&copy; 2016 Naoto Mukai All Rights Reserved.</span> 
  </div>
</footer>


    </body>
</html>
