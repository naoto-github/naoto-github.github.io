<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tech on mLAB</title>
    <link>https://mukai-lab.info/categories/tech/</link>
    <description>Recent content in Tech on mLAB</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Mon, 11 Jan 2021 12:22:29 +0900</lastBuildDate>
    
	<atom:link href="https://mukai-lab.info/categories/tech/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Githubの基本的な使い方</title>
      <link>https://mukai-lab.info/pages/tech/github/github-usage/</link>
      <pubDate>Mon, 11 Jan 2021 12:22:29 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/github/github-usage/</guid>
      <description>GitHubとは GitHubはバージョン管理システムGitのホスティングサービスです．GitHubでソースコードを管理・共有することで，他の開発者と一緒にコーディングをすることが可能になります．オープンソースのプロジェクトなどはGitHubを利用して開発されることが殆どです．ここでは，GitHubの基本的な使い方を解説します．
GitHubを利用する準備 アカウントの作成 公式サイトでGitHubのアカウントを作成してください．一般の無料アカウントでは，誰もが閲覧可能な パブリック・レポジトリ しか作成できませんが，学生や教員はGitHub Educationに登録が可能で，無料で プライベート・レポジトリ を作成できます．下記の記事を参考にGitHub Educationに登録しておきましょう．
【学割】GitHub Educationの申請とPrivateリポジトリを無料で使う方法
GitHub Desktopのインストール GitHubの操作にはクライアント・ソフトウェアを利用します．どのソフトウェアを利用しても構いませんが，GitHub公式のGitHub Desktopがオススメです．向はCUIのGit for Windowsを利用しています．
 GitHub Desktop Sourcetree Git for Windows GitKraken  GitHub Dektopをインストールしたら，GitHubのアカウントでサインインします．

パブリック・レポジトリの作成 GitHubのウェブサイトにアクセスし，ウェブ上にパブリック・レポジトリを作成します．

レポジトリの名前は TestRepository とします． レポジトリで管理するファイルなどの説明を記述するための README ファイルの作成もチェックしておきます．

レポジトリの基本操作 レポジトリをローカルPCにクローン GitHub Desktopで，ウェブ上に作成したレポジトリをローカルPCに Clone（クローン） します． クローンとはファイルをコピーして複製するという意味です． レポジトリのクローンを保存するためのフォルダをローカルPCに作成してください． フォルダ名はレポジトリと同じ TestRepository としておきましょう．



READMEを上書き レポジトリをクローンしたフォルダに README.md が存在しています． このファイルは作成したレポジトリの説明書であり，ウェブサイトのトップ画面に自動的に表示されます． 開発するソフトウェアの名称や使い方などを記述する目的で用いられます． メモ帳などのエディタでファイルを開き，下記のように作成者を追加して保存してください（公開されてしまうので ニックネーム でOK）． 拡張子は md となっており，手軽にウェブページを記述できるMarkdown（マークダウン）記法で記述することを表しています． マークダウン記法はとても便利なので使いこなせるようにしておきましょう．
# TestRepository テスト用のレポジトリです． ## 作成者 向 直人 README.</description>
    </item>
    
    <item>
      <title>ssh-Agentでパスフレーズの省略</title>
      <link>https://mukai-lab.info/pages/tech/github/ssh-agent/</link>
      <pubDate>Sun, 28 Jun 2020 11:17:00 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/github/ssh-agent/</guid>
      <description>Git for Windowsで， pullやpushするときに毎回パスワードを入力する手間を省略する方法です．
ホームディレクトリに~/.bashrcを作成し，下記のコマンドを入力します． この.bashrcはGit for Windowsのデフォルトのシェルである bash が起動時に 自動で読み込むファイルです．
eval `ssh-agent` ssh-add ここで，evalコマンドは引数の文字列をコマンドとして実行します． また，ssh-agentを起動しておき，ssh-addでパスフレーズを登録します．</description>
    </item>
    
    <item>
      <title>Vue.jsを利用した地図アプリ（後編）</title>
      <link>https://mukai-lab.info/pages/tech/vue/vue4/</link>
      <pubDate>Wed, 24 Jun 2020 16:03:21 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/vue/vue4/</guid>
      <description>親子コンポーネントのイベント処理  地図アプリに配置した上下左右のボタンのイベント処理を実装します． このアプリは3つのコンポーネントで構成されています． 親コンポーネントは main.vue ，子コンポーネントは vmap.vue と vcontroller.vue です． 親子コンポーネント間でメソッドを呼び出したり，データを受け渡すには幾つか方法がありますが， 子から親 ，親から子 ではその方法が異なります． 今回の地図アプリでは下記の流れでイベント処理を行います．
 vcontroller.vue（子） のボタンをクリックすると，main.vue（親） のmoveメソッドを呼び出す（子から親）． main.vue（親） のmoveメソッドが呼び出されると，vmap.vue（子） のmoveメソッドを呼び出す（親から子）．  
子から親のメソッドを実行  子コンポーネントから親コンポーネントのメソッドを呼び出すには$emitを利用します．
main.vue（親） 親コンポーネントで moveメソッドを定義します． このmoveメソッドはアラートを表示するだけです． このメソッドを v-onディレクティブを利用して，vcontorllerタグの属性として指定します． このとき，move-eventという属性名になっていることに注意してください．
&amp;lt;template&amp;gt; &amp;lt;div&amp;gt; &amp;lt;h1&amp;gt;Vue.jsで地図アプリ&amp;lt;/h1&amp;gt; &amp;lt;vmap&amp;gt;&amp;lt;/vmap&amp;gt; &amp;lt;vcontroller v-on:move-event=&amp;#34;move&amp;#34;&amp;gt;&amp;lt;/vcontroller&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/template&amp;gt; &amp;lt;script&amp;gt; import vmap from &amp;#34;./vmap&amp;#34; import vcontroller from &amp;#34;./vcontroller&amp;#34; export default{ components:{ vmap, vcontroller }, methods:{ move(direction){ // アラートを表示  alert(direction); } } } &amp;lt;/script&amp;gt; vcontroller.vue（子） 子コンポーネントでも moveメソッドを定義しています． このmoveメソッドは，$emitを利用してmove-eventとして登録されている親コンポーネントの moveメソッドを呼び出します． また，moveメソッドは，v-onディレクティブを利用して，ボタンのクリックがトリガーとなっています． このとき，メソッドの引数としてleft, up, right, downのいずれかの文字列が渡されることに注意してください．</description>
    </item>
    
    <item>
      <title>Vue.jsを利用した地図アプリ（前編）</title>
      <link>https://mukai-lab.info/pages/tech/vue/vue3/</link>
      <pubDate>Wed, 24 Jun 2020 16:03:19 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/vue/vue3/</guid>
      <description>Vue CLIとは  Vue.jsを利用した本格的なアプリ開発のためのツールとしてVue CLIが公開されています． Vue CLIはNode.jsにおいて動作し， テンプレートを利用したプロジェクトの作成や，プラグインによる拡張などが利用可能です． ここではWindows環境を想定して，Vue CLIやNode.jsのインストール方法を示します．
まずは，Node.jsをインストールしましょう． パッケージマネージャのChocolateyを利用すると簡単です． PowerShellなどを起動して下記のコマンドを入力します．
&amp;gt; choco install nodejs Node.jsをインストールしたら，次はVue CLIをインストールします． 同様にPowerShellで下記のコマンドを入力します． この-gオプションはグローバルにインストールすることを意味しており， どのディレクトリ（フォルダ）でも利用が可能になります（ローカルのnode_modulesにはインストールされない）．
&amp;gt; npm install -g @vue/cli &amp;gt; npm install -g @vue/cli-service-global Vue CLIをインストールしたら，vueコマンドを実行してみましょう． -Vオプションでバージョンを確認することができます．
&amp;gt; vue -V @vue/cli 4.4.5 Vue CLIを利用したプロジェクトのビルド（公開可能なファイルに変換する）に失敗する場合， フォルダに下記のコードを記述した vue.config.js というファイルを配置してください（ビルド方法は後述）．
module.exports = { publicPath: &amp;#39;./&amp;#39; } 単一ファイルコンポーネントとは  Vue Cliでは，これまでのように，HTMLファイル（.html），JavaScriptファイル（.js），CSSファイル（.css）にコードを記述しません． これらの代わりに，全ての機能をまとめた単一ファイルコンポーネント（.vue）にコードを記述します． 単一ファイルコンポーネントは下記の構成になっています．
 templateタグはHTMLで記述し，Vueコンポーネントのtemplateオプションとして用いられます． scriptタグはJavaScriptで記述し，Vueコンポーネントとして利用されます（外部から利用可能になる）． styleタグはCSSで記述し，scpedを指定することで，templateタグの内部に限定してスタイルが適用されます．  &amp;lt;template&amp;gt; &amp;lt;/tempalte&amp;gt; &amp;lt;script&amp;gt; export default{ } &amp;lt;/script&amp;gt; &amp;lt;style scoped&amp;gt; &amp;lt;/style&amp;gt; 単一ファイルコンポーネントは階層的に 親コンポーネントから子コンポーネントをインポートすることができます． 例えば，child1.</description>
    </item>
    
    <item>
      <title>Vue.jsを利用したクイズアプリ</title>
      <link>https://mukai-lab.info/pages/tech/vue/vue2/</link>
      <pubDate>Tue, 23 Jun 2020 15:13:21 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/vue/vue2/</guid>
      <description>ルーティングとは  ルーティング（Routing） とはVue.jsにおいて， リクエストされたURLに応じて，コンポーネントを選択して表示する仕組みのことです． シングルページアプリケーション（Single Page Application: SPA） と呼ばれる 単一のWebページで動的にコンテンツを切り替える設計方法を実現するために用いられます． Webページの遷移が発生しないため，サーバとの通信を減らし高速なコンテンツの遷移が可能になることに加え， ネイティブアプリ（JavaやSwiftで開発されたアプリ）の代替としても採用されます． 今回はこのルーティングを利用してクイズアプリを実装してみましょう．
クイズアプリ  それでは， クイズアプリ を実装して行きましょう． 下記が完成したソースコードです． 「問題」をクリックするとクイズの問題が表示され， 「解答」をクリックすると対応するクイズの解答が表示されます．
See the Pen クイズアプリ by Naoto Mukai (@nmukai)on CodePen.CDNのリンク ルーティングを実装するには，vue.js に加え， vue-route.js を導入する必要があります． CodePenのSettingsで下記のようにCDNを追加してください．

ルータの作成 それではルーティングの役割を担う VueRouter クラスのインスタンスをを作成しましょう． このルータには，2つのコンポーネント（component）を登録します． コンポーネントの実体は，変数のQuizBlockとAnsBlockで宣言しています． それぞれのコンポーネントには，pathとnameのプロパティを設定します． pathはリクエストされるURL，nameはルートの名前を表します． また，Vue クラスに router をプロパティとして登録しておきます．
var QuizBlock = { template: &amp;#34;&amp;lt;div&amp;gt; question &amp;lt;/div&amp;gt;&amp;#34; } var AnsBlock = { template: &amp;#34;&amp;lt;div&amp;gt; answer &amp;lt;/div&amp;gt;&amp;#34; } var router = new VueRouter({ routes: [ { path: &amp;#34;/quiz&amp;#34;, name: &amp;#34;quiz&amp;#34;, component: QuizBlock }, { path: &amp;#34;/answer&amp;#34;, name: &amp;#34;answer&amp;#34;, component: AnsBlock } ] }) new Vue({ el: &amp;#34;#main&amp;#34;, router }) ルーティングを利用するにはrouter-linkタグを利用します． to属性は表示したいコンポーネントのpathを指定します． router-linkタグは，リンクを表すaタグとして表示されます． また，router-viewタグは，コンポーネントを表示する領域です．</description>
    </item>
    
    <item>
      <title>Vue.jsを利用したサイコロアプリ</title>
      <link>https://mukai-lab.info/pages/tech/vue/vue1/</link>
      <pubDate>Mon, 22 Jun 2020 16:52:17 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/vue/vue1/</guid>
      <description>コンポーネントとは  コンポーネント（Component） とはVue.jsでページを構成する部品のことです． ページの部分的な機能をコンポーネントとして実装することで，ページ内に複数配置したり再利用が可能になります． 今回は サイコロの目 をコンポーネントとして実装し，1から6の目を配置します．
サイコロアプリ  それでは サイコロアプリ を実装して行きましょう． 下記が完成したソースコードです． 「サイコロを振る」というボタンをクリックすると，1から6のいずれかの目が緑色になります．
See the Pen NWxpRYa by Naoto Mukai (@nmukai)on CodePen.コンポーネントの作成 サイコロの目は divタグで実装します． まずは，divタグをデザインするための CSS を下記のように記述します． pipStyleとpipFocusの2つのクラスがあり，これをdivタグに適用します．
.pipStyle{ width: 50px; height: 50px; border: 1px solid black; text-align: center; display: table-cell; vertical-align: middle; } .pipFocus{ background: #00ff00; } div{ margin: 5px; } pip という名前でコンポーネントを実装します． この名前はHTMLでタグ名として用いられます． オプションには props と template を設定しています． props はpipタグの属性として渡されるデータを指しています． この場合，渡されたデータはflagという変数に格納されます． templateはコンポーネントをHTMLタグとして記述したときの実体であり， ここではサイコロのマス目を表すdiv要素です．</description>
    </item>
    
    <item>
      <title>ジャパンサーチからSPARQLでデータ取得</title>
      <link>https://mukai-lab.info/pages/tech/japan_search/japan_search/</link>
      <pubDate>Tue, 15 Oct 2019 19:30:30 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/japan_search/japan_search/</guid>
      <description>ジャパンサーチとは  ジャパンサーチは，様々な分野のデジタルアーカイブと連携することで， 日本が保有するコンテンツを横断的に検索することができるポータルサイトのことです（現在はベータ版が公開）． コンテンツには，作品名，分類，品質形状，時代世紀，所蔵者などのメタデータ付与されており， 地方活性化，教育研究など様々な目的で活用することが可能となっています． 2019年10月現在においては，連携データベース数は50，また， 登録されているメタデータ件数（コンテンツ数）は17,959,770となっており， 今後も登録数は増加していくと考えられます．
それでは，「名古屋城図」をキーワードに検索してみましょう． すると，国立国会図書館デジタルコレクションに所属される 名古屋城図がヒットします． このデジタル化された地図は パブリックドメイン であり，著作権による制限を受けず，自由に利用することができます． また，メタデータとして，下記に示すような情報が提供されます（一部のみを掲載）．
   項目 値     名称/タイトル 名古屋城図[1]   名称/タイトルヨミ ナゴヤジョウズ   資料種別 和古書   著作権情報 インターネット公開(保護期間満了)   件名 城郭/尾張国/名古屋   提供者のURL http://dl.ndl.go.jp/info:ndljp/pid/2589695   サムネイル画像URL http://dl.ndl.go.jp/titleThumb/info:ndljp/pid/2589695    
これらのレコード（メタデータ）を取得するには下記の2通りの方法があります．
 簡易Web API SPARQL API  簡易Web API は，登録されているレコードを検索するための簡易的な機能です． プログラミングの必要はなく，ブラウザで指定する URL に一工夫するだけで検索が可能です． 一方，SPARQL APIは，問い合わせ言語であるSPARQL（スパークル）を利用して， 様々な条件を指定してレコードを取得する機能です．</description>
    </item>
    
    <item>
      <title>BITalinoで筋電位センサー</title>
      <link>https://mukai-lab.info/pages/tech/bitalino/bitalino2/</link>
      <pubDate>Tue, 20 Aug 2019 16:43:49 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/bitalino/bitalino2/</guid>
      <description>筋電位センサー  今回はBITalinoの 筋電位センサー（EMG） を利用してみましょう． 筋電位センサーの電極ケーブルは3極で構成され，中央が基準電極（REF），左右が正極（+）と負極（-）を表しています． このように，2極の信号の差分を増幅して取得する方法は 作動増幅 と呼ばれ，生体センシングではよく用いられる方法です．

筋電位センサーを腕に取り付けます． 基準となる基準電極は肘の骨がある部分に配置します． また，正極と負極は筋繊維に沿って配置します．

OpenSignalsでデータを取得してみましょう． まずは，BITalinoの設定を変更します． 今回はA1に筋電位センサー（EMG）を接続しているため，EMG を指定しています．

計測を開始すると，0を平均として小さなスパイク状の波形が発生している． ここで，腕に力をいれると，波の振幅は大きくなることがわかります．

Python APIを利用した筋電位の取得  それでは，Python APIを利用して筋電位のデータを取得してみましょう． データの取得方法は光センサーのときと全く同じです． しかし，取得されるデータはBITalino独自の単位となっているため， 一般的なミリボルト（mV）を単位とするには下記の式を用いて変換が必要です （詳細は公式ドキュメント参照）．
$$ EMG(mV) = \frac{ (x / 2^n - 1 / 2) \cdot VCC \cdot 1000 }{ GAIN } $$
上記の式において，$n$は信号のビット幅であり10ビット，$VCC$は電源電圧であり3.3ボルト，$GAIN$はセンサーゲインであり1009を指定します． この変換したデータ（mV）の 平均，標準偏差，最大値 を求めてみることにします．
BITS = 10 # 信号のビット幅 VCC = 3.3 # 電源電圧 GAIN = 1009 # センサーゲイン emg = (((((data[:,5] / 2**BITS) - 1/2) * VCC) / GAIN) * 1000) # 単位変換（mV） emg = np.</description>
    </item>
    
    <item>
      <title>BITalinoで光センサー</title>
      <link>https://mukai-lab.info/pages/tech/bitalino/bitalino/</link>
      <pubDate>Tue, 20 Aug 2019 13:10:45 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/bitalino/bitalino/</guid>
      <description>BITalinoとは  BITalinoは， 医療などを目的として用いられる生体信号をリアルタイムに取得するためのハードウェア（プラットフォーム）です． BITalinoを始めるには，「Plugged Kit」「Board Kit」「Freestyle Kit」の3種類のスタータキットが存在しますが， ここでは，メインボードと各種センサーが切り離された「Plugged Kit」を用いることにします． 2019年8月現在，医療機器のオンラインショップのCreactでは34,700円で販売されています． BITalinoでは，センサーから取得した値をBluetoothを経由して，PCに送信することが可能です． このとき，提供されるAPIを利用することになりますが，Python，Java，Unityなど様々なプログラミング言語が利用可能です． ここでは，Pythonを利用してすることにしましょう．
まずは，BITalinoで何が出来るか確認してみましょう． 下記はBITalinoで筋電位を計測することで，ドローンのコントローラとして用いています． 生体信号を用いることで，これまでには存在しなかった，新しいインターフェースの開発が可能です．
  下記は，マウスの代わりに筋電位を用いてPCの決定操作を行っています． 生体信号をトリガーとして用いることで， 四肢障害を抱えた人でもPCを操作することが可能となります．
  初期設定  それでは，BITalinoの初期設定を行いましょう． まずは，メインボードにバッテリーと光センサー（LUC）を下図のように接続してください． メインボードはバッテリー以外にも，USB Type Bの端子からも給電が可能です． また，光センサーは，メインボードの A1 に接続します（アナログ入力の1番）．

次に，BITalinoのメインボードのスイッチを入れます． ボード上のLEDが白く点灯します． この状態で，PCとBluetoothで接続します． 検出されたBITalinoの追加には，PINコードが必要となりますが， これはウェブで公開されているように 1234 を入力します．


オープンシグナルズ  OpenSignalsは，BITalinoが公式に提供している取得したデータの可視化ソフトウェアです． 様々なプラグインも提供されており，OpenSignals単体だけでも，様々な分析が可能です．
OpenSignalsを起ち上げると，下記のようにBuetoothで接続されたBITalinoが検出されます． ここで，表示されるMACアドレス（ 20:16:12:21:35:82 ）は，後述のAPIでも利用しますのでメモしておきましょう． 加えて，メインボードに接続されているセンサーを設定します． ここでは，A1 に 光センサー（LUX） が接続されていることを指定します．


計測を開始すると，下図のように折れ線グラフでセンサーから取得された値が表示されます． 今回使用しているのは，光センサーであるため，通常は部屋の照明により 30% の照度となっています． このセンサーを手で覆うと，ほぼ 0% の照度となることがわかります．

Python APIを利用した照度の取得  それでは，APIを利用してBITalinoからデータを取得してみましょう． 使用するプログラミング言語は Python です． 対応するPythonのバージョンは 3.</description>
    </item>
    
    <item>
      <title>Caffe-Segnetのインストール</title>
      <link>https://mukai-lab.info/pages/tech/caffe/caffe/</link>
      <pubDate>Wed, 14 Aug 2019 13:07:59 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/caffe/caffe/</guid>
      <description>Caffe-Segnetのインストール ディープラーニングのライブラリである Caffe-Segnet をMacにインストールしたときのメモです． 基本的には公式のインストラクションに従います． このQiitaの記事や ブロクも参考にさせて頂きました．
まずは依存ライブラリをインストールします．
brew install -vd snappy leveldb gflags glog szip lmdb brew tap brewsci/science # homebrew-scienceは廃止 brew install hdf5 opencv 次に，ptofobufとboost-pythonをビルドしてインストールします． このビルドはかなりの時間を要します．
brew install --build-from-source -vd protobuf # --with-pythonは除く brew install --build-from-source -vd boost boost-python CPU_ONLY := 1 mkdir build cd build cmake .. make all make install make runtest </description>
    </item>
    
    <item>
      <title>オープンストリートマップでオリジナル地図を作成しよう</title>
      <link>https://mukai-lab.info/pages/tech/leaflet/leaflet/</link>
      <pubDate>Fri, 09 Aug 2019 14:34:10 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/leaflet/leaflet/</guid>
      <description>オープンストリートマップとは  オープンストリートマップ（OpenStreetMap: OSM）は， グーグルマップ（Google Maps）と同様のオンライン地図のサービスの一つです． グーグルマップは非営利であっても印刷物としての使用に制限がありますが， オープンストリートマップは適切にクレジットを表示すれば， 営利目的も含め，複製・再配布・改変が認められているという特徴があります（CC By-SA 2.0）．
オープンストリートマップで表示される地図は， 地図タイル と呼ばれる格子状の画像を並べることで構成されます． 地図タイルは下記の REST API と呼ばれる方法を用いて取得することが可能です． 倍率Z，座標X，座標Yを指定することで，世界の地図タイルを自由に表示できます．
https://a.tile.openstreetmap.org/{倍率:z}/{座標:X}/{座標:Y}.png 例えば，下記のように倍率Z=0，座標X=0，座標Y=0を指定してみましょう． これを表示すると，世界が一枚の地図タイルで表示されることがわかります．
 https://a.tile.openstreetmap.org/0/0/0.png
次に，倍率Z=1にして，地図タイルを取得します． 地図タイル数は $2^{2 \cdot z}$ で与えらるため， 倍率X=1のときは，4枚の地図タイルで世界を表現していることになります．
【左上】
 https://a.tile.openstreetmap.org/1/0/0.png
【左下】
 https://a.tile.openstreetmap.org/1/0/1.png
【右上】
 https://a.tile.openstreetmap.org/1/1/0.png
【右下】
 https://a.tile.openstreetmap.org/1/1/1.png
現状で，オープンストリートマップは，倍率Z=18まで利用可能となっています． このため，倍率Z=18の地図タイル数は，なんと，$2^{2 \cdot 18} = 68719476736$となっています． 今回は，このオープンストリートマップを利用して，オリジナルの地図を作成しましょう．
サンプルをダウンロードしよう  オープンストリートマップを利用して地図を作成するために， JavaScript というプログラミング言語のライブラリである Leafletを利用します． なんだかとても難しそうですが，今回はサンプル・ファイルをダウンロードして，一部を書き換えるだけなので，とっても簡単です． コンピュータが苦手な人も安心してください．
下記がサンプル・ファイル（Sample.html）です． ダウンロードしたら Chrome などのブラウザで開いてみてください．
 Sample.html
ブラウザで開くと，椙山女学園大学を中心とした地図が表示されます． マウスを使って，地図を自由に操作してみてください． マーカーをクリックすると「椙山女学園大学」や「星ヶ丘駅」と表示されます． また，地図上をクリックすると，クリックした位置の 緯度・経度 が表示されます．</description>
    </item>
    
    <item>
      <title>Carlaでセマンティック・セグメンテーション</title>
      <link>https://mukai-lab.info/pages/tech/carla/carla2/</link>
      <pubDate>Thu, 08 Aug 2019 11:28:22 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/carla/carla2/</guid>
      <description>セマンティック・セグメンテーション  セマンティック・セグメンテーション は人間の目の役割を担う技術のことであり，自動運転車の実現のために必須です． 具体的には，車両に取り付けられたカメラから取得される画像の， 各ピクセル（画素）に対し，「自動車」「人」「道路」などラベルを設定する仕組みのことです．
一般にセマンティック・セグメンテーションを実現するには，ディープラーニング が用いられます． 特にケンブリッジ大学のAlex Kendall氏が提案したSegNetが有名です． SegNetはRGB画像を入力とし，画像に存在する物体にラベルを設定することが出来ます． 下記はSegNetのデモ動画です．
  CARLA Simulatorのセマンティック・セグメンテーションはあくまで疑似センサーであり，正確なラベル付けが可能です． 各画素のラベルは，赤・緑・青（RGB）の 赤（R） の値として表現されます． 設定されるラベルは下記の13種類です．
   値 ラベル     0 なし   1 建物   2 フェンス   3 その他   4 歩行者   5 柱   6 道路線   7 道路   8 歩道   9 街路樹   10 車両   11 壁   12 信号    Pythonクライアントの実装  クライアントの初期化 それでは，セマンティック・セグメンテーションのラベルを得るためのPythonクライアントを実装しましょう． サーバーに接続するためのクライアントは with構文 を利用して生成します． このとき，サーバのIPアドレスとポート（デフォルトでは2000番）を指定します．</description>
    </item>
    
    <item>
      <title>CARLAシミュレータの導入</title>
      <link>https://mukai-lab.info/pages/tech/carla/carla/</link>
      <pubDate>Wed, 07 Aug 2019 13:57:04 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/carla/carla/</guid>
      <description>CARLA  近年，研究が急速に進む 自動運転車（Autonomous Car） のシミュレータの一つである CARLA Simulatorの導入に関して解説します． このCARLAはオープンソースとして開発されており， 自動運転車の実現に必須の，LiDAR（Light Detection and Ranging） ，多視点カメラ，深度カメラ などのセンサー群を利用することができます． この他にも，マップの自動生成や，交通シナリオの作成など，シミュレータに必要な要素も含まれています． このCARLAは，Alexey Dosovitskiy 氏らによって開発され， 2017年に Robot Learning の国際会議において，「CARLA: An Open Urban Driving Simulator」というタイトルで論文も発表されています．
CARLAの導入  CARLAの導入はとても簡単で，Githubで公開されているビルドされたパッケージをダウンロードするだけです． 現時点（2019年8月）では，最新版は 0.9.6 ですが，ここでは安定版の 0.8.2 を利用することにします．
 CARLA 0.8.2(stable)
ダウンロードしたファイルを解凍すると， CarlaUE4.exe というウィンドウズ用の実行ファイルがあります． これをダブルクリックすると，シミュレータが全画面表示されます（スタンドアロン・モード）． 画面左には，速度(Speed)やギア（Gear）などの情報が表示されていることがわかります．

この画面では，キーボードで車両を操作することができます（操作一覧も参照のこと）． キーボードで操作するのはとても難しく，ぎこちない運転になってしまうので，オートパイロット を試してみると良いです．
   キー 操作     W アクセル   A ステアリング（左）   S ブレーキ   D ステアリング（右）   Q ギアチェンジ   P オートパイロット</description>
    </item>
    
    <item>
      <title>Tobii Eye Trackerでシューティング</title>
      <link>https://mukai-lab.info/pages/tech/unity/tobii2/</link>
      <pubDate>Thu, 11 Jul 2019 18:33:33 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/unity/tobii2/</guid>
      <description>シューティング  前回はTobii Unity SDK for Desktopを利用して， オブジェクトの注視状態を取得する方法を説明しました． 今回はスクリーン上の 注視座標 を取得することを目指します． また，この注視座標を利用して，簡単なシューティング・ゲームを制作してみたいと思います．
プロジェクトの作成  Unityでプロジェクトを新規作成します． テンプレートは 2D ， プロジェクト名は TobiiShooting に設定します． また，Tobii Unity SDK for Desktopをプロジェクトに導入してください．

アセットの導入とオブジェクトの配置  シューティングに必要なアセットを導入します． Asset Store から Fighter Interceptor をダウンロードします（無料）． このファイターの3Dモデルを自機とします．
また，背景色は黒にするため，Main Camera の背景（Background）は， 黒（#000000）に設定しておきます．
次に，シーンに下記のオブジェクトを配置します．
 方向性ライト（Directional Light） 導入したFighter Intercepterの Fighter オブジェクト  Fighterオブジェクトの，位置（Position），回転（Rotation），スケール（Scale）は下図のように設定します．

ここまでで，ゲーム画面を確認すると下記のようになります．

ミサイルのプレハブの実装  次に，自機が発射するミサイルを実装します． ミサイルは Sphere で表すことにします． シーンに追加したら Missile にリネームしてください． 位置（Position），回転（Rotation），スケール（Scale）は下図のように設定します．

また，マテリアルを Yellow に変更しておきます．</description>
    </item>
    
    <item>
      <title>Tobii Eye Trackerで視線検出</title>
      <link>https://mukai-lab.info/pages/tech/unity/tobii/</link>
      <pubDate>Wed, 10 Jul 2019 11:06:05 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/unity/tobii/</guid>
      <description>Tobii Eye Tracker  トビー・テクノロジーが開発する Tobii Pro X2や Tbii Eye Tracker 4Cなどの視線入力装置を ゲームエンジンのUnityで利用するための実装方法を解説します． これらの視線入力装置は，既に，FFXV など143のPCゲームで利用可能であり（2019年7月現在）， ゲームをより楽しむためのサポート手段として用いられています． 視線入力装置を利用したゲーム映像はTobii Gamesで公開されているので視聴してみると良いでしょう．
Unityでの開発にはTobii Unity SDK for Desktopを利用します． この開発キットは，あくまで個人利用向けであり，分析を目的として利用する場合は，別途ライセンスの購入が必要なことに注意してください（向は分析用のライセンスも所持しています）． また，システム要件として下記が挙げられています．
 バージョン 5 または 4.5 以上のUnity ウィンドウズ10，8.1，7 Core Softwareのインストール  それでは，Tobii Unity SDK for Desktopをダウンロードしてください． 2019年7月現在のバージョンは 4.04 です．
プロジェクトの作成  まずは，Unityでプロジェクトを新規作成します． テンプレートは 2D ，プロジェクト名は TobiiSample に設定します（もちろん 3D でも利用可能です）．

次に，ダウンロードした，Tobii Unity SDK for Desktopをプロジェクトに導入しましょう． [Assets] → [Import Package] → [Custom Package] をクリックし，ダウンロードしたファイルを選択します．
背景とオブジェクトの設定 背景を設定しましょう． ここでは，Main Camera の Background を白（#FFFFFF）にします（別にどんな色でも構いませんが）．</description>
    </item>
    
    <item>
      <title>Tableauのマップ機能でGoogle Fitを可視化</title>
      <link>https://mukai-lab.info/pages/tech/tableau/tableau4/</link>
      <pubDate>Fri, 14 Jun 2019 18:06:47 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/tableau/tableau4/</guid>
      <description>Tableauのマップ機能  Tableauでは 緯度・経度 の情報を利用して，地図上にデータを可視化することが出来ます． 背景には，Tableauに組み込みの簡易的な地図に加え， Mapboxを利用して独自のスタイルの地図を設定することが可能です（MapboxはOpenStreetMapを採用しています）． ここでは，Google Fitに記録された活動データ（ tcx形式 ）を，Tableauで可視化することに挑戦してみます．
Google Fit  Google Fitは，Googleが開発している日々の活動を記録するためのアプリです． iOSや Androidのアプリとして提供されており，向は毎日の行動をこのアプリで記録しています． 記録されるデータは，緯度・経度などの情報を含む tcx形式 であり，自由に自身のデータを下記リンクからエクスポート（ダウンロード）することが可能です．
https://takeout.google.com/
tcx形式 は下記のように XML で記述されています． ここで， LatitudeDegrees が緯度， LongitudeDegrees は経度， AltitudeMeters は高度を表しています．
&amp;lt;Trackpoint&amp;gt; &amp;lt;DistanceMeters&amp;gt;209.93287658691406&amp;lt;/DistanceMeters&amp;gt; &amp;lt;Time&amp;gt;2019-02-01T11:12:16.277Z&amp;lt;/Time&amp;gt; &amp;lt;Position&amp;gt; &amp;lt;LatitudeDegrees&amp;gt;35.1597349&amp;lt;/LatitudeDegrees&amp;gt; &amp;lt;LongitudeDegrees&amp;gt;136.9893828&amp;lt;/LongitudeDegrees&amp;gt; &amp;lt;/Position&amp;gt; &amp;lt;AltitudeMeters&amp;gt;126.15887451171875&amp;lt;/AltitudeMeters&amp;gt; &amp;lt;/Trackpoint&amp;gt; &amp;lt;Trackpoint&amp;gt; &amp;lt;DistanceMeters&amp;gt;209.93287658691406&amp;lt;/DistanceMeters&amp;gt; &amp;lt;Time&amp;gt;2019-02-01T11:12:17.267Z&amp;lt;/Time&amp;gt; &amp;lt;Position&amp;gt; &amp;lt;LatitudeDegrees&amp;gt;35.1597831&amp;lt;/LatitudeDegrees&amp;gt; &amp;lt;LongitudeDegrees&amp;gt;136.9894012&amp;lt;/LongitudeDegrees&amp;gt; &amp;lt;/Position&amp;gt; &amp;lt;AltitudeMeters&amp;gt;126.45513916015625&amp;lt;/AltitudeMeters&amp;gt; &amp;lt;/Trackpoint&amp;gt; 残念ながら，Tableauでは，この tcx形式 をサポートしていないため， csv形式 に変換が必要です． 変換のための様々なツールがありますが，ここでは備忘録で提供されているツールを利用します． 開発者に感謝です． ここでは，CSVに変換後に，冗長なデータを取り除き，下記のデータを用いることにします．
Number,Longitude,Latitude,Altitude1,136.9665898,35.1939918,58.84051512,136.9665682,35.1939896,59.81506353,136.9665054,35.1939806,59.03094484,136.9664823,35.1939769,58.87731935,136.9664522,35.1939796,60.033026,136.9664263,35.193978,59.54827887,136.9663482,35.1939649,60.19464118,136.9662975,35.1939603,60.5572519,136.966274,35.1939573,59.788879410,136.</description>
    </item>
    
    <item>
      <title>Tableauのダッシュボードを活用しよう</title>
      <link>https://mukai-lab.info/pages/tech/tableau/tableau3/</link>
      <pubDate>Mon, 10 Jun 2019 16:57:29 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/tableau/tableau3/</guid>
      <description>ダッシュボードでプレゼン  Tableauの真骨頂と言える ダッシュボード は，ワークシートで作成したグラフを一箇所にまとめて閲覧できる機能です． グラフ間でフィルタの共有が可能であり，リッチでダイナミックな演出が可能です． データを基にした説得力のある プレゼン を行うには最適な機能です（プレゼンターのセンスが問われる機能でもあります）． 前回作成した「男女人口」「世帯数割合」「気温と降水量」「平均気温の推移」の４つのグラフを利用します．
フィルタ  フィルタ は，その名の通り，データを絞り込む機能のことです． 「ディメンション」と「メジャー」で絞り込むことが可能です． このとき，絞り込む対象のデータの性質（文字列，数値，日付など）に応じて自動的にフィルタの機能が変化します．
まずは，町別人口のグラフを開いてみましょう． グラフの右に「男」「女」と表示されている箇所（ カード と呼びます）がありませんか？ これが フィルタ です（このフィルタは自動で追加されました）． この場合は，ディメンジョンの メジャーネーム に含まれる「男」「女」で切り替えが可能です．


数値フィルタ それでは，新しくフィルタを作成してみましょう． メジャーの 総数 をフィルタに配置します． このとき，総数 は数値（連続値）であるため，範囲を定めるフィルタになります（351~9672の範囲を選択）． 配置したら 総数 を右クリックして，「フィルターを表示」を選びましょう． すると，グラフの右にフィルタが表示されます．


ここで，フィルタの値を2000~3000に変更すると， 該当する梅森台，折戸町，浅田平子，梅森町がピックアップされて表示されます．

最後に，このフィルタをダッシュボードでも利用するため， 再度，総数 を右クリックして， 適用先ワークシート の このデータソースを利用するすべてのアイテム を選択しておきましょう．
日付フィルタ 次は，「気温と降水量」のグラフを開きましょう． ここでは，ディメンジョン 年月 でフィルタを作成します． まずは，年月 を右クリックして， データ型の変更 の 日付 を選択します（事前にデータソースのCSVファイルの年月を西暦に変更する必要有）．

後は先程と同じ操作です． まず，年月 をフィルタに配置します． このとき，年月 は日付であるため，数値と同様に範囲を定めるフィルタになります（平成21年1月~平成29年12月）． 配置したら 年月 を右クリックして，「フィルターを表示」を選びましょう． すると，グラフの右にフィルタが表示されます．</description>
    </item>
    
    <item>
      <title>Tableauで色々なグラフを作成しよう</title>
      <link>https://mukai-lab.info/pages/tech/tableau/tableau2/</link>
      <pubDate>Sat, 08 Jun 2019 11:47:14 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/tableau/tableau2/</guid>
      <description>グラフの種類  前回は棒グラフを作成しましたが，Tableauでは様々な種類のグラフが利用できます． それぞれ適した用途があり，ケース・バイ・ケースで使い分けることが重要です．
 棒グラフ（量的データを並べて比較） 折れ線グラフ（時系列データの比較） 円グラフ（割合に関するデータの比較） 積み上げ棒グラフ（量的データの内訳も比較） 散布図（説明変数と非説明変数の関係） バブルチャート（3つの指標で比較） レーダーチャート（対象の特性を比較）  ここでは，使用頻度が高い 積み上げ棒グラフ，円グラフ，散布図，折れ線グラフ に注目し， オープンデータを利用してグラフを作成してみましょう．
積み上げ棒グラフ  まずは積み上げ棒グラフを作成しましょう． データソースは前回と同じ　町別人口・世帯数　を用いて， 男と女の人口を積み上げ棒グラフで表示します．
ここでは，メジャーネーム と メジャーバリュー を利用します． これらは，複数のディメンションやメジャーを，一つにまとめて扱うときに利用します． まずは，町名 を列， メジャーバリュー を行に設定します． このときの，棒グラフの高さは，$高さ=世帯数+男+女+総数+レコード数$を表しています．

次に，メジャーバリュー にフィルタを設定します． フィルタとは特定のメジャーのみを選択する機能のことです． ここでは，男 と 女 にチェックを入れます． これで，棒グラフの高さは，$高さ=男$と$高さ=女$となり，重なって表示されます．


最後に，メジャーネーム を色に設定することで，積み上げ縦棒グラフの完成です． ワークシート名や並び替えなどグラフの表現を整えていきましょう．
 ワークシート名を 男女人口 に変更 データを 降順 で並べ替え メジャーネームをデータラベルに設定  
円グラフ  次は円グラフを作成しましょう． データソースは，今回も 町別人口・世帯数 です． 地域毎の世帯数を棒グラフで表示します．
最初にワークシートを追加しておきます． 一つのワークシートに，一つのグラフを作成するのが基本であり， 複数のワークシートをまとめるには，ダッシュボード を利用します（次回に説明します）．</description>
    </item>
    
    <item>
      <title>Tableauでオープンデータを可視化</title>
      <link>https://mukai-lab.info/pages/tech/tableau/tableau/</link>
      <pubDate>Fri, 07 Jun 2019 17:16:17 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/tableau/tableau/</guid>
      <description>データサイエンティスト  データサイエンティスト という職業をご存知でしょうか． データサイエンティストとは，企業や組織での意思決定が必要な場面において，データに基づき合理的な判断を下す人を指します． データサイエンティストには，数学，統計学，情報科学，プログラミング など高度な知識が必要であり， 一部の専門家のみがその役割を担ってきました． 一方で，大量のデータを分析するためのソフトウェア，いわゆる「 BI(Business Intelligence)ツール 」が成熟・普及することで， 専門家ではなくともデータの可視化，データの集計・分析が容易となりました． BIツールには，Tableau， Qlik Sense， Actionista，などがありますが， ここでは，IT調査企業であるGartnerが， Learders（成熟したサービスを提供する製品・企業） に選出した Tableau を採用し， データの可視化にチャレンジしてみましょう．
Tableau  Tableau（日本語ではタブローと表記）とはどんなBIツールでしょうか． このBIツールは，アメリカに本社を置く Tableau Software が開発しており， 世界で42,000社以上，また，日本で2,000社以上の企業が利用していると言われています（2016年8月現在）． 本学にも導入される予定であり，教員・学生・事務職員によるIR（Institute Research）において活用される見込みです． PC向けのソフトウェアは Tableau Desktop という名称であり，一般には年間の利用料金が必要ですが， アカデミックプログラムが別途用意されており， 教員と学生は，なんと 無料 で利用可能です（14日間の無料トライアルもある）． 組織内のデータ分析はもちろん，将来のデータサイエンティストを育てるための学習ツールとしても最適と言えるでしょう．
ここで，他のBIツールと比較したTableauの特徴をまとめます．
 プログラミングが不要（ドラッグ&amp;amp;ドロップで操作） 様々なファイル形式（xlsx，csv，etc.），サーバー（MySQL, Googleスプレッドシート, etc.）に対応 可視化のパターンが豊富（ツリーマップ，パレート図，etc.）  一般に，高度なデータ分析には R， Python と呼ばれるプログラミング言語が用いられますが， Tableauではプログラミングの知識は一切不要です． また，組織で蓄積されているであろう Excel や CSV などのファイルはもちろん， データベース・サーバーにも接続可能です． さらに，分析結果を伝えるためには不可欠な データの可視化 にもTableauは優れています． 上記の特徴から，手元にあるデータを利用して，初学者が気軽に学び始めることが出来ます．
オープンデータ  早速，Tableauでデータ分析を始めたいところですが， まずは対象とするデータを用意しなくてはいけません． ここでは，自治体が公開しているオープンデータを利用してみましょう． オープンデータとは下記条件に従って公開されているデータを指しています．</description>
    </item>
    
    <item>
      <title>MacでEmacs</title>
      <link>https://mukai-lab.info/pages/tech/emacs/emacs/</link>
      <pubDate>Wed, 15 May 2019 19:50:00 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/emacs/emacs/</guid>
      <description>Atomへの乗り換えを検討しましたが，やっぱりEmacsが便利なことも多いです．そこで，MacでEmacsを利用する上で，必要最低限な設定をメモしておきます．
インストール Macにインストール可能なEmacsは複数あるが，railwaycat氏のemacs-macがしっくりくる． インストール方法は，この計算物理屋の研究備忘録を参考にした（Mojabe10.14.4の環境では，なぜかcask版は実行できなかった）．
% brew tap railwaycat/emacsmacport % brew install emacs-mac パッケージのインストール Emacsのパッケージをインストールするには，Melpaのレポジトリを登録しておく．.emacs.elに下記のように記述する（もうLispなんて忘れたわい）．
(add-to-list &amp;#39;package-archives &amp;#39;(&amp;#34;melpa-stable&amp;#34; . &amp;#34;https://stable.melpa.org/packages/&amp;#34;) t) その後で，M-x package-list-packagesをして，パッケージを選択すればOK．
emacs-neotree emacs-neotree Atomのようにファイルやフォルダをツリーで表示する． C-tでツリーの表示・非表示を切り替える．
(global-set-key &amp;#34;\C-t&amp;#34; &amp;#39;neotree-toggle) markdonw-mode マークダウン・ファイルのハイライトに用いる．
yatex Texのハイライトなど．
テーマの設定 テーマをtango-dark に設定．
(load-theme &amp;#39;tango-dark t) キーバインディングの設定 Ctr-u でUndo．
(global-set-key &amp;#34;\C-u&amp;#34; &amp;#39;undo) Meta-/ でComment．
(global-set-key &amp;#34;\M-/&amp;#34; &amp;#39;comment-line) キーバインディングの競合の解消 デフォルトのキーバインディングでは， set-mark-command にC-SPC が割り当てられていますが，マック標準のキーバインディングと競合しており動作しません． この場合，【システム環境設定】→【キーボード】→【ショートカット】→【入力ソース】で，チェックを外すと解消されます．</description>
    </item>
    
    <item>
      <title>Atomに必須のプラグイン紹介</title>
      <link>https://mukai-lab.info/pages/tech/atom/atom_plugin/</link>
      <pubDate>Sat, 11 May 2019 13:42:38 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/atom/atom_plugin/</guid>
      <description>GitHubが開発するオープンソスのエディタがAtomです． 様々なプラットフォームがサポートされており，WindowsやMacなどの異なる環境でも，同じ操作感覚で利用が可能です． これまで，Windows環境下では Atom，Mac環境下では emacs を採用していたのですが， キーボード・ショートカットなどの違いにウンザリして， Atom に統一することにしました． そこで，今回は，Atom を日常的にエディタとして利用するために必須のプラグインをご紹介したいと思います．
japanese-menu メニューバーやコンテキストメニューを日本語化してくれるプラグインがJapanese Menuです． 日本人であれば取り敢えず入れておいて損はないでしょう．
atomic-emacs キーボード・バインディングをemacs風に変更してくれるプラグインがAtomic Emacsです． emacsに慣れている人には必須のプラグインです． これを入れないなら素直にemacsを使います． killing &amp;amp; Yanking (コピー &amp;amp; ペースト)のバインディングも覚えておきましょう．
Ctrl + k (Kill Line) Ctrl + w (Kill Region) Ctrl + y (Yank Line) その他，よく利用するキー・バインディングです．
Ctrl + SPACE (Set Mark) Ctrl + / (Undo) Alt + ; (Toggle Comment) Alt + x (Command Palette) project-manager 複数のプロジェクトを切り替えながら作業するときに便利なプラグインがProject Managerです． プロジェクトのリストを表示するキー・バインディングは覚えておきましょう．
Alt + Shift + p (List Projects) markdown-writer 忘れがちなマークダウン記法の入力を強力にサポートしてれるプラグインがMarkdown-Writer for Atomです． 残念ながら，キーバインディングが，emacsと重複していることも多く，全ての機能をフルに活用するのは難しいのですが， チートシート（CHEATSHEET） だけでも何気に有り難いです．</description>
    </item>
    
    <item>
      <title>sedでJekyllからHugoに変換</title>
      <link>https://mukai-lab.info/pages/tech/hugo/2019-05-07-hugo_sed/</link>
      <pubDate>Tue, 07 May 2019 13:11:49 +0900</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/hugo/2019-05-07-hugo_sed/</guid>
      <description>JekyllからHugoに移行する際に，用いたsedワンライナーを記録しておきます． Hugoにはマークダウンの変換用のスクリプトが付属していますが，細かな修正はどうしても必要になります． sed を用いると効率的にテキストの置換が可能です． また，下記コマンドを実行すると結果は標準出力に流れますが， -i -e オプションを付けることでファイルを上書きします．
H1要素を独自のショートコード（title-logo）に置換 % sed &amp;#34;s/^# .*&amp;gt;\(.*\)/\{\{&amp;lt; title-logo title=\&amp;#34;\1\&amp;#34; &amp;gt;\}\}/g&amp;#34; FILE JekyllのLiquid構文をショートコード（gist）に置換 % sed &amp;#34;s/^{% gist \(.*\)\/\(.*\)%}/ /g&amp;#34; FILE インデントを削除 % sed &amp;#34;s/^ \(.*\)/\1/g&amp;#34; FILE </description>
    </item>
    
    <item>
      <title>Java 8における「Access Restriction」の回避</title>
      <link>https://mukai-lab.info/pages/tech/java/2016-02-25-javafx/</link>
      <pubDate>Thu, 25 Feb 2016 06:18:31 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/java/2016-02-25-javafx/</guid>
      <description>Eclipse環境でJavaFX（javafxパッケージ）をインポートしようとすると， 「Access Restriction」エラーとなってしまいコンパイルできません． JavaFXはJava 8では標準のGUIライブラリとして統合されたはずなのに， なんでだー となったので回避方法を調べてみました．
Eclipseのツールバーから[ウィンドウ]-[設定]-[Java]-[コンパイラー]-[エラー/警告]を選択します． ここで下記のように「使用すべきではない制限されたAPI」の「禁止された参照」を 無視 に設定します． これで問題なくインポートできるはずです．</description>
    </item>
    
    <item>
      <title>Jekyllでオープン・グラフ・プロトコル（OGP）の設定</title>
      <link>https://mukai-lab.info/pages/tech/jekyll/2016-02-23-ogp/</link>
      <pubDate>Tue, 23 Feb 2016 00:27:33 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/jekyll/2016-02-23-ogp/</guid>
      <description>フェイスブックでシェアされたときに表示される画像は，オープン・グラフ・プロトコル（OGP）で制御されています． OGPの設定がなければ，自動で適当な画像が選択されますが，意図しない画像が選択されることも多いです． そこで，Jekyllで採用されている「YAML Front Matter」を利用してOGPを設定する方法をメモしておきます．
まずは，ブログ記事などのファイルの先頭に， 下記のようにimageという変数を用意して，参照する画像を設定しておきます．
--- layout: default image: &amp;#34;https://i.gyazo.com/cdf2b2e7be4e80421f55152a4b348a7a.png&amp;#34; --- これで page.image という記述で設定された値を取得することができるようになります． 次に，&amp;quot;_includes/head.html&amp;quot;に下記のように page.image を利用してOGPの設定をします． このとき，条件文を利用することで page.image が設定されている場合に限ってOGPの設定を有効にしています． OGPは&amp;lt;meta property=&amp;quot;og:image&amp;quot; content=&amp;quot;参照画像&amp;quot;&amp;gt;と記述します．
 これで，フェイスブックでシェアされたときの画像が設定されます． もし画像が反映されない場合は，フェイスブックのキャッシュが原因かもしれません． この場合は，Open Graph Object Debuggerでキャッシュを削除してみましょう．</description>
    </item>
    
    <item>
      <title>Git Bashのホームディレクトリの変更</title>
      <link>https://mukai-lab.info/pages/tech/github/2016-02-17-git-bash/</link>
      <pubDate>Wed, 17 Feb 2016 02:34:43 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/github/2016-02-17-git-bash/</guid>
      <description>WindowsでGit環境を構築するとお世話になるであろう Git Bash のホームディレクトリの変更に関してメモします． GitHubのレポジトリはZ:\GitHubに設置しているため，このZ:\GitHubをホームディレクトリに設定することを目指します．
まずは，WindowsとはいえBash であることに変わりはないので，C:\Users\Naoto.bashrcを作成して，環境変数$HOMEの設定を変更してみます．
HOME=/z/GitHub/ これで問題解決と思いきや，Git Bash を実行してみると下記のように，起動時のディレクトリは/c/Users/Naotoのままです． ちなみに，環境変数$HOME自体は設定されていて，cdコマンドを実行すると/z/GitHub/に移動します．

そこで，アプローチを変更して， Git Bash のショートカットアイコンのプロパティを変更してみます． プロパティには 作業フォルダ という項目があり，ここにZ:\GitHub設定します． また，リンク先には &amp;ndash;cd-to-home という引数が設定されているので削除しておきます．

これでGit Bashのショートカットアイコンから起動してみると， 下記のように無事にホームディレクトリを変更することができました．</description>
    </item>
    
    <item>
      <title>Liquidを利用したファイルの埋め込み</title>
      <link>https://mukai-lab.info/2016/02/10/include/</link>
      <pubDate>Wed, 10 Feb 2016 08:45:44 +0000</pubDate>
      
      <guid>https://mukai-lab.info/2016/02/10/include/</guid>
      <description>JekyllではテンプレートエンジンとしてLiquidを採用しています． このLiquidでは タグ や フィルタ という機能を利用して， ウェブページ制作における無駄を減らすすことが可能です．
ここでは，複数のページに共通で利用されるコードの断片を他のファイルから読み込んで埋め込む方法を紹介します． 埋め込むコードを common.html として作成し， _includes ディレクトリに配置します． 後はMDファイルやHTMLファイルに下記のように記述するだけです．
 その他，Gistで公開しているソースコードも下記の方法で公開可能です．
 ただし，jekyll-gistを事前にインストールし， さらに，_config.yml に下記の設定を加えておく必要があるようです． この _config.yml の設定は忘れがちになるので特に注意が必要です．
 </description>
    </item>
    
    <item>
      <title>Github Pagesでの独自ドメインの設定</title>
      <link>https://mukai-lab.info/pages/tech/github/2016-02-08-domain/</link>
      <pubDate>Mon, 08 Feb 2016 09:13:03 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/github/2016-02-08-domain/</guid>
      <description>ブログを立ち上げたので独自ドメインを取得してみることにしました． ドメインはお名前.comで，トップレベルドメインには「.info」を選択しました． １年間の契約でなんと「269円（手数料込）」！ メジャーなトップレベルドメインではないとはいえ，この安さに驚きました（普段，ドメイン取得することないからなぁ）．
さて，ホスティングしているGithub Pagesを独自ドメインで公開する方法ですが， 様々なサイトで詳しく情報が公開されており，検索すれば容易に発見できると思います． 他と重複する情報とはなりますが，念のためメモしておきたいと思います．
まずは，お名前.comの設定ですが， ドメインNaviでGithub PagesのDNSレコードの登録が必要です． 具体的には，下記のようにAレコードとして，192.30.252.153と192.30.252.154を登録するだけです． （参考：Github Help）

次に，自身のレポジトリの設定ですが， CNAME という名前をファイルを作成し， 転送先の独自ドメイン（ここでは muka-lab.info ）を入力します． 後は，このファイルを他のHTMLファイルなどと一緒にデプロイするだけです． これらの変更の適用には時間がかかるようなので， 少し時間を空けてから転送が成功するか確認しましょう．</description>
    </item>
    
    <item>
      <title>Jekyllにおけるシンタックスハイライト</title>
      <link>https://mukai-lab.info/pages/tech/jekyll/2016-02-05-highlight/</link>
      <pubDate>Fri, 05 Feb 2016 02:58:35 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/jekyll/2016-02-05-highlight/</guid>
      <description>Jekyllが得意とするシンタックスハイライトですが，設定でつまずいたのでメモしておきます． シンタックスハイライトにはrougeやpygmentsが利用されるようですが，ここではrougeを適用してみたいと思います． まずは，下記のコマンドでrougeをインストールします．
gem install rouge 次に，&amp;quot;_config.yml&amp;quot;でシンタックスハイライトにrougeを指定しておきます．
highlighter: rouge ここまでは，何てことないのですが，ここからつまずきました． シンタックスハイライトのスタイルは&amp;rdquo;_syntax-highlighting.scss&amp;quot;で設定されているようです． 色などを変更したい場合は，このファイルを修正すれば良いはずですが，何故か変更が適用されません．
マークダウンでハイライトするときは，ソースコードを``で囲みます． 変換後のhtmlを確認すると，該当箇所は下記のように，クラス属性にhighlighter-rougeが設定されています．
&amp;lt;code class=&amp;#34;highlighter-rouge&amp;#34;&amp;gt;&amp;lt;/code&amp;gt; 再度，&amp;quot;_syntax-highlighting.scss&amp;quot;を確認すると，クラス属性はhighlightとして設定されています． そこで，ファイル内のクラス属性をhighlighter-rougeに置換することで，シンタックスハイライトが適用されました． 試行錯誤の末，シンタックスハイライトの適用までこぎつけましたが，この辺り十分に理解できていないかもしれません．</description>
    </item>
    
    <item>
      <title>Jekyllデフォルトテンプレートの自動リンク</title>
      <link>https://mukai-lab.info/pages/tech/jekyll/2016-02-04-top_link/</link>
      <pubDate>Thu, 04 Feb 2016 09:26:26 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/jekyll/2016-02-04-top_link/</guid>
      <description>Jekyllのデフォルトテンプレートでは， トップページのヘッダ部の右にサイトに含まれるページのリンクを自動的に追加してくれます． これはとても便利な機能ではあるのですが，ページ数が増えてくると，特定のページは除いて欲しいというケースが発生します．
そこで，YAML Front Matterを利用して解決します． まずは，&amp;quot;_templates/page&amp;quot;に，top_linkという名前の変数を下記のように用意します． これで，新規に作成するページには，top_linkの行が自動的に追加されます．
 トップページに自動的にリンクを追加する場合は，ページを作成した後で，このtop_linkの値を&amp;quot;true&amp;quot;に設定しておきましょう． 次に，&amp;quot;_includes/header.html&amp;quot;に下記のようにtop_linkに関する条件文を追加します．
 これで，トップページの自動リンクをtop_linkという変数で制御できるようになります．</description>
    </item>
    
    <item>
      <title>CodeCombatでプログラミング学習</title>
      <link>https://mukai-lab.info/pages/tech/codecombat/codecombat/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/codecombat/codecombat/</guid>
      <description>CodeCombatとは  CodeCombatはRPG風のゲームを遊びながらプログラミングを学ぶことができるプラットフォームです． オープンソースのコミュニティで運営されており，2013年に公開されて以降，500人以上のプログラマーがこのプロジェクトに関わって開発が続けられています． IEやChromeなどのウェブブラウザさえあれば，プラットフォームに参加できることが特徴であり，50以上の言語に翻訳され，世界で5,000,000人以上のプレイヤーがこのプラットフォームでプログラミングを学んでいます(2016年11月18日現在)． 本格的にプラットフォームを利用するには，アカウント登録が必要ですが，今回はアカウント登録はしないで，プログラミングの基本的な文法を学ぶ「キースガードのダンジョン」に挑戦してみましょう． この授業を受ける多くの皆さんは「プログラミングは難しい」という先入観を持っているのではないでしょうか． この授業が終わった頃には「プログラミングは楽しい」と思ってもらえることを期待します．

学習コースの選択  早速，CodeCombatでプログラミング学習を始めましょう． まずは，下記のリンクをクリックしてブラウザでCodeCombatを開きます．
CodeCombat CodeCombatを開いたら，「今すぐプレイ」をクリックしましょう． すると，学習コース（ダンジョン）を選択するページに遷移します．

プログラミングに必要な，文法，メソッド，パラメータなどの基本的なスキルを学ぶことが出来る学習コースが「キースガードのダンジョン」です． キースガードのダンジョンの「ゲームスタート」をクリックしましょう．

キースガードのダンジョン ～始まりの回廊~  キースガードのダンジョンは，40のステージで構成されています． まず最初は「始まりの回廊」に挑戦します． 「始まりの回廊」をクリックしすると，このステージの概要が表示されます． このステージの目的は「宝石を集めること」のようです． 「壁の針」がちょっと気になりますね． では，ゲームスタートを選びましょう．


次に，ヒーロー（主人公）を選択する画面になります． 最初は４人しか選べませんが，ゲームを進めると選択できるヒーローが増えていきます． 好きなヒーローを選びましょう． 剣士の「サー・サーリン・サンダフィスト」を選択したとして説明を続けます． また，この画面では学習するプログラミング言語を選択します． ここでは，ウェブで用いられる「JAVASCRIPT」を選びましょう． ヒーローとプログラミング言語の選択が終わったら「次へ」をクリックしましょう．

次に，武器や防具などを装備するイベントリの画面になります． ここでは，ヒーローを上下左右に動かすための「質素な靴」を装備します． 靴のアイコンをダブルクリックすると装備することが可能です． 「質素な靴」を装備したら「ゲームスタート」をクリックしましょう．


さぁ，とうとうゲームが開始しました． 「レベルスタート」をクリックしましょう． このステージの目標は「壁の針をよける」「宝石を集める」の２つです． プログラムをコーディングして目標を達成しましょう．

マップを確認すると，ヒーローはスタート位置から右に進んだ後に，壁の針を避ける必要がありそうです． ヒーローを下に進めるには「hero.moveDown()」というコードを入力すれば良さそうです． では，6行目に上記のコードを追加して，「実行」をクリックしましょう．


これで，残るはヒーローの右にある宝石をゲットするだけです． ヒーローを右に進めるには「hero.moveRight()」というコードを入力しましょう． では，7行目に上記のコードを追加して，「実行」をクリックしましょう

宝石をゲットして，全ての目標を達成したため，ゲームクリアです． 「完了」をクリックしましょう． 獲得した経験値，ジェム，武器・防具を確認したら「次へ」をクリックします．


これで，最初のステージは終了です． プログラミングに興味が湧いてきましたか． ではその調子で次のステージに挑戦してみましょう．
キースガードのダンジョン ～宝石の部屋~  次のステージは「宝石の部屋」です． このステージの目的は「素早く宝石を集めること」のようです． 「将来必要になる」との暗示は，女性にとっては嬉しいかもしれませんが，男性にとっては恐怖です（笑）． では，ゲームスタートを選びましょう．</description>
    </item>
    
    <item>
      <title>Cortexで脳波データの取得</title>
      <link>https://mukai-lab.info/pages/tech/emotiv/emotiv2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/emotiv/emotiv2/</guid>
      <description>Cortexとは  Python を経由して，Emotiv Epoc+ で計測したデータを取得してみましょう． データを取得するには，Emotivの標準APIであるCortexを利用します． 今回はPythonを利用しますが，このCortexは JSON と WebSockets で実装されているため， Python以外のプログラミング言語でも利用可能です． また，Cortexは，EmotivProと一緒にインストールされますが， Cortex UIというソフトウェアで動作確認が可能です． 下図のように対象のデバイスが検出されていればOKです． コンタクト・クオリティは今回も35%とパッとしません（笑）．

ライブラリの導入  Pythonはウィンドウズ版のバージョン3.7.2を用いることにします． Cortexでは，データの送受信には WebSockets という技術を用います． ここでは，WebSockets のクライアント（データ受信側）として実装するため， websocket-client ライブラリをインストールしておきます．
$ pip install websocket-client また，WebSocketsのポート番号は 54231 です． URLには wss://localhost:54321 を指定します． ここで，wss はWebSocketsのプロトコルを表しています．
手順１：認証  Cortexから脳波の生データを取得するには，クライアントIDとシークレットで認証が必要です． この，クライアントIDとシークレットは，EMOTIVのユーザページで事前に取得しておきましょう（ライセンス番号も必要）．
まずは，CORTEXで用いられるJSON-RPCについて簡単に説明します． 上述したように，サーバとクライアント間のデータのやりとりをJSON形式で行うという仕組みです． クライアントが送信するリクエストの基本フォーマットは下記です． ここで，プロトコルバージョンの jsonrpc は常に 2.0 を指定します． メソッドやパラメータには，認証（authorize）やデータ取得（subscribe）などの文字列を指定します．
{ &amp;#34;jsonrpc&amp;#34;: &amp;#34;2.0&amp;#34;, &amp;#34;method&amp;#34;: メソッド, &amp;#34;params&amp;#34;:{ パラメータ: 値 }, &amp;#34;id&amp;#34;: ID番号 } また，クライアントが受信するレスポンスの基本フォーマットは下記となります． サーバーからの応答結果は result に格納されています． また，リクエストと同じID番号が付与されています．</description>
    </item>
    
    <item>
      <title>Emotivで脳波を可視化</title>
      <link>https://mukai-lab.info/pages/tech/emotiv/emotiv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/emotiv/emotiv/</guid>
      <description>
Emotivとは  Emotivは株式会社イノバテックが提供している小型の脳波計です． Epoc+，INSIGHT，EPOC flexなど複数の種類がありますが， ここでは，14チャネルの電極が装着されたEMOTIV EPOC+を対象に開発することを目的とします． この装置は，他の脳波計と比べ，スタイリッシュなデザインが特徴です． 頭に装着するだけで，未来感がビシビシ伝わってきます． 専用のアプリケーションであるEmotivPROで， 脳波データを取得・可視化するだけなら無料のラインセンスで利用可能です． しかし，脳波の生データをAPI（Cortex）を経由して取得するなど， 研究を目的として利用するためには，別途ライセンスの購入が必要です． 月額99ドルと決して安くはないため注意が必要です（年払いで割引されますが）．

EmotivPROを使ってみよう  最初に，専用のアプリケーションであるEmotivPROを利用して， 脳波データを取得・可視化してみましょう． まずは，電極のフェルドパッドを生理食塩水で水和させます． この生理食塩水はコンタクトレンズなどにも用いられているものです． ここで，しっかりと水和させておかないと，センサーの感度が悪くなる可能性があります． 電極を時計回りに回すと取り外すことができますが， 力を入れ過ぎるとフェルト部分がはずれてしまうので要注意です．

それでは，ヘッドセットを頭に装着します． ヘッドセットを両手で持ち，頭上から下に向かってスライドさせます． このとき，左右の参照電極を，ちょうど耳たぶの裏の骨部分に配置します． 参照電極を用いた方法は単極導出と呼ばれ，この参照電極と他の電極との電位差が記録されます． この参照電極は，他の電極にも影響するため，正確な設置が必要です． また，前方の左右の電極は，眉毛から指３本だけ上にあるように配置します．

この電極の設置の質（コンタクト・クオリティ）はEmotivProの下記の画面で確認できますが，・・・，非常に難しい． 緑色の電極はコンタクト・クオリティが高いことを示しています． 電極の位置を微調整しても，コンタクト・クオリティは全体で**28%**がやっとでした． このあたりのコツをご存知でしたら，お教えください．

脳波を可視化しよう  次は脳波を可視化してみましょう． ここで，脳波のサンプリングレートとA/D変換の分解能を確認しておきます． サンプリングレートは128Hz，また，分解能は16ビット（65536階調）です． 図中にEEGと表記がありますが，これはElectroencephalographの略で脳波（図）を意味しています （単に脳波をEEGと呼ぶことも多い）．

次に電極の位置番号を確認しておきます． 上述したようにEpoc+では，14チャネルの電極があり， それぞれ，AF3，AF4，F3，F4，F7，F8，FC5，FC6，T7，T8，P7，P8，O1，O2です． 図中の赤い丸で表現されている電極は，耳たぶの裏にある参照電極です． ここでは，コンタクト・クオリティの高いAF4，F3，P8の３箇所に注目します．

AF4，F3，P8の３箇所の電極の生データは下記です． 横軸は時間で単位は10[ms]，また，縦軸は電位を表し**-100µV**〜**100µV**の範囲を取ります． 一般的にローデータから，脳波の意味を読み取ることは困難なため，高速フーリエ変換をして周波数成分を取り出します．

EmotivProはフーリエ変換した周波数スペクトルの表示も可能です． P8にフーリエ変換をした結果は下記のようになります． 周波数に応じて脳波は分類され，4〜8Hzはシータ波，8〜12Hzはアルファ波，13Hz以上はベータ波と呼ばれます． この結果では，若干ですがシータ波が有意なように見えます． シータ波は眠い状態で発生する波とされ，いかに寝不足かが分かる結果となりました（笑）．

今回はEmotivの基本的な使い方を解説しました． 次回はCortexと呼ばれるAPIを利用して，Pythonで脳波データを取得することに挑戦します．
参考書籍</description>
    </item>
    
    <item>
      <title>Google VR SDKで視線の検知</title>
      <link>https://mukai-lab.info/pages/tech/unity/google_vr2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/unity/google_vr2/</guid>
      <description>視線の検知  VR環境では，タップなどスマートフォンで一般に用いられる操作が出来ません． そこで，Google VRでは， ユーザの視線による操作を可能としたGoogle VR Pointer System が用意されています． 今回は，視線を利用したオブジェクトの選択を実現してみましょう． 事前に準備が必要なプロジェクトの作成やパッケージの導入などは， Google VR SDKを利用した360°画像ビューアーを参考にしてください．
シーンの作成  シーンを新規に作成します． ここでは，シーンの名前は[MenuScene]とします． まずは，Google VR SDKを利用した360°画像ビューアーを参考に， 空のオブジェクトであるVR Cameraを作成し，直下にMain CameraとGvrEditorEmulatorを設定します．
次に対象となるQuadオブジェクトをシーンに配置します． Quadオブジェクトを配置するには，[GameObject]-[3D]-[Quad]を選択します． Quadオブジェクトのインスペクターを開き，Position の Z座標 を5に設定しましょう． また，新規にマテリアルを作成し，Quadオブジェクトに追加しておきます（マテリアルに関しては割愛します）． シーンを再生すると下記のように表示されます．

視線の検知  上記で作成したQuadオブジェクトを視線に捉えていることを検出してみましょう． まずは，ヒエラルキーにGvrEventSystemをドラッグ＆ドロップで配置します． GvrEventSystemは視線に関するイベントの包括的な処理を行います． また，ヒエラルキーのMain Cameraの直下に，GvrReticlePointer を配置します． GvrReticlePointerは，ユーザが見つめている一点をポインタで表します． 対象となるオブジェクトを見続けているとポインタが拡大します． シーンを再生すると下記のように表示されます． 中央に視点を表すポインタが表示されていることが分かります． 現時点ではQuadオブジェクトを見続けてもポインタに変化はありません．

次に，視線を検出するためのスクリプトをMain Cameraに設定します． Main CameraのインスペクターでAdd Componentを選び，Physics Raycaster を選択します（Physics Raycasterはスクリプトであることに注意）．

また，対象となるQuadオブジェクトにEvent Triggerを設定します． QuadオブジェクトのインスペクターでAdd Componentを選び，Event Trigger を選択します（Event Triggerもスクリプト）．
シーンを再生すると下記のように表示されます． 中央にあるポインタがQuadオブジェクトを捉えると， ポインタが大きな円に変形することが分かります．

シーンの切替  Quadオブジェクトを2秒間見続けるとシーンを切り替えるようにします． QuadオブジェクトのインスペクターでAdd Componentを選び，Net Script を選択します（C#で記述する）． ここでは，スクリプト名はCntrol Sceneとします．</description>
    </item>
    
    <item>
      <title>Google VR SDKを利用した360°画像ビューアー</title>
      <link>https://mukai-lab.info/pages/tech/unity/google_vr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/unity/google_vr/</guid>
      <description>Google VR SDK for Unity  近年，VR(Virtual Reality)に関する技術は目覚ましいスピードで発展しています． Oculus社が開発する「Oculus Rift」， SCE(Sony Computer Entertainment)の「PlayStation VR」など， 本格的なVRのためのヘッドマウントディスプレイも手に入れることができます． しかし，これらの製品はまだまだ高価であり，気軽に導入することは難しいです． このような状況のなか，Googleは，スマートフォンを組み合わせて利用する， ダンボール製の安価な「Cardboard」というヘッドマウントディスプレイを提供しています． 同時に，Googleは，「Google VR」という， VRに関するプロジェクトを立ち上げ，開発者向けのツールも提供しています． そこで，今回は，ゲームエンジンの一つであるUnityで， Cardboard向けVRを開発可能な「Google VR SDK for Unity」を利用したアプリを制作してみます． また，VR環境には，リコーが開発する「RICHO THETA S」で撮影した360°画像（全天球画像）を用い， ヘッドセットの動きに合わせて，360°画像を閲覧できるようにします． 開発に当たり，SlideShareで公開されているOculus Rift勉強会の資料 THETAでモバイルVRコンテンツ開発を参考にさせて頂きました．
準備  アプリの開発に当たり下記の機器を利用します． ヘッドマウントディスプレイにはCardboardではなく， サンワサプライが販売している「VR SHINECON」を利用します． ヘッドホンが搭載されており，Cardboardに比べると高級感があるモデルです．
また，360°画像（全天球画像）の撮影にはRICHO THETA Sを採用します． 静止画・動画に対応しており，動画のライブストリーミングも可能なモデルです．
 VR SHINECON RICHO THETA S  下記がRICHO THETA Sで撮影したサンプル画像です． マウスを使って画像をスクロールすると，教室に一人で寂しそうに立っている向の姿が見えるはずです． ここでは，ウェブで360°画像を表示するために「VR view on the web」を利用しています． また，RICHO THETA Sで撮影された画像の解像度は5376x2688ですが，VR viewに最適な4096x2048に変換してあります．
 プロジェクトの作成  Unityでプロジェクトを作成します． Unityのバージョンは5.</description>
    </item>
    
    <item>
      <title>MOONBlockでゲームプログラミング</title>
      <link>https://mukai-lab.info/pages/tech/enchant_js/moonblock/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/enchant_js/moonblock/</guid>
      <description>MOONBlockとは  MOOBlockは株式会社ユビキタスエンターテインメントが 開発する教育用のプログラミング言語です． 普通のプログラミング言語（C言語など）は複雑な命令や制御を理解する必要がありますが， MOONBlockではブロックを並べるだけで簡単にプログラミングできるという特徴があります （同様の方法でプログラミングが可能なMITメディアラボ開発のSCRATCHも有名です）． 事前にソフトウェアをインストールする必要もなく， ウェブブラウザ（IE，Chromeなど）がインストールされていれば動作させることが可能です． 今回はこのMOONBlockを利用して簡単なゲームを制作することを目指しましょう．
MOONBlockの基本  MOONBlockはゲームエンジンの一つであるenchant.jsがベースとなっています． このため，MOONBlockで作成したプログラムを基にPC，Mac，iOS，Androidなど様々なプラットフォームで動作するアプリケーションを開発することが可能です． enchant.jsはMITライセンスであり，ソースコードの著作権の表示を条件に， 「ソースコードの改変」「再配布」が認められています （詳細はenchant.jsのライセンスを参照）．
また，制作したゲームをゲーム投稿サイトの9leapに投稿することで， ゲームを体験したユーザから様々なフィードバックを得ることができます（コンテストに応募して賞金を狙うことも）． 下記は研究室の学生が制作し9leapに応募した作品です． 時間があるときに遊んでみてください．
星を集めようはスライムを避けながら星（スター）を３つ集めるゲームです．
 イライラ棒アプリは木，森などの障害物に当たることなく，ゴールを目指すゲームです．
 ひよこマンは，３種類に変身するひよこを操りながら，鍵を集めてゴールに向かうゲームです．
 MOONBlockのゲーム素材画像  ゲームを制作する際に必要なアイコン，キャラクター，背景などの素材画像は， MOONBlockであらかじめ用意されています（非営利目的であれば自由に利用可能）． アイコンは16×16ピクセル，キャラクターは32&amp;amp;times32;ピクセル， 背景は320×320ピクセルで構成されています． これらの素材画像を重ねて表示することでゲーム画面は構成されます．
ここでキャラクターの素材画像（chara1.png）に注目してみましょう． 少しずつ異なるクマの画像が横に並んでいることが分かります． これらの画像を，パラパラ漫画の要領で，素早く切り替えることでクマが動いているように見せることができます．

Chara1.png

icon0.png

rpg.png
MOONBlockの開発画面  下記のリンクをクリックして，MOONBlockのサイトを開きましょう！
http://www.moonblock.jp/
画面上に並んでいる「パペット」「ビヘイビア」などの箱を「キット」と呼びます． キットにはプログラミングに必要なブロックが使用目的に合わせて別れて入っています． キットは左右にドラッグすることで全ての種類を確認できます．
画面右上にある青い正方形は実行画面です． プログラムの実行結果はここで確認できます． 大きさは背景画像と同じ320×320ピクセルです．
画面左下にあるボタンをクリックすると，プログラムの保存や，プログラムの実行ができます． 一端保存して自宅でプログラミングの続きをしたり，9leapに作品を投稿するときに利用しましょう．
画面右下にあるゴミ箱には，不要になったブロックを入れます． 一度捨てたブロックは復元できないので注意してください．

くまのバナナ拾いゲームの制作  くまが画面内にあるバナナを拾うゲームを作ってみましょう．
まずは，クマのキャラクターを画面に出現させましょう． 「パペット」キットから「パペット」ブロックをドラッグしてワークスペースに配置します． パペットとは”操り人形”の意味であり，MOONBlockではクマなどのキャラクターやバナナなどのアイテムを指します． 次に，「ビヘイビア」キットから「出現」ブロックを引き出し，「パペット」ブロックに下図のように接続します． ビヘイビアとは”振る舞い”の意味であり，パペットの動きなどを設定するときに用います． Runボタンをクリックして実行してみましょう． このときのブロックの状態はここから確認できます．</description>
    </item>
    
    <item>
      <title>NyARToolkit for Unityで3Dモデルのアニメーション</title>
      <link>https://mukai-lab.info/pages/tech/unity/artoolkit4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/unity/artoolkit4/</guid>
      <description>
3Dモデルのアニメーション  前回に引き続き，Unityのオリジナルキャラクターである「ユニティちゃん」を利用して， 3Dモデルにアニメーションを設定してみましょう（詳細はNyARToolkit for 3Dモデルの表示を参照）．
Unityで3Dモデルにアニメーションを設定するときは，Animator ControllerというAssetを利用します． 「ユニティちゃん」のパッケージには，UnityChanLocomotions，UnityChanActionCheck，UnityChanARPoseの ３種類のAnimator Controller **が含まれています．
今回は，UnityChanLocomotionsを「ユニティちゃん」に設定して，キーボードの入力に対応してアニメーションするように改良します．
Animation Controller  [UnityChan]-[Animators]にAnimation ControllerのUnityChanLocomotionsがあります． これをドラッグし，unitychanのInspectorにあるControllerに設定します． これで，「ユニティちゃん」はUnityChanLocomotionsで定義されたアニメーションを行うことが可能となります．

ここで，UnityChanLocomotionsの内容を確認してみましょう． Animation Controllerは下図のようにグラフで定義されます． まずは，Entryから始まり，Idle状態に遷移することが分かります．

Idle状態のInspectorを確認すると， MotionにWAIT00が設定されていることが分かります． このWAIT00が3Dモデルの動きに対応します． また，Idle状態から，Locomotion状態，WalkBack状態，Rest状態の ３つの状態に遷移可能なことが分かります．

では，Idle状態から他の状態に遷移するための条件は何でしょうか． TransitionsのIdle -&amp;gt; Restをクリックすると，状態遷移のための条件（Conditions）が表示されます． ここでは，Restという変数（パラメータ）がtrueであるときにRest状態に遷移することが分かります． 同様に，Locomotions状態に遷移する条件は，Speedが0.1より大きいとき， また，WalkBack状態に遷移する条件は，Speedが-0.1より小さいときということが分かります． このように変数（パラメータ）に基づき，3Dモデルの状態が確定します．

スクリプトによる状態遷移の制御  次に，C#スクリプトを作成し，キーボードからの入力に応じて状態遷移を制御してみます． まずは，[Create]-[C# Script]をクリックして，新規にC#のスクリプトを作成します． ファイル名はARAnimationsとしておきます．
まずは，キーボードの1を押すとRest状態に遷移するようにしてみます． キーの入力判定はInput.GetKey()メソッドを利用します． 引数にはstring型で対象となるキーを指定します． また，状態遷移のトリガーとなる変数Restの値を設定するには， SetBool()メソッドを利用します． 引数には，String型で対象となる変数と，その値を指定します． 再生ボタンをクリックして，「ユニティちゃん」を表示した状態で，キーボードの1を押してみましょう． ユニティちゃんが背伸びをするアニメーションが表示されるはずです．
 次に，キーボードの2を押すとLocomotions状態，3を押すとWalkBack状態， 4を押すとIdle状態に遷移するようにしてみます． トリガーとなる変数Speedはfloat型のため，SetFloat()メソッドを利用して値を変更しています． 再生ボタンをクリックして，「ユニティちゃん」を表示した状態で，キーボードの2，3，4を押してみましょう． ユニティちゃんが走ったり，後ずさりするアニメーションが表示されるはずです．  

これまでに紹介した機能を利用して，拡張現実を利用した作品を制作してみてください．
参考書籍</description>
    </item>
    
    <item>
      <title>NyARToolkit for Unityで3Dモデルを表示</title>
      <link>https://mukai-lab.info/pages/tech/unity/artoolkit3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/unity/artoolkit3/</guid>
      <description>
3Dモデルの準備  NyARToolkitのパッケージに含まれる サンプルSimpleLiteはマーカーを検出すると赤色の立方体（Cube）を表示するプログラムです． このSimpleLiteを修正し，3Dモデルを立体的に表示できるよう改良します．
SimpleLiteフォルダをコピーし，フォルダに含まれるC#スクリプトをARPictureCamera， シーンをARPictureSceneにファイル名を変更した状態を前提とします （詳細はNyARToolkit for Unityの導入を参照）．
Unityでは**.3ds**，**.obj**などの一般的な3Dモデルのファイル形式を利用できますが， 今回はUnityのオリジナルキャラクターである「ユニティちゃん」を利用してみます． 「ユニティちゃん」はUnityの**アセット（素材）**として配布されているので導入はとても簡単です． また，このキャラクターは，ラインセンスロゴもしくはライセンス表記があれば，キャラクターの二次創作物の制作が認められています （詳細は「ユニティちゃんライセンス条項」を参照）．

まずは，「ユニティちゃん」の公式ページから，ライセンスに同意し，データをダウンロードします． データをダウンロードしたら，ツールバーから[Assets]-[Import package]-[Custom package]をクリックして， ダウンロードしたパッケージを選択します． ファイルの読込み後に，ダイアログが表示されたら，全てのファイルにチェックを入れた状態でimportをクリックしましょう． ファイルの取り込みが終わると，プロジェクトのAssetsに新しくUnityChanフォルダが展開されます．

3Dモデルの表示  [UnityChan]-[Models]に「ユニティちゃん」の3Dモデルであるunitychanがあります． これを，ドラッグし，HierarchyにあるMarkerObjectの直下に配置します． すると下記のようにx=0，y=0，ｚ=0の位置に「ユニティちゃん」が配置されます．

次に，「ユニティちゃん」の**サイズ（Scale）や座標（Position）**を調整します． ここでは，Inspectorから，サイズの値をX=80，Y=80，Z=80，位置をX=0，Y=-40，Z=-20に修正しましょう．

再生ボタンをクリックすると，「ユニティちゃん」が表示されることを確認してください． 説明文なしに下記の画像だけだと完全に変な人かもしれません．

次回はAnimator Controllerを利用して「ユニティちゃん」に歩くなどの動きを付けることに挑戦します．
参考書籍</description>
    </item>
    
    <item>
      <title>NyARToolkit for UnityでNFTを利用したマーカーの認識</title>
      <link>https://mukai-lab.info/pages/tech/unity/artoolkit5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/unity/artoolkit5/</guid>
      <description>画像の準備  NyARToolkitのパッケージに含まれるサンプルSimpleNftは NFT(Natural Feature Tracking)を利用したマーカー認識のサンプルです． NFTとは自然特徴点を利用した物体認識のことで，この技術を利用することで，一般的な画像をマーカーとして扱うことができます （詳しくはThe Sixwish Projectを参照）． ここでは，下記の下記の「犬の画像（001.png）」と 「うさぎの画像（002.png）」を マーカーとして利用することに挑戦します． 画像サイズは300x300ピクセル，画像フォーマットは透過背景のPNGです． また，犬の画像を認識すると赤色の立方体，うさぎの画像を認識すると青色の立方体を表示させることにします．


SimpleNftフォルダをコピーし，フォルダに含まれるC#スクリプトをARPictureCamera， シーンをARPictureSceneにファイル名を変更した状態を前提とします（詳細はNYARToolkit for Unityの導入を参照）．
パターンファイルの作成  上記の２種類の画像から，マーカーのパターンファイルを作成します． パターンファイルの作成には，NyARToolkitのパッケージに含まれるNftFileGeneratorを利用します （詳しくはNyARToolkit Projectを参照）． [Data]-[Tools]にあるNftFileGeneratorをダブルクリックするとプログラムが起動します． importをクリックし，特徴点を抽出したいJPGやPNGなどの画像を指定します（透過より背景白の方が認識精度が高いようです）． 次に，Make Feature Setをクリックし，特徴点を抽出します． このとき，Source DPIやIset DPIsなどのパラメータは特に変更する必要はないようですが， 特徴点数が多すぎたり，少なすぎる場合にはFSET parameterを調節すると良いようです．

抽出されたパターンは赤い四角や青い円で表示されます． パターンを保存するにはExportをクリックし，適当なファイル名で保存します． ここでは，「犬の画像」のパターンファイルをpatt_001.bytes， 「うさぎの画像」のパターンファイルをpatt_002.bytesというファイル名で保存します． これらのパターンファイルは，AssetsのResourcesフォルダにコピーしておきます．


マーカーオブジェクトの作成  SimpleNftはマーカーを認識すると赤色の立方体を表示します． ここでは，２種類のマーカーに応じて，赤色の立方体に加え，青色の立方体を作成しておきます．
まずは，シーンのARPictureSceneをダブルクリックし，HierarchyにあるMarkerObjectを複製します． スクリプトからこれのオブジェクトを操作するため，２つのマーカーオブジェクトには共通のタグ（Tag）を設定します （Gameobject.FindGameObjectsWithTag(String tag)メソッドでタグからオブジェクトを取得することが可能）． タグ名は自由に設定することができますが，ここではMarkerObjectとしておきます．

次に，Assetsで赤色と青色のマテリアルを作成します． [Create]-[Materials]をクリックしてマテリアルを作成し，Albedoに赤色と青色を設定します． ここでは，マテリアルの名前をRedとBlueにしておきます． これらのマテリアルはMarkerObjectの直下にあるCubeのMaterialsに設定しておきます．

マーカーが認識されたときに表示されるのは，上記のCubeオブジェクトです． Cubeの**サイズ（Scale）や位置（Position）**はマーカーの大きさに合わせて調整する必要がありますが， ここでは，サイズの値をX=80，Y=80，Z=80，位置をX=-80，Y=80，Z=40に修正しておきます．

スクリプトの修正  最後にC#スクリプトのARPictureCameraを修正します．
まずは，2つのマーカーオブジェクトを配列で取得します． マーカーオブジェクトはMarkerObjectというタグが設定されていることを利用します．</description>
    </item>
    
    <item>
      <title>NyARToolkit for Unityで画像を表示</title>
      <link>https://mukai-lab.info/pages/tech/unity/artoolkit2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/unity/artoolkit2/</guid>
      <description>
画像の準備  NyARToolkitのパッケージに含まれる サンプルSimpleLiteはマーカーを検出すると赤色の立方体（Cube）を表示するプログラムです． このSimpleLiteを修正し，一般的なJPGやPNGなどの画像を立体的に表示できるよう改良します．
SimpleLiteフォルダをコピーし，フォルダに含まれるC#スクリプトをARPictureCamera， シーンをARPictureSceneにファイル名を変更した状態を前提とします （詳細はNyARToolkit for Unityの導入を参照）．
立方体の代わりに表示する犬の画像（001.png）は下記です． この画像は，テンプレートBANKを参考に，本学の学生が作成しました． 画像サイズは300x300ピクセル，画像フォーマットは透過背景のPNGです．

まずは，新規にanimalsフォルダを作成し，上記の画像をコピーしておきます． 対象のフォルダには，C#スクリプトのARPictureCamera，シーンのARPictureScene， 画像フォルダのanimalsが含まれる状態になっていることを確認してください．

テクスチャの利用  立方体（Cube）に犬の画像（001.png）をテクスチャとして貼り付けることで， マーカーを検出すると犬の画像を立体的（マーカーから少し浮いた状態）に表示してみます．
まずは，Assetsフォルダで[Create]-[Materials]を選択し，新規にマテリアルを作成します． マテリアルの名前はDogに変更しておきます． 画像フォーマットが透過背景であることから，InspectorでShaderをUnit/Transparentに設定します． Unit/Transparentはテクスチャ画像のアルファ値を反映して透過にすることが可能なシェーダーです． 次に，Textureを犬の画像に変更します これで，オブジェクトに設定するマテリアルが準備できました．


次に，シーンのARPictureSceneをダブルクリックします． ここで，Hierarchyから[Create]-[3D Object]-[Cube]を選択し，新規に立方体（Cube）のオブジェクトを作成します． オブジェクトの名前はDogObjectに変更しておきます． このDogObjectをドラッグして，MarkerObjectの直下に配置します． このとき，デフォルトで設定されている，Cubeオブジェクトは削除しておきます．
DogObjectのInspectorから，オブジェクトの**位置（Position）やサイズ（Scale）**を修正します． 位置はX=0，Y=0，Z=20とし，サイズはX=80，Y=80，Z=0とします（Z=0とすることで幅がなくなり平面となります）．

最後に，DogObjectのMaterialsをクリックして，作成したDogを選択しておきます． 再生ボタンをクリックすると，犬の画像（001.png）が表示されることを確認してください． テクスチャ画像が上下反対に張り付けられる場合は， TilingのYの値を**-1**に変更します（Direct3DかOpenGLで振る舞いが異なるようです）．


次回は画像の代わりに3Dオブジェクトを表示することにに挑戦してみます．
参考書籍</description>
    </item>
    
    <item>
      <title>NyARToolkit for Unityの導入</title>
      <link>https://mukai-lab.info/pages/tech/unity/artoolkit/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/unity/artoolkit/</guid>
      <description>NYARtoolkitとは  ARToolkitをベースに開発された拡張現実ライブラリが**NyARToolkit**です． ゲーム開発環境である**Unity**で利用可能な**NyARToolkit for Unity**の導入方法を紹介します． ここでは，**Unity**のバージョンは**5.3.5**，**NyARToolkit for Unity** **のバージョンは**5.0.8**を対象とします． また，NyARToolkitのライセンスは**LGPLv3**となっています（商用ライセンスもあるようです）． このライセンスは，「著作権の表示」を条件に，商用利用や配布が認められています（詳細はWikipediaを参照）．
プロジェクトの作成  Unityで新しいプロジェクトを作成します． ここでは，プロジェクト名を「ARPictureBook」としています． また，ゲーム環境は「3D」を選択しておきます．

次に，「NyARToolkit for Unity」のパッケージをインストールします． パッケージは下記URLからダウンロードできます．
https://github.com/nyatla/NyARToolkitUnity/releases
ツールバーから[Assets]-[Import package]-[Custom package]をクリックして， ダウンロードしたパッケージを選択します． ファイルの読込み後に，ダイアログが表示されたら， 全てのファイルにチェックを入れた状態でimportをクリックしましょう．

ファイルの取り込みが終わると，プロジェクトのAssetsには6つのフォルダが展開されます． Assetとは，ゲームを構成する最小の構成単位のことです． 例えば，シーン，キャラクター，画像ファイル，音楽ファイルなどもAssetです． ここでは，sampleフォルダに含まれるSimpleLiteを試しに実行してみましょう．

SimpleLiteを実行する前に下記の準備が必要です． ウェブカメラは標準的なモノであれば問題ないと思われます． また，マーカーはパッケージに付属しているMarkerHiro.pngを利用しますが， NyARToolkit用のマーカーは，tarotaroorg氏が公開している オンラインのツールを利用して，自由に作成することも可能です．
 ウェブカメラ（CMS-V30SETBKを使用） マーカーが印刷された紙（resourceフォルダに含まれるMarkerHiro.pngを印刷します）  
準備が整ったら，画面上部にある再生ボタンをクリックします． すると，ゲーム画面にカメラ映像が映し出されます． このカメラにマーカーを印刷した紙を映すと，マーカー上に赤色の立方体（Cube）表示されることを確認してください． これが，拡張現実と呼ばれる技術です．


フォルダのコピー  Assetsフォルダに新規フォルダを作成し，SimpleLiteのフォルダに含まれる２つのファイルをコピーします（ARCameraBehaviorはC#のスクリプト，simpleLiteはシーンと呼ばれるファイルです）． ここでは，C#スクリプトをARPictureCamera，シーンをARPictureSceneにファイル名を変更しておきます． 次に，ARPictureSceneをダブルクリックし，HierarchyのCameraをクリックします． シーンに関連付けられたコンポーネントが表示されているので， ARCameraBehaviorを削除（Remove Component）します． さらに，Add Componentをクリックし，コピーしたARPictureCameraを選択しておきます． これで，ARPictureCameraに記述したスクリプトが，ARPictureSceneに関連付けられます．


しかし，このままではARPictureCameraがエラーとなり実行できません． これは，変更したファイル名とスクリプトのクラス名が一致しないことが原因です． そこで，ARPictureCameraのソースコードを表示し， クラス宣言部にあるクラス名を，ARCameraBehaviorからARPictureCameraに修正し，エラーを取り除きましょう． 最後に，再生ボタンをクリックして，SimpleLiteと同様の実行結果になることを確認してください．</description>
    </item>
    
    <item>
      <title>OpenCVを利用した視線位置の描画</title>
      <link>https://mukai-lab.info/pages/tech/eyetracker/eyetracker2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/eyetracker/eyetracker2/</guid>
      <description>OpenCVとは  前回はTobii Eye Tracker 4Cを利用して， ディスプレイに対する視線の位置を検出しました． 今回は画像処理ライブラリのOpenCVを利用して， 視線位置に合わせてウィンドウに図形を描画することに挑戦します． OpenCVは，インテル が開発したオープンソースのライブラリであり， 画像処理に関する様々なアルゴリズムを容易に実装することができます（2016年にインテルがItseezを買収）． また，C/C++，Java，Python，MATLAB のライブラリとして配布されており， プログラミング言語を問わず利用できることも魅力です．
図形描画の実装  前回までにライブラリの導入は完了しているので，今回はウィンドウを表示するところからスタートしましょう． ファイル名は MyEyeTrack2.py とします． まずは，OpenCV(cv2) と NumPy のモジュールをインポートします． ここで，NumPy には，np という別名を付けていることに注意してください．
 最初に，np.zeros メソッドを用いて，幅1980px，高さ1080pxの黒色の画像を生成します． このメソッドは値が0の配列を返値とし，引数には配列の長さを表す タプル と データタイプを渡します． ここでは，8ビットの符号なし整数（0~255）であるnp.unit8をデータタイプとしています． この画像を imshow メソッドで表示します． 第1引数の &amp;ldquo;MyEyeTrack&amp;rdquo; はウィンドウのタイトルバーに表示される文字列です．
 前回実装したコールバックメソッドを修正し，視線位置に白色の円を描画するように改良します． 左右の視線位置の平均値を円の中心座標とします． コールバックメソッドで取得される視線位置は， 標準化された値のためウィンドウのサイズを掛けて，ピクセル座標に変換しています． このとき，y座標はウィンドウのタイトルバーの幅を考慮して，50 だけ減らしています （本当はキャリブレーションをやらなきゃいけないけど）． 円を描くには，cv2.circle メソッドを利用します． 引数には，画像，中心座標，半径，色，枠線の太さ を指定します． ここで，枠線の太さに負の値を指定すると，塗りつぶしの円になります． また，global はグローバル変数の img を用いることの宣言です．
 上記で実装したコールバックメソッドをEyeTrackerオブジェクトに登録します．
 最後にwhile文でimshowメソッドを呼び出し，画像の再描画を繰り返します． ここでは，100ms毎に画像を描画しています． このとき，ESCキー が押されると，ループを終了し， コールバックメソッドの解除，ウィンドウの破棄，システムの終了を行います．
 では，プログラムを実行してみましょう． ここでは，四角形を描くように視線を動かしてみました． 視線に合わせて白い円が描画されていることが分かります． しかし，思っていたより視線を安定させるのは難しいです．</description>
    </item>
    
    <item>
      <title>Processingではじめての画像処理</title>
      <link>https://mukai-lab.info/pages/tech/processing/processing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/processing/processing/</guid>
      <description>Processingとは  Processingは，2001年からMIT Media Labでスタートしたオープンソースのプロジェクトです． 当初からプログラミング教育を意識して開発され，初学者でも視覚的なコンテンツ（ビジュアルコンテンツ）を容易に作成できることが大きな特徴です． デザイナーや建築家などの利用も多く，作品はニューヨーク近代美術館など多くの著名な美術館で公開されています． オープンソースであることにもこだわりが強く，活発なコミュニティが形成されており，ソフトウェアを拡張するためのライブラリやツールが多く提供されています． ウィンドウズ，マッキントッシュ，リナックスなどのプラットフォームで動作可能であり，公式サイトから無料でダウンロードすることができます． また，ProcessingはJavaをベースに開発されており，Javaによく似た文法でコードを記述できることも人気の高い理由です．
Processingの開発環境  前節で述べたように様々なプラットフォームで動作可能なパッケージが配布されていますが， 今回はオンラインでコーディングが可能なOpenProcessingを利用してみましょう． このサイトでは，Processingで制作した作品（スケッチ）を公開することができます． また，登録も無料で誰でも利用することが可能です． では，下記のリンクをクリックしてOpenProcessingのサイトを開きましょう．
OpenProcessing
残念ながら現在は英語のサイトしか存在せず，ユーザ同士の交流は英語が中心です． 一方で，日本語でコメントを記述することは問題ないようです． Processingに慣れてきたらオリジナルの作品を投稿することを目指しましょう．

まずは，公開されているスケッチを閲覧してみましょう． 「Activity」のタブをクリックすると，右下に評価の高い作品の一覧が表示されます． これらのスケッチはOpenProcessingの登録者が制作した作品です． 自由にスケッチを選んでプログラムを実行してみましょう． 下記のは2017日6月9日現在のスケッチのリストです．

幾つか人気のある作品を取り上げてみましょう． まずは，aadebdeb氏のrainbow spinという作品です． マウスの動きに合わせて，描かれている虹色の渦巻きの回転方向や倍率が変化します．
 次は，Victor Galve氏のPractica 2という作品です． マウスでクリックすると，様々な色や形の花火が打ちあがります．
 最後は，Raven Kwok氏のNoise Turbulence Doodlesという作品です． マウスをドラッグすると円が重なりながら奇妙な形に成長します．
 スケッチの作成  新しいスケッチを作成してみましょう． サイトのトップから「+Create a Sketch」をクリックしてください． 下記のようにソースコードを入力する画面になります（デフォルトで記述されているコードは削除しておきましょう）． ソースコードを記述した後で，をクリックすると，記述されたコードを実行することができます．

Processingをウェブ上で実行するには「P5js」，または，「Processing.js」というライブラリを利用します． 今回は後者の Processing.js を利用するため，画面右の Settings で， Mode を Processing.js に変更してください．

図形を描く前に，スケッチの大きさや背景色を設定しましょう． スケッチの大きさは300x300ピクセル，背景色は白にします． 大きさを設定するにはsize(幅,高さ)，背景色を設定するにはbackground(色)と記述します（255は白色を意味します）． 下記のソースコードを入力したら，をクリックしましょう． 背景色が白色のスケッチが表示されます（300x300ピクセルのスケッチが中央に配置されています）． （「//」が先頭にある文章はコメントと呼ばれ，プログラムとは認識されません） もとのソースコードの入力画面に戻るにはをクリックします．</description>
    </item>
    
    <item>
      <title>QRコードの読み取り</title>
      <link>https://mukai-lab.info/pages/tech/robohon/robohon3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/robohon/robohon3/</guid>
      <description>ロボホンのカメラ  ロボホンの開発キットRoBoHoN Software Development Kitに 同梱されているカメラ用のアプリ SampleCamera を基にQRコードの読み取り機能を実装します． QRコードの読み取りには，オープンソースとして提供されているZXingを採用します． ちなみに， ZXing の名称は Zebra Crossing が基になっているようです． ZXing の実装方法はNAVERまとめの記事「Java QRコード読み取り」を参考にし， ライブラリはMaven RepositoryからダウンロードしたJARファイルを利用します． 現時点での最新バージョンは 3.3.0 です（2017年9月20日現在）．
カメラの利用  まずは，SampleCamera のプロジェクトを Android Studio で開き確認しましょう． MainActivity.javaがこのサンプルの中心となるソースファイルです．
ロボホンのカメラを利用した撮影には，静止画，動画，また，顔認識の有無などの設定が可能です． ここでは，写真撮影 顔認識無ボタン の挙動にQRコードの読み取り機能を加えます． 下記が該当部分のソースコードです． 背面のモニタにあるcameraButtonをタップすると，getIntentForPhoto メソッドの 返り値（Intent クラスのインスタンス）が，sendBroadcast メソッドで通知される処理となっていることが分かります．
 次に，getIntentForPhoto メソッドを確認します． ここで，登場する ShootMediaUtil クラスが重要な役割を担います． Intent クラスのコンストラクタの引数には静止画撮影用のアクション名であるShotMediaUtil.ACTION_SHOOT_IMAGEを指定します． また，アクション（撮影）終了後の結果通知を得るために，putExtraメソッドで ShotMediaUtil.EXTRA_REPLYTO_ACTIONを指定し， ACTION_RESULT_TAKE_PICTURE をその返り値としています． このインテントを sendBroadcast で通知することで，カメラの撮影機能が実行されます．
 カメラの撮影後には，結果通知として ACTION_RESULT_TAKE_PICTURE を CameraResultReceiver クラスの onReceive メソッドで受け取ります． ここでは，ACTION_RESULT_TAKE_PICTURE に該当するコードのみを抜き出してみます． 撮影が成功していれば，インテントから ShootMediaUtil.RESULT_OK を受け取り， 保存した画像ファイルのパスを取得します． 取得されたパスは，リソースIDを利用して，ロボホンの背面の TextView に表示されます．</description>
    </item>
    
    <item>
      <title>Tobii Eye Trackerを利用した視線の認識</title>
      <link>https://mukai-lab.info/pages/tech/eyetracker/eyetracker1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/eyetracker/eyetracker1/</guid>
      <description>Tobii Eye Trackerとは  近年，ユーザの視線を検出するアイトラッキングという技術が注目されています． アイトラッキングはマウスやキーボードの代替となりうるヒューマンインターフェイスの一つであり， 手足が不自由な障がい者のコミュニケーション装置としても活用されています． 光学センサーと画像処理技術を用いて，眼球の動き を解析することで， ユーザの注視点（見つめている場所）を推定する方法が一般的です． 今回は，トビー・テクノロジー株式会社が ゲーム用に提供しているEye Tracker 4Cという 視線入力装置を利用して，ユーザの視線を検出するプログラムを実装してみます． トビー・テクノロジーは自社の製品を利用したソフトウェアを開発するための Tobii Pro SDKを独自に提供していますが， Eye Tracker 4CでTobii Pro SDKを利用するためには， プロアップグレードキーが必要となることに注意が必要です （Tobii Core SDK や Tobii Gaming SDK という選択肢もありますが，研究やデータ分析を用途とする場合は Tobii Pro SDK を利用する必要があります）． また，開発用の言語には Python，Matlab，C，Unity などに対応していますが， ここでは，機械学習に適したPythonを採用します． Tobii Pro SDK のドキュメントが公開されおり，この情報を参考にしながら開発を進めることになります．

開発環境の準備  まずは，Pythonをインストールします． Pythonのバージョンには3.xと2.xの２通り存在しますが， Tobii Pro SDK に対応している 2.x を選択する必要があることに注意してください． ここでは，現時点での最新バージョンである2.7.14を利用します(2017年10月17日)．
Pythonの本体に加えて，画像処理ライブラリのOpenCVと， 数値計算ライブラリのNumPyを追加でインストールします． インストール方法はPythonのパッケージ管理システムであるpipを利用すれば簡単です． コマンドプロンプトで下記のように入力します． ここでは，3.3.0.10 のOpenCVと 1.13.3 のNumPyがインストールされました．
$ python -m pip install opencv-python Collecting opencv-python Downloading opencv_python-3.</description>
    </item>
    
    <item>
      <title>トリガを用いた発話</title>
      <link>https://mukai-lab.info/pages/tech/robohon/robohon2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/robohon/robohon2/</guid>
      <description>サンプルアプリ  ロボホンの開発キットRoBoHoN Software Development Kitには， 下記のサンプルアプリが同梱されています． 今回は基本的な音声UIの使い方を理解するためSampleSimpleのソースコードを解析し， 改良してみましょう．
 SampleSimple 音声UIを使った基本的な機能のアプリ SampleScenario シナリオで使える変数やタグなどのアプリ SampleProjector プロジェクターを利用したアプリ SampleAddressBook 電話帳を利用したアプリ SampleCamera カメラを利用したアプリ SampleDance ダンスを利用したアプリ SampleMultilingual 多言語対応を実装したアプリ  まずは，Android Studio で上記のサンプルを読み込み，プロジェクトとして展開します． Android Studioを起動したら，[File]-[Open]-[Open File or Project]を選択し， 開発キットに含まれる SampleSimple のフォルダをクリックします． ビルドが終了したら，Run Appをクリックして実行してみましょう．
実行するとロボホンの背面には「ACCOST」「RESOLVE VARIABLE」「SET_MEMORY_P」「GET_MEMORY_P」「FINISH APP」の５つのボタンが表示されています． ここで，「ACCOST」ボタンをタップすると，「アプリから発話開始するサンプルだよ」とロボホンが発話します． ちなみに，ACCOSTとは，アプリから強制的にトピック（発話やモーション）を実行することを意味します．

 アプリトリガによる発話  最初に，ボタンなどアプリからのアクションをトリガとして，トピックを実行するアプリトリガによる発話について学びます． まずは「ACCOST」ボタンに該当するソースコードを確認していきましょう． 一般にアンドロイドのアプリはActivityクラスを継承して開発します． Activityクラスにはライフサイクルがあり，アプリを起動すると下図のようにメソッドを実行します．

ここで，「ACCOST」ボタンに関する振る舞いは，onCreate()メソッドの内部にあり，下記のように記述されています． 最初に，リソースIDを利用して，変数voiceAccostButtonに，Buttonクラスのオブジェクトを代入し，ボタンをタップしたときのイベントリスナーを登録しています． ボタンがタップされると，VoiceUIVariableListHelper クラスのインスタンスに実行したいACCOSTを登録し，VoiceUIManagerUtil クラスのupdateAppInfoメソッドで発話を実行しています． ここで，ScenarioDefinitions.ACC_ACCOST には，実行するACCOSTの名称である jp.co.sharp.sample.simple.accost.t1 が代入されています．
 ここでのポイントは，実行するACCOSTの定義です． 具体的な定義は，シャープ株式会社が独自に定義している HVML(Hyper Voice Markup Language) というXMLファイルに記述します． ここでは，assetsフォルダに含まれるjp_co_sharp_sample_simple_accost.</description>
    </item>
    
    <item>
      <title>ロボホンの開発環境の構築</title>
      <link>https://mukai-lab.info/pages/tech/robohon/robohon1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/robohon/robohon1/</guid>
      <description>ロボホンとは  ロボホン（RoBoHoN）はシャープ株式会社が開発する人型のロボットであり，音声UIを利用して自然言語で会話が出来ることを特徴としています． ロボホンのOSはグーグル社が開発するAndroidであり，Android用のアプリ開発と同様にロボホンのアプリを制作することができます． ロボホンの開発に必要なRoBoHoN Software Development Kitが公開されており， ガイドラインに従って音声UIを利用したアプリを開発することが可能です． ここで，ガイドラインとは，ロボホンの世界観を維持するための，ロボホンの キャラクター ， 話し方 ， ユーザとの関係性 のことを指しています． 今回は，ロボホンのアプリ開発に必要な環境の構築方法に関して解説します．
開発環境  ロボホンアプリの開発には下記の環境が必要です． ココロプランには，「ビジネス基本プラン」「ビジネスプラン2000」など数種類が設定されており， プランに応じて月毎の会話上限が定められています． アプリ開発で頻繁に音声UIを利用する場合は高額なプランも検討する必要があります．
 ロボホン本体（現状ではエミュレータは存在しない） ネットワーク環境（音声UIを利用するため） ココロプランの契約（本体購入時に同時契約）  開発プラットフォームとしては，Android Studioを利用します（ロボホンのOSはAndroid 5.0です）． また，Android Studio のバージョンは 1.5 以降が必要とされており，今回は現時点での最新版である 2.3.3.0を採用します(2017年9月18日)． ファイルサイズは約1.9GBと，かなり大きいので注意してください．

Android Studioのインストール・パッケージをダウンロードしたら，インストールを始めましょう． インストールが完了したらロボホンのアプリ開発に必要なパッケージを追加でインストールします． まずは，バージョン 1.5 の Android SDKです． Settingsのメニューから[Appearance&amp;amp;Behavior]-[System Settings]-[Android SDK]を選択し， Android 5.0 (Lollipop) にチェックを入れて Apply をクリックします．

同様にAndroid SDK Build-Toolsを追加します． バージョンは21.0.0以降にチェックを入れて Apply をクリックします（Show Package Detailsをクリックするとバージョンの選択が可能です）．

開発したアプリをデバッグする際に，音声対話に失敗することを回避するために， Instant Run の設定を無効化します． Settingsのメニューから[Build, Execution, Developemnt]を選択し，全てのチェックをはずします． これで，Android Studio の準備は完了です．</description>
    </item>
    
    <item>
      <title>脳波データをフーリエ解析</title>
      <link>https://mukai-lab.info/pages/tech/emotiv/emotiv3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://mukai-lab.info/pages/tech/emotiv/emotiv3/</guid>
      <description>フーリエ変換とは  前回までにCortexを利用してEmotiv Epoc+で計測したデータをPythonから取得する方法を説明しました． 一般に，脳波の生データから情報を読み取ることは困難であり，周波数成分に変換することが必要になります． この周波数成分の変換に必要な技術が鬼門のフーリエ変換（Fourier Transform: FT） です． 大学の工学部ではカリキュラムの１つとなっているフーリエ変換（フーリエ級数）に躓いた人も多いのではないでしょうか（何を隠そう向もその一人です）． ここでは，あまり深く考えず，フーリエ変換は 時間領域 から 周波数領域 に変換する仕組みだと理解しておけば十分です．
例を挙げて考えてみましょう． Emotivのサンプリングレートは 128Hz であることから，1秒間に 128個の信号が計測されることに注意してください． 仮に計測された脳波が，下記のグラフのような信号だったとします． この信号には，1秒間に 3周期分 のSin関数が含まれています． グラフの横軸が 時間 であることから，このデータは 時間領域 に存在しています．

これを，フーリエ変換（高速フーリエ変換）すると下記のグラフになります． このグラフの横軸は 周波数 であり，このデータは 周波数領域 に存在しています． 周波数 が 3 のところに，縦棒がありますよね． この結果は，先のグラフには 周期3 の信号が含まれていることを表しています．

フーリエ変換の凄いところは，異なる周期の信号が混ざっていても，それぞれの周波数成分の強さが取得できることです． 次に，下記のグラフの信号を考えます この信号には，1秒間に 5周期分 のSin関数が含まれています． また，その振幅は先の信号と比べて 1/2 の大きさです．

この信号を，先の周期3の信号に加えます（本当に足し算するだけ）． すると，下図のようなグラフになります． これだけで，複雑なグラフになり，人間には理解できないレベルに到達します．

さぁ，フーリエ変換の出番です． 上の周期3と周期5のグラフを加えた信号を変換すると，下記のグラフになります． 周期3 の振幅は 1，また，周期5 の振幅が 0.5 となっていることが読み取れますね． フーリエ級数を発明したジョゼフ・フーリエは本当に天才だと思います（真面目）．

脳波データの記録  それでは，前回までに実装したプログラムを利用して脳波データを取得しましょう． 準備として，数値計算ライブラリのNumpyと，描画ライブラリのmatplotlib.</description>
    </item>
    
  </channel>
</rss>