<!DOCTYPE html>
<html lang="ja">
    <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    Tech | mLAB
  </title>

  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
  <link rel="stylesheet" href="/css/custom.css">
  <link rel="stylesheet" href="/css/syntax.css">
  
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png">

  
  <link href="/categories/tech/index.xml" rel="alternate" type="application/rss+xml" title="mLAB" />

  
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Kosugi&display=swap" rel="stylesheet">

  
  <script src="https://kit.fontawesome.com/0c97f11cd6.js" crossorigin="anonymous"></script>

  
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.js"></script>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.css"/>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick-theme.min.css"/>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/SlickNav/1.0.10/jquery.slicknav.js"></script>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/SlickNav/1.0.10/slicknav.min.css" />

  
  <div id="fb-root"></div>
  <script async defer crossorigin="anonymous" src="https://connect.facebook.net/ja_JP/sdk.js#xfbml=1&version=v9.0"></script>  
  
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
	     tex2jax: {
	        inlineMath: [['$','$']]
        }
    });
  </script>

  
  

  
  <script src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
  
  
  

  <meta property="og:title" content="Tech" />



  <meta property="og:type" content="article" />



  <meta property="og:url" content="https://mukai-lab.info/categories/tech/" />



  <meta property="og:image" content="https://i.gyazo.com/620adf7e5d9b2cdc983a80d01b7e3367.png" />



  <meta property="og:site_name" content="mLAB" />



  <meta property="og:description" content="椙山女学園大学 文化情報学部 向研究室のブログです．女子大教員の不思議な日常を４コマ漫画でお伝えします．" />



  
  
<meta name="twitter:card" content="summary" />

<meta name="twitter:site" content="@nmukai1978" />


  <meta name="twitter:title" content="Tech" />



  <meta name="twitter:description" content="椙山女学園大学 文化情報学部 向研究室のブログです．女子大教員の不思議な日常を４コマ漫画でお伝えします．" />



  <meta name="twitter:image" content="https://i.gyazo.com/620adf7e5d9b2cdc983a80d01b7e3367.png" />


  
</head>

    <body>
        <nav class="nav">
  <div class="nav-container">
    
    <a href="/">
    <h2 class="nav-title">
      <img src="https://mukai-lab.info/favicon/favicon-48x48.png" align="top"/>
      <span>mLAB</span>
      

<ul id="menu">
  
  
  <li>
    <a href="/">Top</a>
  </li>
  
  
  
  <li>
    <a href="/classes/">Classes</a>
  </li>
  
  
  
  <li>
    <a href="/posts/">Comics</a>
  </li>
  
  
</ul>

<script>
  $(function(){
    $("#menu").slicknav({
      label: "メニュー"
    });
  });
</script>



    </h2>
    </a>
  </div>
</nav>

        

<main>
    <h1>Category Tech</h1>
	<div class="catalogue">
		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/emotiv/emotiv2/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">Cortexで脳波データの取得</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/a097e50f64142dd6cf07b41f3b8ee1ea.png"></img>
	
      </div>
      
      <p>
	CortexとはPython を経由して，Emotiv Epoc+ で計測したデータを取得してみましょう． データを取得するには，Emotivの標準APIであるCortexを利用します． 今回はPythonを利用しますが，このCortexは JSON と WebSockets で実装されているため， Python以外のプログラミング言語でも利用可能です． また，Cortexは，EmotivProと一緒にインストールされますが， Cortex UIというソフトウェアで動作確認が可能です． 下図のように対象のデバイスが検出されていればOKです． コンタクト・クオリティは今回も35%とパッとしません（笑）．

ライブラリの導入Pythonはウィンドウズ版のバージョン3.7.2を用いることにします． Cortexでは，データの送受信には WebSockets という技術を用います． ここでは，WebSockets のクライアント（データ受信側）として実装するため， websocket-client ライブラリをインストールしておきます．
$ pip install websocket-client また，WebSocketsのポート番号は 54231 です． URLには wss://localhost:54321 を指定します． ここで，wss はWebSocketsのプロトコルを表しています．
手順１：認証Cortexから脳波の生データを取得するには，クライアントIDとシークレットで認証が必要です． この，クライアントIDとシークレットは，EMOTIVのユーザページで事前に取得しておきましょう（ライセンス番号も必要）．
まずは，CORTEXで用いられるJSON-RPCについて簡単に説明します． 上述したように，サーバとクライアント間のデータのやりとりをJSON形式で行うという仕組みです． クライアントが送信するリクエストの基本フォーマットは下記です． ここで，プロトコルバージョンの jsonrpc は常に 2.0 を指定します． メソッドやパラメータには，認証（authorize）やデータ取得（subscribe）などの文字列を指定します．
{ &#34;jsonrpc&#34;: &#34;2.0&#34;, &#34;method&#34;: メソッド, &#34;params&#34;:{ パラメータ: 値 }, &#34;id&#34;: ID番号 } また，クライアントが受信するレスポンスの基本フォーマットは下記となります． サーバーからの応答結果は result に格納されています． また，リクエストと同じID番号が付与されています．
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/emotiv/emotiv/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">Emotivで脳波を可視化</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/7ccfb248a92d7521fe8851edaab45742.png"></img>
	
      </div>
      
      <p>
	
EmotivとはEmotivは株式会社イノバテックが提供している小型の脳波計です． Epoc+，INSIGHT，EPOC flexなど複数の種類がありますが， ここでは，14チャネルの電極が装着されたEMOTIV EPOC+を対象に開発することを目的とします． この装置は，他の脳波計と比べ，スタイリッシュなデザインが特徴です． 頭に装着するだけで，未来感がビシビシ伝わってきます． 専用のアプリケーションであるEmotivPROで， 脳波データを取得・可視化するだけなら無料のラインセンスで利用可能です． しかし，脳波の生データをAPI（Cortex）を経由して取得するなど， 研究を目的として利用するためには，別途ライセンスの購入が必要です． 月額99ドルと決して安くはないため注意が必要です（年払いで割引されますが）．

EmotivPROを使ってみよう最初に，専用のアプリケーションであるEmotivPROを利用して， 脳波データを取得・可視化してみましょう． まずは，電極のフェルドパッドを生理食塩水で水和させます． この生理食塩水はコンタクトレンズなどにも用いられているものです． ここで，しっかりと水和させておかないと，センサーの感度が悪くなる可能性があります． 電極を時計回りに回すと取り外すことができますが， 力を入れ過ぎるとフェルト部分がはずれてしまうので要注意です．

それでは，ヘッドセットを頭に装着します． ヘッドセットを両手で持ち，頭上から下に向かってスライドさせます． このとき，左右の参照電極を，ちょうど耳たぶの裏の骨部分に配置します． 参照電極を用いた方法は単極導出と呼ばれ，この参照電極と他の電極との電位差が記録されます． この参照電極は，他の電極にも影響するため，正確な設置が必要です． また，前方の左右の電極は，眉毛から指３本だけ上にあるように配置します．

この電極の設置の質（コンタクト・クオリティ）はEmotivProの下記の画面で確認できますが，・・・，非常に難しい． 緑色の電極はコンタクト・クオリティが高いことを示しています． 電極の位置を微調整しても，コンタクト・クオリティは全体で**28%**がやっとでした． このあたりのコツをご存知でしたら，お教えください．

脳波を可視化しよう次は脳波を可視化してみましょう． ここで，脳波のサンプリングレートとA/D変換の分解能を確認しておきます． サンプリングレートは128Hz，また，分解能は16ビット（65536階調）です． 図中にEEGと表記がありますが，これはElectroencephalographの略で脳波（図）を意味しています （単に脳波をEEGと呼ぶことも多い）．

次に電極の位置番号を確認しておきます． 上述したようにEpoc+では，14チャネルの電極があり， それぞれ，AF3，AF4，F3，F4，F7，F8，FC5，FC6，T7，T8，P7，P8，O1，O2です． 図中の赤い丸で表現されている電極は，耳たぶの裏にある参照電極です． ここでは，コンタクト・クオリティの高いAF4，F3，P8の３箇所に注目します．

AF4，F3，P8の３箇所の電極の生データは下記です． 横軸は時間で単位は10[ms]，また，縦軸は電位を表し**-100µV**〜**100µV**の範囲を取ります． 一般的にローデータから，脳波の意味を読み取ることは困難なため，高速フーリエ変換をして周波数成分を取り出します．

EmotivProはフーリエ変換した周波数スペクトルの表示も可能です． P8にフーリエ変換をした結果は下記のようになります． 周波数に応じて脳波は分類され，4〜8Hzはシータ波，8〜12Hzはアルファ波，13Hz以上はベータ波と呼ばれます． この結果では，若干ですがシータ波が有意なように見えます． シータ波は眠い状態で発生する波とされ，いかに寝不足かが分かる結果となりました（笑）．

今回はEmotivの基本的な使い方を解説しました． 次回はCortexと呼ばれるAPIを利用して，Pythonで脳波データを取得することに挑戦します．
参考書籍
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/unity/google_vr2/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">Google VR SDKで視線の検知</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/94f1fd7ba2ea456b3500e3e87b9e8947.png"></img>
	
      </div>
      
      <p>
	視線の検知VR環境では，タップなどスマートフォンで一般に用いられる操作が出来ません． そこで，Google VRでは， ユーザの視線による操作を可能としたGoogle VR Pointer System が用意されています． 今回は，視線を利用したオブジェクトの選択を実現してみましょう． 事前に準備が必要なプロジェクトの作成やパッケージの導入などは， Google VR SDKを利用した360°画像ビューアーを参考にしてください．
シーンの作成シーンを新規に作成します． ここでは，シーンの名前は[MenuScene]とします． まずは，Google VR SDKを利用した360°画像ビューアーを参考に， 空のオブジェクトであるVR Cameraを作成し，直下にMain CameraとGvrEditorEmulatorを設定します．
次に対象となるQuadオブジェクトをシーンに配置します． Quadオブジェクトを配置するには，[GameObject]-[3D]-[Quad]を選択します． Quadオブジェクトのインスペクターを開き，Position の Z座標 を5に設定しましょう． また，新規にマテリアルを作成し，Quadオブジェクトに追加しておきます（マテリアルに関しては割愛します）． シーンを再生すると下記のように表示されます．

視線の検知上記で作成したQuadオブジェクトを視線に捉えていることを検出してみましょう． まずは，ヒエラルキーにGvrEventSystemをドラッグ＆ドロップで配置します． GvrEventSystemは視線に関するイベントの包括的な処理を行います． また，ヒエラルキーのMain Cameraの直下に，GvrReticlePointer を配置します． GvrReticlePointerは，ユーザが見つめている一点をポインタで表します． 対象となるオブジェクトを見続けているとポインタが拡大します． シーンを再生すると下記のように表示されます． 中央に視点を表すポインタが表示されていることが分かります． 現時点ではQuadオブジェクトを見続けてもポインタに変化はありません．

次に，視線を検出するためのスクリプトをMain Cameraに設定します． Main CameraのインスペクターでAdd Componentを選び，Physics Raycaster を選択します（Physics Raycasterはスクリプトであることに注意）．

また，対象となるQuadオブジェクトにEvent Triggerを設定します． QuadオブジェクトのインスペクターでAdd Componentを選び，Event Trigger を選択します（Event Triggerもスクリプト）．
シーンを再生すると下記のように表示されます． 中央にあるポインタがQuadオブジェクトを捉えると， ポインタが大きな円に変形することが分かります．

シーンの切替Quadオブジェクトを2秒間見続けるとシーンを切り替えるようにします． QuadオブジェクトのインスペクターでAdd Componentを選び，Net Script を選択します（C#で記述する）． ここでは，スクリプト名はCntrol Sceneとします．
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/unity/google_vr/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">Google VR SDKを利用した360°画像ビューアー</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/adf9cd1b07e2aab6bd9365cd25b33a5a.png"></img>
	
      </div>
      
      <p>
	Google VR SDK for Unity近年，VR(Virtual Reality)に関する技術は目覚ましいスピードで発展しています． Oculus社が開発する「Oculus Rift」， SCE(Sony Computer Entertainment)の「PlayStation VR」など， 本格的なVRのためのヘッドマウントディスプレイも手に入れることができます． しかし，これらの製品はまだまだ高価であり，気軽に導入することは難しいです． このような状況のなか，Googleは，スマートフォンを組み合わせて利用する， ダンボール製の安価な「Cardboard」というヘッドマウントディスプレイを提供しています． 同時に，Googleは，「Google VR」という， VRに関するプロジェクトを立ち上げ，開発者向けのツールも提供しています． そこで，今回は，ゲームエンジンの一つであるUnityで， Cardboard向けVRを開発可能な「Google VR SDK for Unity」を利用したアプリを制作してみます． また，VR環境には，リコーが開発する「RICHO THETA S」で撮影した360°画像（全天球画像）を用い， ヘッドセットの動きに合わせて，360°画像を閲覧できるようにします． 開発に当たり，SlideShareで公開されているOculus Rift勉強会の資料 THETAでモバイルVRコンテンツ開発を参考にさせて頂きました．
準備アプリの開発に当たり下記の機器を利用します． ヘッドマウントディスプレイにはCardboardではなく， サンワサプライが販売している「VR SHINECON」を利用します． ヘッドホンが搭載されており，Cardboardに比べると高級感があるモデルです．
また，360°画像（全天球画像）の撮影にはRICHO THETA Sを採用します． 静止画・動画に対応しており，動画のライブストリーミングも可能なモデルです．
 VR SHINECON RICHO THETA S  下記がRICHO THETA Sで撮影したサンプル画像です． マウスを使って画像をスクロールすると，教室に一人で寂しそうに立っている向の姿が見えるはずです． ここでは，ウェブで360°画像を表示するために「VR view on the web」を利用しています． また，RICHO THETA Sで撮影された画像の解像度は5376x2688ですが，VR viewに最適な4096x2048に変換してあります．
 プロジェクトの作成Unityでプロジェクトを作成します． Unityのバージョンは5.
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/enchant_js/moonblock/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">MOONBlockでゲームプログラミング</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/b8f987aeb531e03dd9cd3a076d09a0b3.png"></img>
	
      </div>
      
      <p>
	MOONBlockとはMOOBlockは株式会社ユビキタスエンターテインメントが 開発する教育用のプログラミング言語です． 普通のプログラミング言語（C言語など）は複雑な命令や制御を理解する必要がありますが， MOONBlockではブロックを並べるだけで簡単にプログラミングできるという特徴があります （同様の方法でプログラミングが可能なMITメディアラボ開発のSCRATCHも有名です）． 事前にソフトウェアをインストールする必要もなく， ウェブブラウザ（IE，Chromeなど）がインストールされていれば動作させることが可能です． 今回はこのMOONBlockを利用して簡単なゲームを制作することを目指しましょう．
MOONBlockの基本MOONBlockはゲームエンジンの一つであるenchant.jsがベースとなっています． このため，MOONBlockで作成したプログラムを基にPC，Mac，iOS，Androidなど様々なプラットフォームで動作するアプリケーションを開発することが可能です． enchant.jsはMITライセンスであり，ソースコードの著作権の表示を条件に， 「ソースコードの改変」「再配布」が認められています （詳細はenchant.jsのライセンスを参照）．
また，制作したゲームをゲーム投稿サイトの9leapに投稿することで， ゲームを体験したユーザから様々なフィードバックを得ることができます（コンテストに応募して賞金を狙うことも）． 下記は研究室の学生が制作し9leapに応募した作品です． 時間があるときに遊んでみてください．
星を集めようはスライムを避けながら星（スター）を３つ集めるゲームです．
 イライラ棒アプリは木，森などの障害物に当たることなく，ゴールを目指すゲームです．
 ひよこマンは，３種類に変身するひよこを操りながら，鍵を集めてゴールに向かうゲームです．
 MOONBlockのゲーム素材画像ゲームを制作する際に必要なアイコン，キャラクター，背景などの素材画像は， MOONBlockであらかじめ用意されています（非営利目的であれば自由に利用可能）． アイコンは16×16ピクセル，キャラクターは32&amp;times32;ピクセル， 背景は320×320ピクセルで構成されています． これらの素材画像を重ねて表示することでゲーム画面は構成されます．
ここでキャラクターの素材画像（chara1.png）に注目してみましょう． 少しずつ異なるクマの画像が横に並んでいることが分かります． これらの画像を，パラパラ漫画の要領で，素早く切り替えることでクマが動いているように見せることができます．

Chara1.png

icon0.png

rpg.png
MOONBlockの開発画面下記のリンクをクリックして，MOONBlockのサイトを開きましょう！
http://www.moonblock.jp/
画面上に並んでいる「パペット」「ビヘイビア」などの箱を「キット」と呼びます． キットにはプログラミングに必要なブロックが使用目的に合わせて別れて入っています． キットは左右にドラッグすることで全ての種類を確認できます．
画面右上にある青い正方形は実行画面です． プログラムの実行結果はここで確認できます． 大きさは背景画像と同じ320×320ピクセルです．
画面左下にあるボタンをクリックすると，プログラムの保存や，プログラムの実行ができます． 一端保存して自宅でプログラミングの続きをしたり，9leapに作品を投稿するときに利用しましょう．
画面右下にあるゴミ箱には，不要になったブロックを入れます． 一度捨てたブロックは復元できないので注意してください．

くまのバナナ拾いゲームの制作くまが画面内にあるバナナを拾うゲームを作ってみましょう．
まずは，クマのキャラクターを画面に出現させましょう． 「パペット」キットから「パペット」ブロックをドラッグしてワークスペースに配置します． パペットとは”操り人形”の意味であり，MOONBlockではクマなどのキャラクターやバナナなどのアイテムを指します． 次に，「ビヘイビア」キットから「出現」ブロックを引き出し，「パペット」ブロックに下図のように接続します． ビヘイビアとは”振る舞い”の意味であり，パペットの動きなどを設定するときに用います． Runボタンをクリックして実行してみましょう． このときのブロックの状態はここから確認できます．
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/unity/artoolkit4/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">NyARToolkit for Unityで3Dモデルのアニメーション</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/1a1f0c692c93350bcbf3157ff85ff93a.png"></img>
	
      </div>
      
      <p>
	
3Dモデルのアニメーション前回に引き続き，Unityのオリジナルキャラクターである「ユニティちゃん」を利用して， 3Dモデルにアニメーションを設定してみましょう（詳細はNyARToolkit for 3Dモデルの表示を参照）．
Unityで3Dモデルにアニメーションを設定するときは，Animator ControllerというAssetを利用します． 「ユニティちゃん」のパッケージには，UnityChanLocomotions，UnityChanActionCheck，UnityChanARPoseの ３種類のAnimator Controller **が含まれています．
今回は，UnityChanLocomotionsを「ユニティちゃん」に設定して，キーボードの入力に対応してアニメーションするように改良します．
Animation Controller[UnityChan]-[Animators]にAnimation ControllerのUnityChanLocomotionsがあります． これをドラッグし，unitychanのInspectorにあるControllerに設定します． これで，「ユニティちゃん」はUnityChanLocomotionsで定義されたアニメーションを行うことが可能となります．

ここで，UnityChanLocomotionsの内容を確認してみましょう． Animation Controllerは下図のようにグラフで定義されます． まずは，Entryから始まり，Idle状態に遷移することが分かります．

Idle状態のInspectorを確認すると， MotionにWAIT00が設定されていることが分かります． このWAIT00が3Dモデルの動きに対応します． また，Idle状態から，Locomotion状態，WalkBack状態，Rest状態の ３つの状態に遷移可能なことが分かります．

では，Idle状態から他の状態に遷移するための条件は何でしょうか． TransitionsのIdle -&gt; Restをクリックすると，状態遷移のための条件（Conditions）が表示されます． ここでは，Restという変数（パラメータ）がtrueであるときにRest状態に遷移することが分かります． 同様に，Locomotions状態に遷移する条件は，Speedが0.1より大きいとき， また，WalkBack状態に遷移する条件は，Speedが-0.1より小さいときということが分かります． このように変数（パラメータ）に基づき，3Dモデルの状態が確定します．

スクリプトによる状態遷移の制御次に，C#スクリプトを作成し，キーボードからの入力に応じて状態遷移を制御してみます． まずは，[Create]-[C# Script]をクリックして，新規にC#のスクリプトを作成します． ファイル名はARAnimationsとしておきます．
まずは，キーボードの1を押すとRest状態に遷移するようにしてみます． キーの入力判定はInput.GetKey()メソッドを利用します． 引数にはstring型で対象となるキーを指定します． また，状態遷移のトリガーとなる変数Restの値を設定するには， SetBool()メソッドを利用します． 引数には，String型で対象となる変数と，その値を指定します． 再生ボタンをクリックして，「ユニティちゃん」を表示した状態で，キーボードの1を押してみましょう． ユニティちゃんが背伸びをするアニメーションが表示されるはずです．
 次に，キーボードの2を押すとLocomotions状態，3を押すとWalkBack状態， 4を押すとIdle状態に遷移するようにしてみます． トリガーとなる変数Speedはfloat型のため，SetFloat()メソッドを利用して値を変更しています． 再生ボタンをクリックして，「ユニティちゃん」を表示した状態で，キーボードの2，3，4を押してみましょう． ユニティちゃんが走ったり，後ずさりするアニメーションが表示されるはずです．  

これまでに紹介した機能を利用して，拡張現実を利用した作品を制作してみてください．
参考書籍
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/unity/artoolkit3/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">NyARToolkit for Unityで3Dモデルを表示</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/8cb7bde6f98680bdc444e9022a6c9192.png"></img>
	
      </div>
      
      <p>
	
3Dモデルの準備NyARToolkitのパッケージに含まれる サンプルSimpleLiteはマーカーを検出すると赤色の立方体（Cube）を表示するプログラムです． このSimpleLiteを修正し，3Dモデルを立体的に表示できるよう改良します．
SimpleLiteフォルダをコピーし，フォルダに含まれるC#スクリプトをARPictureCamera， シーンをARPictureSceneにファイル名を変更した状態を前提とします （詳細はNyARToolkit for Unityの導入を参照）．
Unityでは**.3ds**，**.obj**などの一般的な3Dモデルのファイル形式を利用できますが， 今回はUnityのオリジナルキャラクターである「ユニティちゃん」を利用してみます． 「ユニティちゃん」はUnityの**アセット（素材）**として配布されているので導入はとても簡単です． また，このキャラクターは，ラインセンスロゴもしくはライセンス表記があれば，キャラクターの二次創作物の制作が認められています （詳細は「ユニティちゃんライセンス条項」を参照）．

まずは，「ユニティちゃん」の公式ページから，ライセンスに同意し，データをダウンロードします． データをダウンロードしたら，ツールバーから[Assets]-[Import package]-[Custom package]をクリックして， ダウンロードしたパッケージを選択します． ファイルの読込み後に，ダイアログが表示されたら，全てのファイルにチェックを入れた状態でimportをクリックしましょう． ファイルの取り込みが終わると，プロジェクトのAssetsに新しくUnityChanフォルダが展開されます．

3Dモデルの表示[UnityChan]-[Models]に「ユニティちゃん」の3Dモデルであるunitychanがあります． これを，ドラッグし，HierarchyにあるMarkerObjectの直下に配置します． すると下記のようにx=0，y=0，ｚ=0の位置に「ユニティちゃん」が配置されます．

次に，「ユニティちゃん」の**サイズ（Scale）や座標（Position）**を調整します． ここでは，Inspectorから，サイズの値をX=80，Y=80，Z=80，位置をX=0，Y=-40，Z=-20に修正しましょう．

再生ボタンをクリックすると，「ユニティちゃん」が表示されることを確認してください． 説明文なしに下記の画像だけだと完全に変な人かもしれません．

次回はAnimator Controllerを利用して「ユニティちゃん」に歩くなどの動きを付けることに挑戦します．
参考書籍
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/unity/artoolkit5/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">NyARToolkit for UnityでNFTを利用したマーカーの認識</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/399be0179fc2bc72d21c82d597d18d35.png"></img>
	
      </div>
      
      <p>
	画像の準備NyARToolkitのパッケージに含まれるサンプルSimpleNftは NFT(Natural Feature Tracking)を利用したマーカー認識のサンプルです． NFTとは自然特徴点を利用した物体認識のことで，この技術を利用することで，一般的な画像をマーカーとして扱うことができます （詳しくはThe Sixwish Projectを参照）． ここでは，下記の下記の「犬の画像（001.png）」と 「うさぎの画像（002.png）」を マーカーとして利用することに挑戦します． 画像サイズは300x300ピクセル，画像フォーマットは透過背景のPNGです． また，犬の画像を認識すると赤色の立方体，うさぎの画像を認識すると青色の立方体を表示させることにします．


SimpleNftフォルダをコピーし，フォルダに含まれるC#スクリプトをARPictureCamera， シーンをARPictureSceneにファイル名を変更した状態を前提とします（詳細はNYARToolkit for Unityの導入を参照）．
パターンファイルの作成上記の２種類の画像から，マーカーのパターンファイルを作成します． パターンファイルの作成には，NyARToolkitのパッケージに含まれるNftFileGeneratorを利用します （詳しくはNyARToolkit Projectを参照）． [Data]-[Tools]にあるNftFileGeneratorをダブルクリックするとプログラムが起動します． importをクリックし，特徴点を抽出したいJPGやPNGなどの画像を指定します（透過より背景白の方が認識精度が高いようです）． 次に，Make Feature Setをクリックし，特徴点を抽出します． このとき，Source DPIやIset DPIsなどのパラメータは特に変更する必要はないようですが， 特徴点数が多すぎたり，少なすぎる場合にはFSET parameterを調節すると良いようです．

抽出されたパターンは赤い四角や青い円で表示されます． パターンを保存するにはExportをクリックし，適当なファイル名で保存します． ここでは，「犬の画像」のパターンファイルをpatt_001.bytes， 「うさぎの画像」のパターンファイルをpatt_002.bytesというファイル名で保存します． これらのパターンファイルは，AssetsのResourcesフォルダにコピーしておきます．


マーカーオブジェクトの作成SimpleNftはマーカーを認識すると赤色の立方体を表示します． ここでは，２種類のマーカーに応じて，赤色の立方体に加え，青色の立方体を作成しておきます．
まずは，シーンのARPictureSceneをダブルクリックし，HierarchyにあるMarkerObjectを複製します． スクリプトからこれのオブジェクトを操作するため，２つのマーカーオブジェクトには共通のタグ（Tag）を設定します （Gameobject.FindGameObjectsWithTag(String tag)メソッドでタグからオブジェクトを取得することが可能）． タグ名は自由に設定することができますが，ここではMarkerObjectとしておきます．

次に，Assetsで赤色と青色のマテリアルを作成します． [Create]-[Materials]をクリックしてマテリアルを作成し，Albedoに赤色と青色を設定します． ここでは，マテリアルの名前をRedとBlueにしておきます． これらのマテリアルはMarkerObjectの直下にあるCubeのMaterialsに設定しておきます．

マーカーが認識されたときに表示されるのは，上記のCubeオブジェクトです． Cubeの**サイズ（Scale）や位置（Position）**はマーカーの大きさに合わせて調整する必要がありますが， ここでは，サイズの値をX=80，Y=80，Z=80，位置をX=-80，Y=80，Z=40に修正しておきます．

スクリプトの修正最後にC#スクリプトのARPictureCameraを修正します．
まずは，2つのマーカーオブジェクトを配列で取得します． マーカーオブジェクトはMarkerObjectというタグが設定されていることを利用します．
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/unity/artoolkit2/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">NyARToolkit for Unityで画像を表示</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/f6c58bee65721d185c566a05eaa41920.png"></img>
	
      </div>
      
      <p>
	
画像の準備NyARToolkitのパッケージに含まれる サンプルSimpleLiteはマーカーを検出すると赤色の立方体（Cube）を表示するプログラムです． このSimpleLiteを修正し，一般的なJPGやPNGなどの画像を立体的に表示できるよう改良します．
SimpleLiteフォルダをコピーし，フォルダに含まれるC#スクリプトをARPictureCamera， シーンをARPictureSceneにファイル名を変更した状態を前提とします （詳細はNyARToolkit for Unityの導入を参照）．
立方体の代わりに表示する犬の画像（001.png）は下記です． この画像は，テンプレートBANKを参考に，本学の学生が作成しました． 画像サイズは300x300ピクセル，画像フォーマットは透過背景のPNGです．

まずは，新規にanimalsフォルダを作成し，上記の画像をコピーしておきます． 対象のフォルダには，C#スクリプトのARPictureCamera，シーンのARPictureScene， 画像フォルダのanimalsが含まれる状態になっていることを確認してください．

テクスチャの利用立方体（Cube）に犬の画像（001.png）をテクスチャとして貼り付けることで， マーカーを検出すると犬の画像を立体的（マーカーから少し浮いた状態）に表示してみます．
まずは，Assetsフォルダで[Create]-[Materials]を選択し，新規にマテリアルを作成します． マテリアルの名前はDogに変更しておきます． 画像フォーマットが透過背景であることから，InspectorでShaderをUnit/Transparentに設定します． Unit/Transparentはテクスチャ画像のアルファ値を反映して透過にすることが可能なシェーダーです． 次に，Textureを犬の画像に変更します これで，オブジェクトに設定するマテリアルが準備できました．


次に，シーンのARPictureSceneをダブルクリックします． ここで，Hierarchyから[Create]-[3D Object]-[Cube]を選択し，新規に立方体（Cube）のオブジェクトを作成します． オブジェクトの名前はDogObjectに変更しておきます． このDogObjectをドラッグして，MarkerObjectの直下に配置します． このとき，デフォルトで設定されている，Cubeオブジェクトは削除しておきます．
DogObjectのInspectorから，オブジェクトの**位置（Position）やサイズ（Scale）**を修正します． 位置はX=0，Y=0，Z=20とし，サイズはX=80，Y=80，Z=0とします（Z=0とすることで幅がなくなり平面となります）．

最後に，DogObjectのMaterialsをクリックして，作成したDogを選択しておきます． 再生ボタンをクリックすると，犬の画像（001.png）が表示されることを確認してください． テクスチャ画像が上下反対に張り付けられる場合は， TilingのYの値を**-1**に変更します（Direct3DかOpenGLで振る舞いが異なるようです）．


次回は画像の代わりに3Dオブジェクトを表示することにに挑戦してみます．
参考書籍
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/unity/artoolkit/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">NyARToolkit for Unityの導入</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/2e5257b7d23188715fea274c2d50aaf9.png"></img>
	
      </div>
      
      <p>
	NYARtoolkitとはARToolkitをベースに開発された拡張現実ライブラリが**NyARToolkit**です． ゲーム開発環境である**Unity**で利用可能な**NyARToolkit for Unity**の導入方法を紹介します． ここでは，**Unity**のバージョンは**5.3.5**，**NyARToolkit for Unity** **のバージョンは**5.0.8**を対象とします． また，NyARToolkitのライセンスは**LGPLv3**となっています（商用ライセンスもあるようです）． このライセンスは，「著作権の表示」を条件に，商用利用や配布が認められています（詳細はWikipediaを参照）．
プロジェクトの作成Unityで新しいプロジェクトを作成します． ここでは，プロジェクト名を「ARPictureBook」としています． また，ゲーム環境は「3D」を選択しておきます．

次に，「NyARToolkit for Unity」のパッケージをインストールします． パッケージは下記URLからダウンロードできます．
https://github.com/nyatla/NyARToolkitUnity/releases
ツールバーから[Assets]-[Import package]-[Custom package]をクリックして， ダウンロードしたパッケージを選択します． ファイルの読込み後に，ダイアログが表示されたら， 全てのファイルにチェックを入れた状態でimportをクリックしましょう．

ファイルの取り込みが終わると，プロジェクトのAssetsには6つのフォルダが展開されます． Assetとは，ゲームを構成する最小の構成単位のことです． 例えば，シーン，キャラクター，画像ファイル，音楽ファイルなどもAssetです． ここでは，sampleフォルダに含まれるSimpleLiteを試しに実行してみましょう．

SimpleLiteを実行する前に下記の準備が必要です． ウェブカメラは標準的なモノであれば問題ないと思われます． また，マーカーはパッケージに付属しているMarkerHiro.pngを利用しますが， NyARToolkit用のマーカーは，tarotaroorg氏が公開している オンラインのツールを利用して，自由に作成することも可能です．
 ウェブカメラ（CMS-V30SETBKを使用） マーカーが印刷された紙（resourceフォルダに含まれるMarkerHiro.pngを印刷します）  
準備が整ったら，画面上部にある再生ボタンをクリックします． すると，ゲーム画面にカメラ映像が映し出されます． このカメラにマーカーを印刷した紙を映すと，マーカー上に赤色の立方体（Cube）表示されることを確認してください． これが，拡張現実と呼ばれる技術です．


フォルダのコピーAssetsフォルダに新規フォルダを作成し，SimpleLiteのフォルダに含まれる２つのファイルをコピーします（ARCameraBehaviorはC#のスクリプト，simpleLiteはシーンと呼ばれるファイルです）． ここでは，C#スクリプトをARPictureCamera，シーンをARPictureSceneにファイル名を変更しておきます． 次に，ARPictureSceneをダブルクリックし，HierarchyのCameraをクリックします． シーンに関連付けられたコンポーネントが表示されているので， ARCameraBehaviorを削除（Remove Component）します． さらに，Add Componentをクリックし，コピーしたARPictureCameraを選択しておきます． これで，ARPictureCameraに記述したスクリプトが，ARPictureSceneに関連付けられます．


しかし，このままではARPictureCameraがエラーとなり実行できません． これは，変更したファイル名とスクリプトのクラス名が一致しないことが原因です． そこで，ARPictureCameraのソースコードを表示し， クラス宣言部にあるクラス名を，ARCameraBehaviorからARPictureCameraに修正し，エラーを取り除きましょう． 最後に，再生ボタンをクリックして，SimpleLiteと同様の実行結果になることを確認してください．
      </p>
    </div>
  </a>
</div>

		
	</div>
	
	<div class="pagination">
		
			<a href="/categories/tech/page/3/" class="left arrow">&#8592;</a>
		
		
			<a href="/categories/tech/page/5/" class="right arrow">&#8594;</a>
		
	
		<span>4</span>
	</div>
</main>


        <footer>
  <div>
    <span>&copy; 2016 Naoto Mukai All Rights Reserved.</span> 
  </div>
</footer>


    </body>
</html>
