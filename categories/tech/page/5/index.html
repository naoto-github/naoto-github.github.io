<!DOCTYPE html>
<html lang="ja">
    <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    Tech | mLAB
  </title>

  
  <link rel="stylesheet" href="/css/style.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">
  <link rel="stylesheet" href="/css/custom.css">
  <link rel="stylesheet" href="/css/syntax.css">
  
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/favicon/apple-touch-icon.png">

  
  <link href="/categories/tech/index.xml" rel="alternate" type="application/rss+xml" title="mLAB" />

  
  <link rel="preconnect" href="https://fonts.gstatic.com">
  <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Kosugi&display=swap" rel="stylesheet">

  
  <script src="https://kit.fontawesome.com/0c97f11cd6.js" crossorigin="anonymous"></script>

  
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1/dist/jquery.min.js"></script>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.js"></script>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.css"/>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick-theme.min.css"/>

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/SlickNav/1.0.10/jquery.slicknav.js"></script>
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/SlickNav/1.0.10/slicknav.min.css" />

  
  <div id="fb-root"></div>
  <script async defer crossorigin="anonymous" src="https://connect.facebook.net/ja_JP/sdk.js#xfbml=1&version=v9.0"></script>  
  
  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
	     tex2jax: {
	        inlineMath: [['$','$']]
        }
    });
  </script>

  
  <script src="./freetile/jquery.freetile.min.js"></script>  

  
  

  <meta property="og:title" content="Tech" />



  <meta property="og:type" content="article" />



  <meta property="og:url" content="https://mukai-lab.info/categories/tech/" />



  <meta property="og:image" content="https://i.gyazo.com/620adf7e5d9b2cdc983a80d01b7e3367.png" />



  <meta property="og:site_name" content="mLAB" />



  <meta property="og:description" content="椙山女学園大学 文化情報学部 向研究室のブログです．大学教員の不思議な日常を４コマ漫画でお伝えします．" />



  
  
<meta name="twitter:card" content="summary" />

<meta name="twitter:site" content="@nmukai1978" />


  <meta name="twitter:title" content="Tech" />



  <meta name="twitter:description" content="椙山女学園大学 文化情報学部 向研究室のブログです．大学教員の不思議な日常を４コマ漫画でお伝えします．" />



  <meta name="twitter:image" content="https://i.gyazo.com/620adf7e5d9b2cdc983a80d01b7e3367.png" />


  
</head>

    <body>
        <nav class="nav">
  <div class="nav-container">
    
    <a href="/">
    <h2 class="nav-title">
      <img src="https://mukai-lab.info/favicon/favicon-48x48.png" align="top"/>
      <span>mLAB</span>
      

<ul id="menu">
  
  
  <li>
    <a href="/">Top</a>
  </li>
  
  
  
  <li>
    <a href="/classes/">Classes</a>
  </li>
  
  
  
  <li>
    <a href="/posts/">Comics</a>
  </li>
  
  
  
  <li>
    <a href="/tech/">Tech</a>
  </li>
  
  
  
  <li>
    <a href="/projects/">Projects</a>
  </li>
  
  
</ul>

<script>
  $(function(){
    $("#menu").slicknav({
      label: "メニュー"
    });
  });
</script>



    </h2>
    </a>
  </div>
</nav>

        

<main>
    <h1>Category Tech</h1>
	<div class="catalogue">
		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/eyetracker/eyetracker2/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">OpenCVを利用した視線位置の描画</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/10494725ec7c3a2ea055060c3bc2e51a.png"></img>
	
      </div>
      
      <p>
	OpenCVとは  前回はTobii Eye Tracker 4Cを利用して， ディスプレイに対する視線の位置を検出しました． 今回は画像処理ライブラリのOpenCVを利用して， 視線位置に合わせてウィンドウに図形を描画することに挑戦します． OpenCVは，インテル が開発したオープンソースのライブラリであり， 画像処理に関する様々なアルゴリズムを容易に実装することができます（2016年にインテルがItseezを買収）． また，C/C++，Java，Python，MATLAB のライブラリとして配布されており， プログラミング言語を問わず利用できることも魅力です．
図形描画の実装  前回までにライブラリの導入は完了しているので，今回はウィンドウを表示するところからスタートしましょう． ファイル名は MyEyeTrack2.py とします． まずは，OpenCV(cv2) と NumPy のモジュールをインポートします． ここで，NumPy には，np という別名を付けていることに注意してください．
 最初に，np.zeros メソッドを用いて，幅1980px，高さ1080pxの黒色の画像を生成します． このメソッドは値が0の配列を返値とし，引数には配列の長さを表す タプル と データタイプを渡します． ここでは，8ビットの符号なし整数（0~255）であるnp.unit8をデータタイプとしています． この画像を imshow メソッドで表示します． 第1引数の &ldquo;MyEyeTrack&rdquo; はウィンドウのタイトルバーに表示される文字列です．
 前回実装したコールバックメソッドを修正し，視線位置に白色の円を描画するように改良します． 左右の視線位置の平均値を円の中心座標とします． コールバックメソッドで取得される視線位置は， 標準化された値のためウィンドウのサイズを掛けて，ピクセル座標に変換しています． このとき，y座標はウィンドウのタイトルバーの幅を考慮して，50 だけ減らしています （本当はキャリブレーションをやらなきゃいけないけど）． 円を描くには，cv2.circle メソッドを利用します． 引数には，画像，中心座標，半径，色，枠線の太さ を指定します． ここで，枠線の太さに負の値を指定すると，塗りつぶしの円になります． また，global はグローバル変数の img を用いることの宣言です．
 上記で実装したコールバックメソッドをEyeTrackerオブジェクトに登録します．
 最後にwhile文でimshowメソッドを呼び出し，画像の再描画を繰り返します． ここでは，100ms毎に画像を描画しています． このとき，ESCキー が押されると，ループを終了し， コールバックメソッドの解除，ウィンドウの破棄，システムの終了を行います．
 では，プログラムを実行してみましょう． ここでは，四角形を描くように視線を動かしてみました． 視線に合わせて白い円が描画されていることが分かります． しかし，思っていたより視線を安定させるのは難しいです．
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/processing/processing/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">Processingではじめての画像処理</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/b8c6235a70fd7b5283e36b41f66a483a.png"></img>
	
      </div>
      
      <p>
	Processingとは  Processingは，2001年からMIT Media Labでスタートしたオープンソースのプロジェクトです． 当初からプログラミング教育を意識して開発され，初学者でも視覚的なコンテンツ（ビジュアルコンテンツ）を容易に作成できることが大きな特徴です． デザイナーや建築家などの利用も多く，作品はニューヨーク近代美術館など多くの著名な美術館で公開されています． オープンソースであることにもこだわりが強く，活発なコミュニティが形成されており，ソフトウェアを拡張するためのライブラリやツールが多く提供されています． ウィンドウズ，マッキントッシュ，リナックスなどのプラットフォームで動作可能であり，公式サイトから無料でダウンロードすることができます． また，ProcessingはJavaをベースに開発されており，Javaによく似た文法でコードを記述できることも人気の高い理由です．
Processingの開発環境  前節で述べたように様々なプラットフォームで動作可能なパッケージが配布されていますが， 今回はオンラインでコーディングが可能なOpenProcessingを利用してみましょう． このサイトでは，Processingで制作した作品（スケッチ）を公開することができます． また，登録も無料で誰でも利用することが可能です． では，下記のリンクをクリックしてOpenProcessingのサイトを開きましょう．
OpenProcessing
残念ながら現在は英語のサイトしか存在せず，ユーザ同士の交流は英語が中心です． 一方で，日本語でコメントを記述することは問題ないようです． Processingに慣れてきたらオリジナルの作品を投稿することを目指しましょう．

まずは，公開されているスケッチを閲覧してみましょう． 「Activity」のタブをクリックすると，右下に評価の高い作品の一覧が表示されます． これらのスケッチはOpenProcessingの登録者が制作した作品です． 自由にスケッチを選んでプログラムを実行してみましょう． 下記のは2017日6月9日現在のスケッチのリストです．

幾つか人気のある作品を取り上げてみましょう． まずは，aadebdeb氏のrainbow spinという作品です． マウスの動きに合わせて，描かれている虹色の渦巻きの回転方向や倍率が変化します．
 次は，Victor Galve氏のPractica 2という作品です． マウスでクリックすると，様々な色や形の花火が打ちあがります．
 最後は，Raven Kwok氏のNoise Turbulence Doodlesという作品です． マウスをドラッグすると円が重なりながら奇妙な形に成長します．
 スケッチの作成  新しいスケッチを作成してみましょう． サイトのトップから「+Create a Sketch」をクリックしてください． 下記のようにソースコードを入力する画面になります（デフォルトで記述されているコードは削除しておきましょう）． ソースコードを記述した後で，をクリックすると，記述されたコードを実行することができます．

Processingをウェブ上で実行するには「P5js」，または，「Processing.js」というライブラリを利用します． 今回は後者の Processing.js を利用するため，画面右の Settings で， Mode を Processing.js に変更してください．

図形を描く前に，スケッチの大きさや背景色を設定しましょう． スケッチの大きさは300x300ピクセル，背景色は白にします． 大きさを設定するにはsize(幅,高さ)，背景色を設定するにはbackground(色)と記述します（255は白色を意味します）． 下記のソースコードを入力したら，をクリックしましょう． 背景色が白色のスケッチが表示されます（300x300ピクセルのスケッチが中央に配置されています）． （「//」が先頭にある文章はコメントと呼ばれ，プログラムとは認識されません） もとのソースコードの入力画面に戻るにはをクリックします．
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/robohon/robohon3/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">QRコードの読み取り</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/52ce7ebbce4cb3079c55c9dce75ece7f.png"></img>
	
      </div>
      
      <p>
	ロボホンのカメラ  ロボホンの開発キットRoBoHoN Software Development Kitに 同梱されているカメラ用のアプリ SampleCamera を基にQRコードの読み取り機能を実装します． QRコードの読み取りには，オープンソースとして提供されているZXingを採用します． ちなみに， ZXing の名称は Zebra Crossing が基になっているようです． ZXing の実装方法はNAVERまとめの記事「Java QRコード読み取り」を参考にし， ライブラリはMaven RepositoryからダウンロードしたJARファイルを利用します． 現時点での最新バージョンは 3.3.0 です（2017年9月20日現在）．
カメラの利用  まずは，SampleCamera のプロジェクトを Android Studio で開き確認しましょう． MainActivity.javaがこのサンプルの中心となるソースファイルです．
ロボホンのカメラを利用した撮影には，静止画，動画，また，顔認識の有無などの設定が可能です． ここでは，写真撮影 顔認識無ボタン の挙動にQRコードの読み取り機能を加えます． 下記が該当部分のソースコードです． 背面のモニタにあるcameraButtonをタップすると，getIntentForPhoto メソッドの 返り値（Intent クラスのインスタンス）が，sendBroadcast メソッドで通知される処理となっていることが分かります．
 次に，getIntentForPhoto メソッドを確認します． ここで，登場する ShootMediaUtil クラスが重要な役割を担います． Intent クラスのコンストラクタの引数には静止画撮影用のアクション名であるShotMediaUtil.ACTION_SHOOT_IMAGEを指定します． また，アクション（撮影）終了後の結果通知を得るために，putExtraメソッドで ShotMediaUtil.EXTRA_REPLYTO_ACTIONを指定し， ACTION_RESULT_TAKE_PICTURE をその返り値としています． このインテントを sendBroadcast で通知することで，カメラの撮影機能が実行されます．
 カメラの撮影後には，結果通知として ACTION_RESULT_TAKE_PICTURE を CameraResultReceiver クラスの onReceive メソッドで受け取ります． ここでは，ACTION_RESULT_TAKE_PICTURE に該当するコードのみを抜き出してみます． 撮影が成功していれば，インテントから ShootMediaUtil.RESULT_OK を受け取り， 保存した画像ファイルのパスを取得します． 取得されたパスは，リソースIDを利用して，ロボホンの背面の TextView に表示されます．
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/eyetracker/eyetracker1/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">Tobii Eye Trackerを利用した視線の認識</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/7278ccfa8d68bdb1ff12fc50372c2373.png"></img>
	
      </div>
      
      <p>
	Tobii Eye Trackerとは  近年，ユーザの視線を検出するアイトラッキングという技術が注目されています． アイトラッキングはマウスやキーボードの代替となりうるヒューマンインターフェイスの一つであり， 手足が不自由な障がい者のコミュニケーション装置としても活用されています． 光学センサーと画像処理技術を用いて，眼球の動き を解析することで， ユーザの注視点（見つめている場所）を推定する方法が一般的です． 今回は，トビー・テクノロジー株式会社が ゲーム用に提供しているEye Tracker 4Cという 視線入力装置を利用して，ユーザの視線を検出するプログラムを実装してみます． トビー・テクノロジーは自社の製品を利用したソフトウェアを開発するための Tobii Pro SDKを独自に提供していますが， Eye Tracker 4CでTobii Pro SDKを利用するためには， プロアップグレードキーが必要となることに注意が必要です （Tobii Core SDK や Tobii Gaming SDK という選択肢もありますが，研究やデータ分析を用途とする場合は Tobii Pro SDK を利用する必要があります）． また，開発用の言語には Python，Matlab，C，Unity などに対応していますが， ここでは，機械学習に適したPythonを採用します． Tobii Pro SDK のドキュメントが公開されおり，この情報を参考にしながら開発を進めることになります．

開発環境の準備  まずは，Pythonをインストールします． Pythonのバージョンには3.xと2.xの２通り存在しますが， Tobii Pro SDK に対応している 2.x を選択する必要があることに注意してください． ここでは，現時点での最新バージョンである2.7.14を利用します(2017年10月17日)．
Pythonの本体に加えて，画像処理ライブラリのOpenCVと， 数値計算ライブラリのNumPyを追加でインストールします． インストール方法はPythonのパッケージ管理システムであるpipを利用すれば簡単です． コマンドプロンプトで下記のように入力します． ここでは，3.3.0.10 のOpenCVと 1.13.3 のNumPyがインストールされました．
$ python -m pip install opencv-python Collecting opencv-python Downloading opencv_python-3.
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/robohon/robohon2/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">トリガを用いた発話</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/eda3a28b300bedb562b9e858dc667ad1.png"></img>
	
      </div>
      
      <p>
	サンプルアプリ  ロボホンの開発キットRoBoHoN Software Development Kitには， 下記のサンプルアプリが同梱されています． 今回は基本的な音声UIの使い方を理解するためSampleSimpleのソースコードを解析し， 改良してみましょう．
 SampleSimple 音声UIを使った基本的な機能のアプリ SampleScenario シナリオで使える変数やタグなどのアプリ SampleProjector プロジェクターを利用したアプリ SampleAddressBook 電話帳を利用したアプリ SampleCamera カメラを利用したアプリ SampleDance ダンスを利用したアプリ SampleMultilingual 多言語対応を実装したアプリ  まずは，Android Studio で上記のサンプルを読み込み，プロジェクトとして展開します． Android Studioを起動したら，[File]-[Open]-[Open File or Project]を選択し， 開発キットに含まれる SampleSimple のフォルダをクリックします． ビルドが終了したら，Run Appをクリックして実行してみましょう．
実行するとロボホンの背面には「ACCOST」「RESOLVE VARIABLE」「SET_MEMORY_P」「GET_MEMORY_P」「FINISH APP」の５つのボタンが表示されています． ここで，「ACCOST」ボタンをタップすると，「アプリから発話開始するサンプルだよ」とロボホンが発話します． ちなみに，ACCOSTとは，アプリから強制的にトピック（発話やモーション）を実行することを意味します．

 アプリトリガによる発話  最初に，ボタンなどアプリからのアクションをトリガとして，トピックを実行するアプリトリガによる発話について学びます． まずは「ACCOST」ボタンに該当するソースコードを確認していきましょう． 一般にアンドロイドのアプリはActivityクラスを継承して開発します． Activityクラスにはライフサイクルがあり，アプリを起動すると下図のようにメソッドを実行します．

ここで，「ACCOST」ボタンに関する振る舞いは，onCreate()メソッドの内部にあり，下記のように記述されています． 最初に，リソースIDを利用して，変数voiceAccostButtonに，Buttonクラスのオブジェクトを代入し，ボタンをタップしたときのイベントリスナーを登録しています． ボタンがタップされると，VoiceUIVariableListHelper クラスのインスタンスに実行したいACCOSTを登録し，VoiceUIManagerUtil クラスのupdateAppInfoメソッドで発話を実行しています． ここで，ScenarioDefinitions.ACC_ACCOST には，実行するACCOSTの名称である jp.co.sharp.sample.simple.accost.t1 が代入されています．
 ここでのポイントは，実行するACCOSTの定義です． 具体的な定義は，シャープ株式会社が独自に定義している HVML(Hyper Voice Markup Language) というXMLファイルに記述します． ここでは，assetsフォルダに含まれるjp_co_sharp_sample_simple_accost.
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/robohon/robohon1/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">ロボホンの開発環境の構築</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/f28e97ef3b604599c791a2dc40e76551.png"></img>
	
      </div>
      
      <p>
	ロボホンとは  ロボホン（RoBoHoN）はシャープ株式会社が開発する人型のロボットであり，音声UIを利用して自然言語で会話が出来ることを特徴としています． ロボホンのOSはグーグル社が開発するAndroidであり，Android用のアプリ開発と同様にロボホンのアプリを制作することができます． ロボホンの開発に必要なRoBoHoN Software Development Kitが公開されており， ガイドラインに従って音声UIを利用したアプリを開発することが可能です． ここで，ガイドラインとは，ロボホンの世界観を維持するための，ロボホンの キャラクター ， 話し方 ， ユーザとの関係性 のことを指しています． 今回は，ロボホンのアプリ開発に必要な環境の構築方法に関して解説します．
開発環境  ロボホンアプリの開発には下記の環境が必要です． ココロプランには，「ビジネス基本プラン」「ビジネスプラン2000」など数種類が設定されており， プランに応じて月毎の会話上限が定められています． アプリ開発で頻繁に音声UIを利用する場合は高額なプランも検討する必要があります．
 ロボホン本体（現状ではエミュレータは存在しない） ネットワーク環境（音声UIを利用するため） ココロプランの契約（本体購入時に同時契約）  開発プラットフォームとしては，Android Studioを利用します（ロボホンのOSはAndroid 5.0です）． また，Android Studio のバージョンは 1.5 以降が必要とされており，今回は現時点での最新版である 2.3.3.0を採用します(2017年9月18日)． ファイルサイズは約1.9GBと，かなり大きいので注意してください．

Android Studioのインストール・パッケージをダウンロードしたら，インストールを始めましょう． インストールが完了したらロボホンのアプリ開発に必要なパッケージを追加でインストールします． まずは，バージョン 1.5 の Android SDKです． Settingsのメニューから[Appearance&amp;Behavior]-[System Settings]-[Android SDK]を選択し， Android 5.0 (Lollipop) にチェックを入れて Apply をクリックします．

同様にAndroid SDK Build-Toolsを追加します． バージョンは21.0.0以降にチェックを入れて Apply をクリックします（Show Package Detailsをクリックするとバージョンの選択が可能です）．

開発したアプリをデバッグする際に，音声対話に失敗することを回避するために， Instant Run の設定を無効化します． Settingsのメニューから[Build, Execution, Developemnt]を選択し，全てのチェックをはずします． これで，Android Studio の準備は完了です．
      </p>
    </div>
  </a>
</div>

		
			<div style="display:flex; justify-content:center">
  <a href="https://mukai-lab.info/pages/tech/emotiv/emotiv3/" class="catalogue-item">
    <div>
      <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" class="catalogue-time">January 1, 0001</time>
      <h1 class="catalogue-title">脳波データをフーリエ解析</h1>
      <div class="catalogue-line"></div>
      
      <div>
	
	<img src="https://i.gyazo.com/46bdd4888ca253d334c2d14292b7df84.png"></img>
	
      </div>
      
      <p>
	フーリエ変換とは  前回までにCortexを利用してEmotiv Epoc+で計測したデータをPythonから取得する方法を説明しました． 一般に，脳波の生データから情報を読み取ることは困難であり，周波数成分に変換することが必要になります． この周波数成分の変換に必要な技術が鬼門のフーリエ変換（Fourier Transform: FT） です． 大学の工学部ではカリキュラムの１つとなっているフーリエ変換（フーリエ級数）に躓いた人も多いのではないでしょうか（何を隠そう向もその一人です）． ここでは，あまり深く考えず，フーリエ変換は 時間領域 から 周波数領域 に変換する仕組みだと理解しておけば十分です．
例を挙げて考えてみましょう． Emotivのサンプリングレートは 128Hz であることから，1秒間に 128個の信号が計測されることに注意してください． 仮に計測された脳波が，下記のグラフのような信号だったとします． この信号には，1秒間に 3周期分 のSin関数が含まれています． グラフの横軸が 時間 であることから，このデータは 時間領域 に存在しています．

これを，フーリエ変換（高速フーリエ変換）すると下記のグラフになります． このグラフの横軸は 周波数 であり，このデータは 周波数領域 に存在しています． 周波数 が 3 のところに，縦棒がありますよね． この結果は，先のグラフには 周期3 の信号が含まれていることを表しています．

フーリエ変換の凄いところは，異なる周期の信号が混ざっていても，それぞれの周波数成分の強さが取得できることです． 次に，下記のグラフの信号を考えます この信号には，1秒間に 5周期分 のSin関数が含まれています． また，その振幅は先の信号と比べて 1/2 の大きさです．

この信号を，先の周期3の信号に加えます（本当に足し算するだけ）． すると，下図のようなグラフになります． これだけで，複雑なグラフになり，人間には理解できないレベルに到達します．

さぁ，フーリエ変換の出番です． 上の周期3と周期5のグラフを加えた信号を変換すると，下記のグラフになります． 周期3 の振幅は 1，また，周期5 の振幅が 0.5 となっていることが読み取れますね． フーリエ級数を発明したジョゼフ・フーリエは本当に天才だと思います（真面目）．

脳波データの記録  それでは，前回までに実装したプログラムを利用して脳波データを取得しましょう． 準備として，数値計算ライブラリのNumpyと，描画ライブラリのmatplotlib.
      </p>
    </div>
  </a>
</div>

		
	</div>
	
	<div class="pagination">
		
			<a href="/categories/tech/page/4/" class="left arrow">&#8592;</a>
		
		
	
		<span>5</span>
	</div>
</main>


        <footer>
  <div>
    <span>&copy; 2016 Naoto Mukai All Rights Reserved.</span> 
  </div>
</footer>


    </body>
</html>
